<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 99]
- [eess.SY](#eess.SY) [Total: 11]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.SD](#cs.SD) [Total: 7]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.HC](#cs.HC) [Total: 17]
- [cs.LG](#cs.LG) [Total: 99]
- [cs.RO](#cs.RO) [Total: 25]
- [cs.GT](#cs.GT) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
*Mustafa Chasmai,Gauri Jagatap,Gouthaman KV,Grant Van Horn,Subhransu Maji,Andrea Fanelli*

Main category: cs.CV

TL;DR: 提出了一种名为“moment sampling”的新方法，利用文本到视频时刻检索模型指导帧采样，以提升长视频问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在长视频问答中表现不佳，主要因为帧采样方法导致关键帧丢失或冗余信息过多。

Method: 采用轻量级时刻检索模型，根据问题上下文选择最相关帧，优化帧采样过程。

Result: 在四个长视频问答数据集和四种先进视频大语言模型上的实验验证了方法的有效性。

Conclusion: 提出的“moment sampling”方法显著提升了长视频问答的性能，且模型无关。

Abstract: Recent advancements in video large language models (Video LLMs) have
significantly advanced the field of video question answering (VideoQA). While
existing methods perform well on short videos, they often struggle with
long-range reasoning in longer videos. To scale Video LLMs for longer video
content, frame sub-sampling (selecting frames at regular intervals) is commonly
used. However, this approach is suboptimal, often leading to the loss of
crucial frames or the inclusion of redundant information from multiple similar
frames. Missing key frames impairs the model's ability to answer questions
accurately, while redundant frames lead the model to focus on irrelevant video
segments and increase computational resource consumption. In this paper, we
investigate the use of a general-purpose text-to-video moment retrieval model
to guide the frame sampling process. We propose "moment sampling", a novel,
model-agnostic approach that enables the model to select the most relevant
frames according to the context of the question. Specifically, we employ a
lightweight moment retrieval model to prioritize frame selection. By focusing
on the frames most pertinent to the given question, our method enhances
long-form VideoQA performance in Video LLMs. Through extensive experiments on
four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we
demonstrate the effectiveness of the proposed approach.

</details>


### [2] [Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay](https://arxiv.org/abs/2507.00042)
*Xinrun Xu,Jianwen Yang,Qiuhong Zhang,Zhanbiao Lian,Zhiming Ding,Shan Jiang*

Main category: cs.CV

TL;DR: ER-EMU算法通过自适应经验回放和领域距离度量选择历史数据，解决云边协同目标检测中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 动态交通环境中，模型在适应新数据分布时会遗忘旧知识，现有方法无法有效利用历史数据。

Method: 提出ER-EMU算法，结合FIFO经验缓冲区和DDM-ES算法，利用MK-MMD度量选择多样化历史数据。

Result: 在Bellevue交通数据集上，ER-EMU显著提升了多种云边协同目标检测框架的性能。

Conclusion: ER-EMU通过优化历史数据选择和多样性训练，有效缓解了灾难性遗忘问题。

Abstract: Continually adapting edge models in cloud-edge collaborative object detection
for traffic monitoring suffers from catastrophic forgetting, where models lose
previously learned knowledge when adapting to new data distributions. This is
especially problematic in dynamic traffic environments characterised by
periodic variations (e.g., day/night, peak hours), where past knowledge remains
valuable. Existing approaches like experience replay and visual prompts offer
some mitigation, but struggle to effectively prioritize and leverage historical
data for optimal knowledge retention and adaptation. Specifically, simply
storing and replaying all historical data can be inefficient, while treating
all historical experiences as equally important overlooks their varying
relevance to the current domain. This paper proposes ER-EMU, an edge model
update algorithm based on adaptive experience replay, to address these
limitations. ER-EMU utilizes a limited-size experience buffer managed using a
First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based
Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel
maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target
domains, prioritizing the selection of historical data that is most dissimilar
to the current target domain. This ensures training diversity and facilitates
the retention of knowledge from a wider range of past experiences, while also
preventing overfitting to the new domain. The experience buffer is also updated
using a simple random sampling strategy to maintain a balanced representation
of previous domains. Experiments on the Bellevue traffic video dataset,
involving repeated day/night cycles, demonstrate that ER-EMU consistently
improves the performance of several state-of-the-art cloud-edge collaborative
object detection frameworks.

</details>


### [3] [MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations](https://arxiv.org/abs/2507.00043)
*Mehmet Yigit Avci,Pedro Borges,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP是一种多模态对比学习框架，通过将MRI图像与DICOM元数据对齐，学习对比感知表示，无需依赖人工标签。


<details>
  <summary>Details</summary>
Motivation: MRI扫描的准确解释依赖于对图像对比度的精确理解，但现有标签（如T1或T2加权）过于粗略，且元数据常不完整或噪声多。这影响了图像解释、检索和临床工作流的整合。

Method: 提出MR-CLIP框架，通过对比学习对齐MRI图像和DICOM元数据，学习对比感知表示。训练数据涵盖多种扫描仪和协议。

Result: MR-CLIP能够捕捉不同采集间的对比度变化，实现解剖结构不变的表示，在跨模态检索和对比分类中表现优异。

Conclusion: MR-CLIP为临床应用提供了可扩展的解决方案，代码和模型权重已开源。

Abstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical
systems is based on a precise understanding of image contrast. This contrast is
primarily governed by acquisition parameters, such as echo time and repetition
time, which are stored in the DICOM metadata. To simplify contrast
identification, broad labels such as T1-weighted or T2-weighted are commonly
used, but these offer only a coarse approximation of the underlying acquisition
settings. In many real-world datasets, such labels are entirely missing,
leaving raw acquisition parameters as the only indicators of contrast. Adding
to this challenge, the available metadata is often incomplete, noisy, or
inconsistent. The lack of reliable and standardized metadata complicates tasks
such as image interpretation, retrieval, and integration into clinical
workflows. Furthermore, robust contrast-aware representations are essential to
enable more advanced clinical applications, such as achieving
modality-invariant representations and data harmonization. To address these
challenges, we propose MR-CLIP, a multimodal contrastive learning framework
that aligns MR images with their DICOM metadata to learn contrast-aware
representations, without relying on manual labels. Trained on a diverse
clinical dataset that spans various scanners and protocols, MR-CLIP captures
contrast variations across acquisitions and within scans, enabling
anatomy-invariant representations. We demonstrate its effectiveness in
cross-modal retrieval and contrast classification, highlighting its scalability
and potential for further clinical applications. The code and weights are
publicly available at https://github.com/myigitavci/MR-CLIP.

</details>


### [4] [HistoART: Histopathology Artifact Detection and Reporting Tool](https://arxiv.org/abs/2507.00044)
*Seyed Kahaki,Alexander R. Webber,Ghada Zamzmi,Adarsh Subbaswamy,Rucha Deshpande,Aldo Badano*

Main category: cs.CV

TL;DR: 论文提出了三种全切片图像（WSI）中的伪影检测方法，比较了它们的性能，并开发了一个质量报告评分卡。


<details>
  <summary>Details</summary>
Motivation: WSI在癌症诊断中广泛应用，但伪影会影响图像分析，因此需要有效的检测方法。

Method: 比较了三种方法：基于基础模型的FMA、基于深度学习的DLA和基于知识的KBA，用于检测六种常见伪影。

Result: FMA表现最佳（AUROC: 0.995），优于DLA（0.977）和KBA（0.940）。

Conclusion: FMA是检测WSI伪影的最有效方法，质量报告评分卡为实际应用提供了支持。

Abstract: In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to
digitize tissue specimens for detailed, high-resolution examination; however,
other diagnostic approaches, such as liquid biopsy and molecular testing, are
also utilized based on the cancer type and clinical context. While WSI has
revolutionized digital histopathology by enabling automated, precise analysis,
it remains vulnerable to artifacts introduced during slide preparation and
scanning. These artifacts can compromise downstream image analysis. To address
this challenge, we propose and compare three robust artifact detection
approaches for WSIs: (1) a foundation model-based approach (FMA) using a
fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning
approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach
(KBA) leveraging handcrafted features from texture, color, and frequency-based
metrics. The methods target six common artifact types: tissue folds,
out-of-focus regions, air bubbles, tissue damage, marker traces, and blood
contamination. Evaluations were conducted on 50,000+ image patches from diverse
scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA
achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),
outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])
and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into
actionable insights, we developed a quality report scorecard that quantifies
high-quality patches and visualizes artifact distributions.

</details>


### [5] [VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos](https://arxiv.org/abs/2507.00261)
*Zhiyin Lin,Purvi Goel,Joy Yun,C. Karen Liu,Joao Pedro Araujo*

Main category: cs.CV

TL;DR: VirtualFencer系统通过无监督学习从视频中提取3D击剑动作和策略，并生成逼真的击剑行为。


<details>
  <summary>Details</summary>
Motivation: 击剑动作多样且受对手行为影响，需数据驱动建模。

Method: 从视频中无监督提取3D动作和策略，生成击剑行为。

Result: 系统能自我对战、与真实击剑手对战，并与专业击剑手互动。

Conclusion: VirtualFencer展示了数据驱动建模在击剑中的潜力。

Abstract: Fencing is a sport where athletes engage in diverse yet strategically logical
motions. While most motions fall into a few high-level actions (e.g. step,
lunge, parry), the execution can vary widely-fast vs. slow, large vs. small,
offensive vs. defensive. Moreover, a fencer's actions are informed by a
strategy that often comes in response to the opponent's behavior. This
combination of motion diversity with underlying two-player strategy motivates
the application of data-driven modeling to fencing. We present VirtualFencer, a
system capable of extracting 3D fencing motion and strategy from in-the-wild
video without supervision, and then using that extracted knowledge to generate
realistic fencing behavior. We demonstrate the versatile capabilities of our
system by having it (i) fence against itself (self-play), (ii) fence against a
real fencer's motion from online video, and (iii) fence interactively against a
professional fencer.

</details>


### [6] [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
*Ming Li,Chenguang Wang,Yijun Liang,Xiyao Wang,Yuhang Zhou,Xiyang Wu,Yuqing Zhang,Ruiyi Zhang,Tianyi Zhou*

Main category: cs.CV

TL;DR: 论文探讨了多模态大语言模型（MLLMs）在复杂任务中的表现，发现其在特定场景（CaughtCheating）中表现极差，揭示了其局限性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在多数基准测试中表现优异，但在某些复杂任务（如侦探推理）中表现不佳，需进一步研究其能力边界。

Method: 通过设计CaughtCheating任务（基于社交媒体中检测伴侣可疑行为的场景），对GPT-o3进行实验分析。

Result: GPT-o3在CaughtCheating任务中表现接近零分，表明其在复杂视觉感知和推理任务中存在明显不足。

Conclusion: CaughtCheating任务为MLLMs提供了具有挑战性的测试场景，成功解决此类任务将推动其达到人类水平的侦探能力。

Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have
achieved near-ceiling scores on various existing benchmarks, motivating a
demand for more challenging test tasks. These MLLMs have been reported to excel
in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their
potential as a detective who can notice minuscule cues in an image and weave
them into coherent, situational explanations, leading to a reliable answer. But
can they match the performance of excellent human detectives? To answer this
question, we investigate some hard scenarios where GPT-o3 can still handle, and
find a common scenario where o3's performance drops to nearly zero, which we
name CaughtCheating. It is inspired by the social media requests that ask
others to detect suspicious clues from photos shared by the poster's partner.
We conduct extensive experiments and analysis to understand why existing MLLMs
lack sufficient capability to solve this kind of task. CaughtCheating provides
a class of challenging visual perception and reasoning tasks with great value
and practical usage. Success in these tasks paves the way for MLLMs to acquire
human-level detective perception and reasoning capabilities.

</details>


### [7] [Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections](https://arxiv.org/abs/2507.00263)
*Vignesh Ram Nithin Kappagantula,Shayan Hassantabar*

Main category: cs.CV

TL;DR: 提出了一种高效的方法，用于解决度假租赁平台中房间场景发现和分组问题，以及识别卧室中的床型。


<details>
  <summary>Details</summary>
Motivation: 度假租赁平台上的房产图片缺乏结构化分类，给旅行者理解空间布局带来挑战。

Method: 结合监督学习模型（房间类型检测、重叠检测）和聚类算法，利用多模态大语言模型映射床型。

Result: 该方法在性能和效率上显著优于对比学习和预训练嵌入聚类等现有方法。

Conclusion: 提出的方法在实时和数据稀缺环境中表现优异，有助于旅行者更好地理解房产布局。

Abstract: The rapid growth of vacation rental (VR) platforms has led to an increasing
volume of property images, often uploaded without structured categorization.
This lack of organization poses significant challenges for travelers attempting
to understand the spatial layout of a property, particularly when multiple
rooms of the same type are present. To address this issue, we introduce an
effective approach for solving the room scene discovery and grouping problem,
as well as identifying bed types within each bedroom group. This grouping is
valuable for travelers to comprehend the spatial organization, layout, and the
sleeping configuration of the property. We propose a computationally efficient
machine learning pipeline characterized by low latency and the ability to
perform effectively with sample-efficient learning, making it well-suited for
real-time and data-scarce environments. The pipeline integrates a supervised
room-type detection model, a supervised overlap detection model to identify the
overlap similarity between two images, and a clustering algorithm to group the
images of the same space together using the similarity scores. Additionally,
the pipeline maps each bedroom group to the corresponding bed types specified
in the property's metadata, based on the visual content present in the group's
images using a Multi-modal Large Language Model (MLLM) model. We evaluate the
aforementioned models individually and also assess the pipeline in its
entirety, observing strong performance that significantly outperforms
established approaches such as contrastive learning and clustering with
pretrained embeddings.

</details>


### [8] [Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process](https://arxiv.org/abs/2507.00046)
*Akshansh Mishra,Eyob Mesele Sefene,Shivraman Thapliyal*

Main category: cs.CV

TL;DR: 提出了一种基于进化计算的图像分割方法，用于分析增材摩擦搅拌沉积（AFSD）过程中的完整性。


<details>
  <summary>Details</summary>
Motivation: 传统成像方法难以检测AFSD多层构建中的缺陷和特征，需要一种更精确的自动化方法。

Method: 结合粒子群优化（PSO）确定最佳分割阈值，并整合梯度幅度分析和距离变换生成注意力加权可视化。

Result: PSO自动确定了最佳阈值（156-173），多通道可视化技术成功揭示了材料过渡区和潜在缺陷。

Conclusion: 基于注意力的分析能有效识别AFSD接头中的不完全结合和不均匀性，为工艺优化提供了定量指标。

Abstract: This work proposes an evolutionary computing-based image segmentation
approach for analyzing soundness in Additive Friction Stir Deposition (AFSD)
processes. Particle Swarm Optimization (PSO) was employed to determine optimal
segmentation thresholds for detecting defects and features in multilayer AFSD
builds. The methodology integrates gradient magnitude analysis with distance
transforms to create novel attention-weighted visualizations that highlight
critical interface regions. Five AFSD samples processed under different
conditions were analyzed using multiple visualization techniques i.e.
self-attention maps, and multi-channel visualization. These complementary
approaches reveal subtle material transition zones and potential defect regions
which were not readily observable through conventional imaging. The PSO
algorithm automatically identified optimal threshold values (ranging from
156-173) for each sample, enabling precise segmentation of material interfaces.
The multi-channel visualization technique effectively combines boundary
information (red channel), spatial relationships (green channel), and material
density data (blue channel) into cohesive representations that quantify
interface quality. The results demonstrate that attention-based analysis
successfully identifies regions of incomplete bonding and inhomogeneities in
AFSD joints, providing quantitative metrics for process optimization and
quality assessment of additively manufactured components.

</details>


### [9] [AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training](https://arxiv.org/abs/2507.00049)
*Feiyang Kang,Nadine Chang,Maying Shen,Marc T. Law,Rafid Mahmood,Ruoxi Jia,Jose M. Alvarez*

Main category: cs.CV

TL;DR: AdaDeDup是一种混合框架，结合密度剪枝和模型反馈，自适应地优化数据剪枝，显著提升大规模模型训练的数据效率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据集的计算负担和冗余问题，现有方法存在任务无关性或计算成本高的缺陷。

Method: AdaDeDup通过密度剪枝和代理模型反馈，自适应调整剪枝阈值，优化数据子集选择。

Result: 在Waymo、COCO等基准测试中，AdaDeDup显著优于基线方法，减少性能下降，剪枝20%数据时接近原始性能。

Conclusion: AdaDeDup有效提升数据效率，适用于大规模模型训练，代码已开源。

Abstract: The computational burden and inherent redundancy of large-scale datasets
challenge the training of contemporary machine learning models. Data pruning
offers a solution by selecting smaller, informative subsets, yet existing
methods struggle: density-based approaches can be task-agnostic, while
model-based techniques may introduce redundancy or prove computationally
prohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid
framework that synergistically integrates density-based pruning with
model-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions
data and applies an initial density-based pruning. It then employs a proxy
model to evaluate the impact of this initial pruning within each cluster by
comparing losses on kept versus pruned samples. This task-aware signal
adaptively adjusts cluster-specific pruning thresholds, enabling more
aggressive pruning in redundant clusters while preserving critical data in
informative ones. Extensive experiments on large-scale object detection
benchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster
R-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms
prominent baselines, substantially reduces performance degradation (e.g., over
54% versus random sampling on Waymo), and achieves near-original model
performance while pruning 20% of data, highlighting its efficacy in enhancing
data efficiency for large-scale model training. Code is open-sourced.

</details>


### [10] [VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models](https://arxiv.org/abs/2507.00052)
*Binesh Sadanandan,Vahid Behzadan*

Main category: cs.CV

TL;DR: VSF-Med是一个端到端的医疗视觉语言模型（VLM）漏洞评分框架，结合了文本提示攻击模板、视觉扰动和八维评分标准，用于评估医疗VLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 医疗VLM在临床应用中潜力巨大，但缺乏系统性的安全性评估，VSF-Med旨在填补这一空白。

Method: VSF-Med包含三个新组件：文本提示攻击模板库、基于SSIM的视觉扰动和八维评分标准，通过z-score归一化生成0-32的综合风险指标。

Result: 实验显示，VSF-Med在30,000个对抗样本上测试了5,000张放射图像，Llama-3.2-11B-Vision-Instruct和GPT-4o等模型在攻击效果持久性、提示注入有效性等方面表现出不同程度的脆弱性。

Conclusion: VSF-Med为医疗VLM的安全性提供了可复现的基准测试工具，揭示了现有模型的安全漏洞。

Abstract: Vision Language Models (VLMs) hold great promise for streamlining
labour-intensive medical imaging workflows, yet systematic security evaluations
in clinical settings remain scarce. We introduce VSF--Med, an end-to-end
vulnerability-scoring framework for medical VLMs that unites three novel
components: (i) a rich library of sophisticated text-prompt attack templates
targeting emerging threat vectors; (ii) imperceptible visual perturbations
calibrated by structural similarity (SSIM) thresholds to preserve clinical
realism; and (iii) an eight-dimensional rubric evaluated by two independent
judge LLMs, whose raw scores are consolidated via z-score normalization to
yield a 0--32 composite risk metric. Built entirely on publicly available
datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000
adversarial variants from 5,000 radiology images and enables reproducible
benchmarking of any medical VLM with a single command. Our consolidated
analysis reports mean z-score shifts of $0.90\sigma$ for
persistence-of-attack-effects, $0.74\sigma$ for prompt-injection effectiveness,
and $0.63\sigma$ for safety-bypass success across state-of-the-art VLMs.
Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase
of $1.29\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases
of $0.69\sigma$ for that same vector and $0.28\sigma$ for prompt-injection
attacks.

</details>


### [11] [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
*Ziqi Zhong,Daniel Tang*

Main category: cs.CV

TL;DR: MANTA是一个多模态学习框架，通过文本对齐统一视觉和听觉输入，解决了语义对齐、时间同步、多尺度表示和稀疏信息检索等挑战，显著提升了长视频问答和跨模态理解的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态学习方法通常将模态分开处理，导致表示和推理不一致，需要一种统一的方法来优化多模态数据的处理。

Method: MANTA通过信息论优化实现跨模态语义对齐，自适应时间同步，分层内容表示和上下文感知检索，采用数学框架确保最优性。

Result: 在长视频问答任务中，MANTA将最先进模型的准确率提高了22.6%，在30分钟以上的视频中提升27.3%，在时间推理和跨模态理解任务中也有显著提升。

Conclusion: MANTA通过结构化文本统一多模态表示，为多模态学习提供了新的理论基础和技术框架。

Abstract: While multi-modal learning has advanced significantly, current approaches
often treat modalities separately, creating inconsistencies in representation
and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization
via Textual Alignment), a theoretically-grounded framework that unifies visual
and auditory inputs into a structured textual space for seamless processing
with large language models. MANTA addresses four key challenges: (1) semantic
alignment across modalities with information-theoretic optimization, (2)
adaptive temporal synchronization for varying information densities, (3)
hierarchical content representation for multi-scale understanding, and (4)
context-aware retrieval of sparse information from long sequences. We formalize
our approach within a rigorous mathematical framework, proving its optimality
for context selection under token constraints. Extensive experiments on the
challenging task of Long Video Question Answering show that MANTA improves
state-of-the-art models by up to 22.6% in overall accuracy, with particularly
significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we
demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)
and cross-modal understanding (25.1% improvement). Our framework introduces
novel density estimation techniques for redundancy minimization while
preserving rare signals, establishing new foundations for unifying multimodal
representations through structured text.

</details>


### [12] [An efficient plant disease detection using transfer learning approach](https://arxiv.org/abs/2507.00070)
*Bosubabu Sambana,Hillary Sunday Nnadi,Mohd Anas Wajid,Nwosu Ogochukwu Fidelia,Claudia Camacho-Zuñiga,Henry Dozie Ajuzie,Edeh Michael Onyema*

Main category: cs.CV

TL;DR: 该研究提出了一种基于YOLOv7和YOLOv8的植物病害检测系统，通过迁移学习方法实现了高精度的病害识别，结果表明YOLOv8性能优越。


<details>
  <summary>Details</summary>
Motivation: 植物病害对农业造成严重影响，早期检测至关重要。技术发展为自动化监测提供了可能。

Method: 使用YOLOv7和YOLOv8模型，通过迁移学习在植物叶片图像数据集上进行微调，检测细菌、真菌和病毒病害。

Result: 模型性能指标（mAP、F1-score、Precision、Recall）分别为91.05、89.40、91.22和87.66，YOLOv8表现最佳。

Conclusion: 该系统为植物病害早期检测提供了高效、可扩展的自动化解决方案，有助于提高作物产量和可持续农业。

Abstract: Plant diseases pose significant challenges to farmers and the agricultural
sector at large. However, early detection of plant diseases is crucial to
mitigating their effects and preventing widespread damage, as outbreaks can
severely impact the productivity and quality of crops. With advancements in
technology, there are increasing opportunities for automating the monitoring
and detection of disease outbreaks in plants. This study proposed a system
designed to identify and monitor plant diseases using a transfer learning
approach. Specifically, the study utilizes YOLOv7 and YOLOv8, two
state-ofthe-art models in the field of object detection. By fine-tuning these
models on a dataset of plant leaf images, the system is able to accurately
detect the presence of Bacteria, Fungi and Viral diseases such as Powdery
Mildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's
performance was evaluated using several metrics, including mean Average
Precision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,
89.40, 91.22, and 87.66, respectively. The result demonstrates the superior
effectiveness and efficiency of YOLOv8 compared to other object detection
methods, highlighting its potential for use in modern agricultural practices.
The approach provides a scalable, automated solution for early any plant
disease detection, contributing to enhanced crop yield, reduced reliance on
manual monitoring, and supporting sustainable agricultural practices.

</details>


### [13] [Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics](https://arxiv.org/abs/2507.00153)
*Peter Mortimer,Mirko Maehlisch*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的图像增强方法，用于解决自动驾驶车辆在雪地等分布外环境中的感知性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 学习型感知算法在分布外和代表性不足的环境中性能下降，尤其是户外机器人因光照、季节和天气变化导致视觉场景变化迅速。

Method: 利用基于扩散模型的图像增强方法，通过公共可用的视觉基础模型生成更接近部署环境的训练数据，并结合开放词汇语义分割模型过滤幻觉增强候选。

Result: 该方法能够更精确地控制训练数据中地面语义分布，并针对部署环境微调模型。

Conclusion: 扩散模型图像增强可扩展至雪地以外的多种环境，如沙地和火山地形。

Abstract: The performance of leaning-based perception algorithms suffer when deployed
in out-of-distribution and underrepresented environments. Outdoor robots are
particularly susceptible to rapid changes in visual scene appearance due to
dynamic lighting, seasonality and weather effects that lead to scenes
underrepresented in the training data of the learning-based perception system.
In this conceptual paper, we focus on preparing our autonomous vehicle for
deployment in snow-filled environments. We propose a novel method for
diffusion-based image augmentation to more closely represent the deployment
environment in our training data. Diffusion-based image augmentations rely on
the public availability of vision foundation models learned on internet-scale
datasets. The diffusion-based image augmentations allow us to take control over
the semantic distribution of the ground surfaces in the training data and to
fine-tune our model for its deployment environment. We employ open vocabulary
semantic segmentation models to filter out augmentation candidates that contain
hallucinations. We believe that diffusion-based image augmentations can be
extended to many other environments apart from snow surfaces, like sandy
environments and volcanic terrains.

</details>


### [14] [FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion](https://arxiv.org/abs/2507.00162)
*Yu Lu,Yi Yang*

Main category: cs.CV

TL;DR: FreeLong和FreeLong++是无需训练的框架，通过平衡长视频特征的频率分布，显著提升长视频生成的时间一致性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在生成长视频时存在时间一致性和视觉保真度下降的问题，尤其是高频失真现象。

Method: FreeLong通过融合全局低频特征和局部高频特征；FreeLong++扩展为多分支架构，实现多频段融合。

Result: FreeLong++在无需额外训练的情况下，显著提升了长视频生成的质量，支持多提示和可控生成。

Conclusion: FreeLong++为解决长视频生成中的高频失真问题提供了有效方案，并兼容现有模型。

Abstract: Recent advances in video generation models have enabled high-quality short
video generation from text prompts. However, extending these models to longer
videos remains a significant challenge, primarily due to degraded temporal
consistency and visual fidelity. Our preliminary observations show that naively
applying short-video generation models to longer sequences leads to noticeable
quality degradation. Further analysis identifies a systematic trend where
high-frequency components become increasingly distorted as video length grows,
an issue we term high-frequency distortion. To address this, we propose
FreeLong, a training-free framework designed to balance the frequency
distribution of long video features during the denoising process. FreeLong
achieves this by blending global low-frequency features, which capture holistic
semantics across the full video, with local high-frequency features extracted
from short temporal windows to preserve fine details. Building on this,
FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture
with multiple attention branches, each operating at a distinct temporal scale.
By arranging multiple window sizes from global to local, FreeLong++ enables
multi-band frequency fusion from low to high frequencies, ensuring both
semantic continuity and fine-grained motion dynamics across longer video
sequences. Without any additional training, FreeLong++ can be plugged into
existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer
videos with substantially improved temporal consistency and visual fidelity. We
demonstrate that our approach outperforms previous methods on longer video
generation tasks (e.g. 4x and 8x of native length). It also supports coherent
multi-prompt video generation with smooth scene transitions and enables
controllable video generation using long depth or pose sequences.

</details>


### [15] [SelvaBox: A high-resolution dataset for tropical tree crown detection](https://arxiv.org/abs/2507.00170)
*Hugo Baudchon,Arthur Ouaknine,Martin Weiss,Mélisande Teng,Thomas R. Walla,Antoine Caron-Guay,Christopher Pal,Etienne Laliberté*

Main category: cs.CV

TL;DR: SelvaBox是一个用于热带树冠检测的最大开放数据集，包含83,000多个标记树冠，显著提升了检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 热带森林树冠检测对研究生态系统至关重要，但现有数据集稀缺，限制了模型开发。

Method: 引入SelvaBox数据集，并在高分辨率无人机图像上进行标注，结合多分辨率管道训练检测模型。

Result: 高分辨率输入提升检测精度，SelvaBox训练的模型在未见数据集上表现优异，联合训练后检测器排名领先。

Conclusion: SelvaBox填补了热带树冠检测数据集的空白，为相关研究提供了重要资源。

Abstract: Detecting individual tree crowns in tropical forests is essential to study
these complex and crucial ecosystems impacted by human interventions and
climate change. However, tropical crowns vary widely in size, structure, and
pattern and are largely overlapping and intertwined, requiring advanced remote
sensing methods applied to high-resolution imagery. Despite growing interest in
tropical tree crown detection, annotated datasets remain scarce, hindering
robust model development. We introduce SelvaBox, the largest open-access
dataset for tropical tree crown detection in high-resolution drone imagery. It
spans three countries and contains more than 83,000 manually labeled crowns -
an order of magnitude larger than all previous tropical forest datasets
combined. Extensive benchmarks on SelvaBox reveal two key findings: (1)
higher-resolution inputs consistently boost detection accuracy; and (2) models
trained exclusively on SelvaBox achieve competitive zero-shot detection
performance on unseen tropical tree crown datasets, matching or exceeding
competing methods. Furthermore, jointly training on SelvaBox and three other
datasets at resolutions from 3 to 10 cm per pixel within a unified
multi-resolution pipeline yields a detector ranking first or second across all
evaluated datasets. Our dataset, code, and pre-trained weights are made public.

</details>


### [16] [Graph-Based Deep Learning for Component Segmentation of Maize Plants](https://arxiv.org/abs/2507.00182)
*J. I. Ruíz,A. Méndez,E. Rodríguez*

Main category: cs.CV

TL;DR: 提出了一种基于图神经网络（GNN）和主成分分析（PCA）的深度学习架构，用于从LiDAR 3D点云数据中识别植物组件，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 传统方法在3D数据处理和植物组件识别上存在不足，需要更高效的解决方案。

Method: 采用GNN和PCA增强特征，利用KNN建立点云图的边结构，结合Edge-Conv和GAT进行分类。

Result: 模型在IoU平均指标上超过80%，优于现有基于点云的模型。

Conclusion: 图神经网络方法在植物组件识别任务中表现出色，为精准农业提供了有效工具。

Abstract: In precision agriculture, one of the most important tasks when exploring crop
production is identifying individual plant components. There are several
attempts to accomplish this task by the use of traditional 2D imaging, 3D
reconstructions, and Convolutional Neural Networks (CNN). However, they have
several drawbacks when processing 3D data and identifying individual plant
components. Therefore, in this work, we propose a novel Deep Learning
architecture to detect components of individual plants on Light Detection and
Ranging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on
the concept of Graph Neural Networks (GNN), and feature enhancing with
Principal Component Analysis (PCA). For this, each point is taken as a vertex
and by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,
thus representing the 3D PC data set. Subsequently, Edge-Conv layers are used
to further increase the features of each point. Finally, Graph Attention
Networks (GAT) are applied to classify visible phenotypic components of the
plant, such as the leaf, stem, and soil. This study demonstrates that our
graph-based deep learning approach enhances segmentation accuracy for
identifying individual plant components, achieving percentages above 80% in the
IoU average, thus outperforming other existing models based on point clouds.

</details>


### [17] [Computer Vision for Objects used in Group Work: Challenges and Opportunities](https://arxiv.org/abs/2507.00224)
*Changsoo Jung,Sheikh Mannan,Jack Fitzgerald,Nathaniel Blanchard*

Main category: cs.CV

TL;DR: 论文提出FiboSB数据集，用于解决协作任务中6D姿态估计的挑战，并通过改进YOLO11-x提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统在捕捉学生与物理对象的真实互动时存在不足，6D姿态估计可解决这一问题。

Method: 引入FiboSB数据集，评估四种6D姿态估计方法，并改进YOLO11-x。

Result: YOLO11-x在FiboSB上达到mAP_50为0.898，优于其他方法。

Conclusion: FiboSB数据集和YOLO11-x的改进为复杂协作场景中的6D姿态估计奠定了基础。

Abstract: Interactive and spatially aware technologies are transforming educational
frameworks, particularly in K-12 settings where hands-on exploration fosters
deeper conceptual understanding. However, during collaborative tasks, existing
systems often lack the ability to accurately capture real-world interactions
between students and physical objects. This issue could be addressed with
automatic 6D pose estimation, i.e., estimation of an object's position and
orientation in 3D space from RGB images or videos. For collaborative groups
that interact with physical objects, 6D pose estimates allow AI systems to
relate objects and entities. As part of this work, we introduce FiboSB, a novel
and challenging 6D pose video dataset featuring groups of three participants
solving an interactive task featuring small hand-held cubes and a weight scale.
This setup poses unique challenges for 6D pose because groups are holistically
recorded from a distance in order to capture all participants -- this, coupled
with the small size of the cubes, makes 6D pose estimation inherently
non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on
FiboSB, exposing the limitations of current algorithms on collaborative group
work. An error analysis of these methods reveals that the 6D pose methods'
object detection modules fail. We address this by fine-tuning YOLO11-x for
FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,
and analysis of YOLO11-x errors presented here lay the groundwork for
leveraging the estimation of 6D poses in difficult collaborative contexts.

</details>


### [18] [VOCAL: Visual Odometry via ContrAstive Learning](https://arxiv.org/abs/2507.00243)
*Chi-Yao Huang,Zeel Bhatt,Yezhou Yang*

Main category: cs.CV

TL;DR: VOCAL是一种基于对比学习的视觉里程计框架，通过将VO问题转化为标签排序任务，结合贝叶斯推理和表示学习，提升了特征的可解释性和多模态兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有学习型VO方法依赖刚性几何假设，缺乏可解释性和理论支持，VOCAL旨在解决这些问题。

Method: VOCAL将VO视为标签排序任务，结合贝叶斯推理和表示学习，使相似相机状态在潜在空间中形成一致表示。

Result: 在KITTI数据集上的实验表明，VOCAL显著提升了可解释性和灵活性。

Conclusion: VOCAL推动了视觉里程计向更通用和可解释的空间智能发展。

Abstract: Breakthroughs in visual odometry (VO) have fundamentally reshaped the
landscape of robotics, enabling ultra-precise camera state estimation that is
crucial for modern autonomous systems. Despite these advances, many
learning-based VO techniques rely on rigid geometric assumptions, which often
fall short in interpretability and lack a solid theoretical basis within fully
data-driven frameworks. To overcome these limitations, we introduce VOCAL
(Visual Odometry via ContrAstive Learning), a novel framework that reimagines
VO as a label ranking challenge. By integrating Bayesian inference with a
representation learning framework, VOCAL organizes visual features to mirror
camera states. The ranking mechanism compels similar camera states to converge
into consistent and spatially coherent representations within the latent space.
This strategic alignment not only bolsters the interpretability of the learned
features but also ensures compatibility with multimodal data sources. Extensive
evaluations on the KITTI dataset highlight VOCAL's enhanced interpretability
and flexibility, pushing VO toward more general and explainable spatial
intelligence.

</details>


### [19] [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
*Nikita Nikitin,Eugene Fomin*

Main category: cs.CV

TL;DR: 提出了一种基于轻量级DNN的实时手语识别框架，解决了数据稀缺、高计算成本和帧率差异等问题，实现了高精度和低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决手语识别中的数据稀缺、高计算成本和训练与推理环境帧率差异等关键挑战。

Method: 通过编码手语特定参数（如手形、手掌方向、动作和位置）为向量化输入，利用MediaPipe提取关键点，设计轻量级DNN架构（小于10MB）。

Result: 模型在343个手语分类中达到92%的准确率，延迟低于10毫秒，并集成到'slait ai'应用中。

Conclusion: 该框架在边缘设备上实现了高效、准确的实时手语识别。

Abstract: We present a novel framework for real-time sign language recognition using
lightweight DNNs trained on limited data. Our system addresses key challenges
in sign language recognition, including data scarcity, high computational
costs, and discrepancies in frame rates between training and inference
environments. By encoding sign language specific parameters, such as handshape,
palm orientation, movement, and location into vectorized inputs, and leveraging
MediaPipe for landmark extraction, we achieve highly separable input data
representations. Our DNN architecture, optimized for sub 10MB deployment,
enables accurate classification of 343 signs with less than 10ms latency on
edge devices. The data annotation platform 'slait data' facilitates structured
labeling and vector extraction. Our model achieved 92% accuracy in isolated
sign recognition and has been integrated into the 'slait ai' web application,
where it demonstrates stable inference.

</details>


### [20] [GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception](https://arxiv.org/abs/2507.00253)
*Zhuangzhuang Dai,Vincent Gbouna Zakka,Luis J. Manso,Chen Li*

Main category: cs.CV

TL;DR: 提出GazeTarget360系统，用于从图像中估计360度视线目标，适用于真实场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法有效预测视线目标的问题，特别是在视线离开相机时。

Method: 结合条件推理引擎（眼接触检测器、预训练视觉编码器和多尺度融合解码器）。

Result: 在未见场景中能准确预测视线目标，高效且可部署。

Conclusion: GazeTarget360是首个能从真实相机画面预测视线目标的系统。

Abstract: Enabling robots to understand human gaze target is a crucial step to allow
capabilities in downstream tasks, for example, attention estimation and
movement anticipation in real-world human-robot interactions. Prior works have
addressed the in-frame target localization problem with data-driven approaches
by carefully removing out-of-frame samples. Vision-based gaze estimation
methods, such as OpenFace, do not effectively absorb background information in
images and cannot predict gaze target in situations where subjects look away
from the camera. In this work, we propose a system to address the problem of
360-degree gaze target estimation from an image in generalized visual scenes.
The system, named GazeTarget360, integrates conditional inference engines of an
eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion
decoder. Cross validation results show that GazeTarget360 can produce accurate
and reliable gaze target predictions in unseen scenarios. This makes a
first-of-its-kind system to predict gaze targets from realistic camera footage
which is highly efficient and deployable. Our source code is made publicly
available at: https://github.com/zdai257/DisengageNet.

</details>


### [21] [Self-Supervised Multiview Xray Matching](https://arxiv.org/abs/2507.00287)
*Mohamad Dabboussi,Malo Huard,Yann Gousseau,Pietro Gori*

Main category: cs.CV

TL;DR: 提出一种自监督方法，通过生成多视图X射线之间的对应矩阵，提升骨折检测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法在多视图X射线对应关系上表现不足，影响临床诊断准确性。

Method: 使用数字重建X射线（DRR）生成对应矩阵，结合Transformer训练多视图对应关系。

Result: 在合成和真实X射线数据上验证，多视图对应关系提升了骨折分类性能。

Conclusion: 自监督学习多视图对应关系可作为预训练策略，提升真实数据的骨折检测效果。

Abstract: Accurate interpretation of multi-view radiographs is crucial for diagnosing
fractures, muscular injuries, and other anomalies. While significant advances
have been made in AI-based analysis of single images, current methods often
struggle to establish robust correspondences between different X-ray views, an
essential capability for precise clinical evaluations. In this work, we present
a novel self-supervised pipeline that eliminates the need for manual annotation
by automatically generating a many-to-many correspondence matrix between
synthetic X-ray views. This is achieved using digitally reconstructed
radiographs (DRR), which are automatically derived from unannotated CT volumes.
Our approach incorporates a transformer-based training phase to accurately
predict correspondences across two or more X-ray views. Furthermore, we
demonstrate that learning correspondences among synthetic X-ray views can be
leveraged as a pretraining strategy to enhance automatic multi-view fracture
detection on real data. Extensive evaluations on both synthetic and real X-ray
datasets show that incorporating correspondences improves performance in
multi-view fracture classification.

</details>


### [22] [Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters](https://arxiv.org/abs/2507.00792)
*Hendric Voss,Stefan Kopp*

Main category: cs.CV

TL;DR: 本文提出了一种基于TensorFlow的实时逆向运动学（IK）求解器，用于生成逼真的人体运动，解决了多约束问题中的误差累积和关节限制等挑战。


<details>
  <summary>Details</summary>
Motivation: 在计算机图形学、虚拟环境、机器人和生物力学等领域，生成准确且逼真的虚拟人体运动至关重要。

Method: 利用TensorFlow的自动微分和即时编译技术，将正向和逆向运动学视为可微分操作，处理高自由度的人体骨骼模型。

Result: 实验表明，该求解器在SMPLX人体骨骼模型上表现优异，相比现有方法（如CCD、FABRIK和IPOPT）具有更快的收敛速度、更低的计算开销和更高的成功率。

Conclusion: 该方法在实时性和逼真性上优于现有技术，代码已开源。

Abstract: Generating accurate and realistic virtual human movements in real-time is of
high importance for a variety of applications in computer graphics, interactive
virtual environments, robotics, and biomechanics. This paper introduces a novel
real-time inverse kinematics (IK) solver specifically designed for realistic
human-like movement generation. Leveraging the automatic differentiation and
just-in-time compilation of TensorFlow, the proposed solver efficiently handles
complex articulated human skeletons with high degrees of freedom. By treating
forward and inverse kinematics as differentiable operations, our method
effectively addresses common challenges such as error accumulation and
complicated joint limits in multi-constrained problems, which are critical for
realistic human motion modeling. We demonstrate the solver's effectiveness on
the SMPLX human skeleton model, evaluating its performance against widely used
iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,
and the nonlinear optimization algorithm IPOPT. Our experiments cover both
simple end-effector tasks and sophisticated, multi-constrained problems with
realistic joint limits. Results indicate that our IK solver achieves real-time
performance, exhibiting rapid convergence, minimal computational overhead per
iteration, and improved success rates compared to existing methods. The project
code is available at https://github.com/hvoss-techfak/TF-JAX-IK

</details>


### [23] [Reducing Variability of Multiple Instance Learning Methods for Digital Pathology](https://arxiv.org/abs/2507.00292)
*Ali Mammadov,Loïc Le Folgoc,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TL;DR: 提出了一种多保真度模型融合策略，以减少多实例学习（MIL）方法在数字病理学中的性能波动，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 数字病理学中的全切片图像（WSI）分类面临深度学习的挑战，MIL方法虽适用但性能波动大，难以可靠比较不同方法。

Method: 通过训练多个模型并基于验证分数平均最稳定和有前景的模型，提出了一种多保真度模型融合策略。

Result: 在2个数据集、3种初始化策略和5种MIL方法上进行了2000多次实验，验证了该策略能减少性能波动并提高可重复性。

Conclusion: 该策略适用于任何现有MIL模型，简化了超参数调优，同时保持计算效率。

Abstract: Digital pathology has revolutionized the field by enabling the digitization
of tissue samples into whole slide images (WSIs). However, the high resolution
and large size of WSIs present significant challenges when it comes to applying
Deep Learning models. As a solution, WSIs are often divided into smaller
patches with a global label (\textit{i.e., diagnostic}) per slide, instead of a
(too) costly pixel-wise annotation. By treating each slide as a bag of patches,
Multiple Instance Learning (MIL) methods have emerged as a suitable solution
for WSI classification. A major drawback of MIL methods is their high
variability in performance across different runs, which can reach up to 10-15
AUC points on the test set, making it difficult to compare different MIL
methods reliably. This variability mainly comes from three factors: i) weight
initialization, ii) batch (shuffling) ordering, iii) and learning rate. To
address that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL
methods. We first train multiple models for a few epochs and average the most
stable and promising ones based on validation scores. This approach can be
applied to any existing MIL model to reduce performance variability. It also
simplifies hyperparameter tuning and improves reproducibility while maintaining
computational efficiency. We extensively validate our approach on WSI
classification tasks using 2 different datasets, 3 initialization strategies
and 5 MIL methods, for a total of more than 2000 experiments.

</details>


### [24] [Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes](https://arxiv.org/abs/2507.00327)
*Chuyan Zhang,Kefan Wang,Yun Gu*

Main category: cs.CV

TL;DR: SR-LoRA利用预训练权重矩阵的稳定秩作为层间秩分配的先验，提升适应性，无需额外搜索成本，在领域差距大的任务中表现优于现有自适应LoRA方法。


<details>
  <summary>Details</summary>
Motivation: 解决固定低秩结构在领域差距大时适应性不足的问题，避免现有方法依赖计算密集型技术。

Method: 利用预训练权重矩阵的稳定秩指导层间秩分配，实现高效适应性调整。

Result: 在领域差距大的少样本任务中，SR-LoRA性能优于现有自适应LoRA方法，且效率更高。

Conclusion: SR-LoRA通过稳定秩指导的秩分配，实现了性能与效率的更好平衡。

Abstract: Low-Rank Adaptation (LoRA) has proven effective in reducing computational
costs while maintaining performance comparable to fully fine-tuned foundation
models across various tasks. However, its fixed low-rank structure restricts
its adaptability in scenarios with substantial domain gaps, where higher ranks
are often required to capture domain-specific complexities. Current adaptive
LoRA methods attempt to overcome this limitation by dynamically expanding or
selectively allocating ranks, but these approaches frequently depend on
computationally intensive techniques such as iterative pruning, rank searches,
or additional regularization. To address these challenges, we introduce Stable
Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the
stable rank of pre-trained weight matrices as a natural prior for layer-wise
rank allocation. By leveraging the stable rank, which reflects the intrinsic
dimensionality of the weights, SR-LoRA enables a principled and efficient
redistribution of ranks across layers, enhancing adaptability without incurring
additional search costs. Empirical evaluations on few-shot tasks with
significant domain gaps show that SR-LoRA consistently outperforms recent
adaptive LoRA variants, achieving a superior trade-off between performance and
efficiency. Our code is available at
https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.

</details>


### [25] [MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms](https://arxiv.org/abs/2507.00328)
*Xuan Liu,Yinhao Ren,Marc D. Ryser,Lars J. Grimm,Joseph Y. Lo*

Main category: cs.CV

TL;DR: MammoTracker是一个基于掩模引导的乳腺病灶跟踪框架，通过全局搜索、局部搜索和分数细化模块实现病灶自动定位，并在新数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 乳腺病灶在时间序列乳腺X光片中的准确跟踪对乳腺癌进展监测和早期诊断至关重要，但现有CAD系统在自动化病灶对应方面存在挑战。

Method: 采用粗到细策略，结合全局搜索、局部搜索和分数细化三个模块，并引入新数据集支持训练与评估。

Result: MammoTracker在平均重叠率和准确率上分别达到0.455和0.509，优于基线模型8%。

Conclusion: MammoTracker展示了提升CAD病灶进展分析的潜力，相关数据集将公开。

Abstract: Accurate lesion tracking in temporal mammograms is essential for monitoring
breast cancer progression and facilitating early diagnosis. However, automated
lesion correspondence across exams remains a challenges in computer-aided
diagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,
a mask-guided lesion tracking framework that automates lesion localization
across consecutively exams. Our approach follows a coarse-to-fine strategy
incorporating three key modules: global search, local search, and score
refinement. To support large-scale training and evaluation, we introduce a new
dataset with curated prior-exam annotations for 730 mass and calcification
cases from the public EMBED mammogram dataset, yielding over 20000 lesion
pairs, making it the largest known resource for temporal lesion tracking in
mammograms. Experimental results demonstrate that MammoTracker achieves 0.455
average overlap and 0.509 accuracy, surpassing baseline models by 8%,
highlighting its potential to enhance CAD-based lesion progression analysis.
Our dataset will be available at
https://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.

</details>


### [26] [Populate-A-Scene: Affordance-Aware Human Video Generation](https://arxiv.org/abs/2507.00334)
*Mengyi Shan,Zecheng He,Haoyu Ma,Felix Juefei-Xu,Peizhao Zhang,Tingbo Hou,Ching-Yao Chuang*

Main category: cs.CV

TL;DR: 研究探讨了文本到视频模型能否通过预测人与环境互动来感知场景功能，无需显式条件即可生成符合场景功能的人类行为视频。


<details>
  <summary>Details</summary>
Motivation: 探索文本到视频模型在感知场景功能（affordance）方面的潜力，使其能根据单张场景图像生成符合人类行为逻辑的视频。

Method: 通过微调模型，在给定场景图像和动作描述的情况下，插入符合场景功能的人类行为，并分析跨注意力热图以揭示模型的固有功能感知能力。

Result: 研究表明，无需显式条件（如边界框或姿态），预训练视频模型能够感知场景功能并生成符合逻辑的人类行为视频。

Conclusion: 文本到视频模型具备作为交互式世界模拟器的潜力，能够通过单张图像推断场景功能并生成合理的人类行为视频。

Abstract: Can a video generation model be repurposed as an interactive world simulator?
We explore the affordance perception potential of text-to-video models by
teaching them to predict human-environment interaction. Given a scene image and
a prompt describing human actions, we fine-tune the model to insert a person
into the scene, while ensuring coherent behavior, appearance, harmonization,
and scene affordance. Unlike prior work, we infer human affordance for video
generation (i.e., where to insert a person and how they should behave) from a
single scene image, without explicit conditions like bounding boxes or body
poses. An in-depth study of cross-attention heatmaps demonstrates that we can
uncover the inherent affordance perception of a pre-trained video model without
labeled affordance datasets.

</details>


### [27] [Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video](https://arxiv.org/abs/2507.00339)
*Alexander Moore,Amar Saini,Kylie Cancilla,Doug Poland,Carmen Carrano*

Main category: cs.CV

TL;DR: MOVi-MC-AC是一个多摄像头视角下的模态分割和内容补全数据集，首次提供真实遮挡内容标注，包含580万对象实例。


<details>
  <summary>Details</summary>
Motivation: 现有数据缺乏多摄像头共享场景的上下文信息，MOVi-MC-AC填补了这一空白，支持更复杂的计算机视觉任务。

Method: 通过模拟多摄像头视频场景，提供一致的物体ID和模态内容标注，解决遮挡问题。

Result: 数据集包含580万对象实例，是当前最大的模态分割数据集，并首次提供真实遮挡内容标注。

Conclusion: MOVi-MC-AC为计算机视觉领域提供了新的研究资源，支持多摄像头视角和遮挡内容预测任务。

Abstract: Amodal segmentation and amodal content completion require using object priors
to estimate occluded masks and features of objects in complex scenes. Until
now, no data has provided an additional dimension for object context: the
possibility of multiple cameras sharing a view of a scene. We introduce
MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the
largest amodal segmentation and first amodal content dataset to date. Cluttered
scenes of generic household objects are simulated in multi-camera video.
MOVi-MC-AC contributes to the growing literature of object detection, tracking,
and segmentation by including two new contributions to the deep learning for
computer vision world. Multiple Camera (MC) settings where objects can be
identified and tracked between various unique camera perspectives are rare in
both synthetic and real-world video. We introduce a new complexity to synthetic
video by providing consistent object ids for detections and segmentations
between both frames and multiple cameras each with unique features and motion
patterns on a single scene. Amodal Content (AC) is a reconstructive task in
which models predict the appearance of target objects through occlusions. In
the amodal segmentation literature, some datasets have been released with
amodal detection, tracking, and segmentation labels. While other methods rely
on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do
not account for natural occlusions present in the modal masks. MOVi-MC-AC
provides labels for ~5.8 million object instances, setting a new maximum in the
amodal dataset literature, along with being the first to provide ground-truth
amodal content. The full dataset is available at
https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,

</details>


### [28] [CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation](https://arxiv.org/abs/2507.00356)
*Zhiwei Yi,Xin Cheng,Jingyu Ma,Ruifei Zhu,Junwei Tian,Yuanxiu Zhou,Xinge Zhao,Hongzhe Li*

Main category: cs.CV

TL;DR: 该研究提出了CGEarthEye框架，针对吉林一号卫星特性设计，包含五个参数规模不同的主干网络，总参数达21亿。通过构建JLSSD数据集和多种对比学习策略，实现了在10个基准数据集上的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率光学遥感影像获取渠道有限，限制了高分辨率遥感视觉基础模型（RSVFM）的发展。吉林一号卫星作为全球最大的亚米级商业遥感卫星星座，拥有丰富的亚米级影像资源，为研究提供了数据基础。

Method: 研究设计了CGEarthEye框架，包含五个主干网络，并构建了JLSSD数据集（全球覆盖、季度时间采样）。采用季节性对比、增强对比和掩码补丁标记对比策略进行预训练。

Result: CGEarthEye在10个基准数据集上实现了SOTA性能，并在特征可视化、模型收敛、参数效率和实际应用中表现出优越特性。

Conclusion: CGEarthEye的卓越表征能力有望推动吉林一号数据在传统地球观测应用中的更广泛和高效使用。

Abstract: Deep learning methods have significantly advanced the development of
intelligent rinterpretation in remote sensing (RS), with foundational model
research based on large-scale pre-training paradigms rapidly reshaping various
domains of Earth Observation (EO). However, compared to the open accessibility
and high spatiotemporal coverage of medium-resolution data, the limited
acquisition channels for ultra-high-resolution optical RS imagery have
constrained the progress of high-resolution remote sensing vision foundation
models (RSVFM). As the world's largest sub-meter-level commercial RS satellite
constellation, the Jilin-1 constellation possesses abundant sub-meter-level
image resources. This study proposes CGEarthEye, a RSVFM framework specifically
designed for Jilin-1 satellite characteristics, comprising five backbones with
different parameter scales with totaling 2.1 billion parameters. To enhance the
representational capacity of the foundation model, we developed JLSSD, the
first 15-million-scale multi-temporal self-supervised learning (SSL) dataset
featuring global coverage with quarterly temporal sampling within a single
year, constructed through multi-level representation clustering and sampling
strategies. The framework integrates seasonal contrast, augmentation-based
contrast, and masked patch token contrastive strategies for pre-training.
Comprehensive evaluations across 10 benchmark datasets covering four typical RS
tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art
(SOTA) performance. Further analysis reveals CGEarthEye's superior
characteristics in feature visualization, model convergence, parameter
efficiency, and practical mapping applications. This study anticipates that the
exceptional representation capabilities of CGEarthEye will facilitate broader
and more efficient applications of Jilin-1 data in traditional EO application.

</details>


### [29] [GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control](https://arxiv.org/abs/2507.00363)
*Xingjun Wang,Lianlei Shan*

Main category: cs.CV

TL;DR: 提出了一种改进3D高斯泼溅（3DGS）的方法，解决了初始化、优化和密度控制的挑战。


<details>
  <summary>Details</summary>
Motivation: 3DGS因其显式的高斯表示支持实时渲染，但依赖精确初始化且难以优化无序高斯分布为有序表面，缺乏自适应密度控制机制。

Method: 1. 几何引导的初始化预测高斯参数；2. 表面对齐的优化策略；3. 动态自适应密度控制机制。

Result: 实现了高保真实时渲染，视觉质量显著提升，在复杂场景中表现优异。

Conclusion: 方法在实时渲染中达到或超越现有技术，生成高保真图像。

Abstract: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.

</details>


### [30] [Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation](https://arxiv.org/abs/2507.00752)
*Hao Xing,Kai Zhe Boey,Yuankai Wu,Darius Burschka,Gordon Cheng*

Main category: cs.CV

TL;DR: 提出了一种多模态图卷积网络（MMGCN），通过结合低帧率视觉数据和高帧率运动数据，减少动作分割中的过分割错误。


<details>
  <summary>Details</summary>
Motivation: 在协作场景中，精确的时间动作分割对机器人至关重要，但现有方法因噪声导致过分割问题。

Method: 采用正弦编码策略增强空间表示，通过时间图融合模块对齐多模态数据，并使用SmoothLabelMix数据增强技术提升时间一致性。

Result: 在Bimanual Actions Dataset上表现优异，F1@10达94.5%，F1@25达92.8%。

Conclusion: MMGCN有效提升了动作分割的准确性和时间一致性，优于现有方法。

Abstract: Accurate temporal segmentation of human actions is critical for intelligent
robots in collaborative settings, where a precise understanding of sub-activity
labels and their temporal structure is essential. However, the inherent noise
in both human pose estimation and object detection often leads to
over-segmentation errors, disrupting the coherence of action sequences. To
address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that
integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,
30 fps) motion data (skeleton and object detections) to mitigate fragmentation.
Our framework introduces three key contributions. First, a sinusoidal encoding
strategy that maps 3D skeleton coordinates into a continuous sin-cos space to
enhance spatial representation robustness. Second, a temporal graph fusion
module that aligns multi-modal inputs with differing resolutions via
hierarchical feature aggregation, Third, inspired by the smooth transitions
inherent to human actions, we design SmoothLabelMix, a data augmentation
technique that mixes input sequences and labels to generate synthetic training
examples with gradual action transitions, enhancing temporal consistency in
predictions and reducing over-segmentation artifacts.
  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for
human-object interaction understanding, demonstrate that our approach
outperforms state-of-the-art methods, especially in action segmentation
accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.

</details>


### [31] [An Improved U-Net Model for Offline handwriting signature denoising](https://arxiv.org/abs/2507.00365)
*Wanghui Xiao*

Main category: cs.CV

TL;DR: 提出了一种基于改进U-net结构的签名去噪模型，通过离散小波变换和PCA变换增强去噪能力，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 手写签名在身份识别中广泛应用，但签名样本常受干扰信息影响，给识别工作带来挑战。

Method: 采用改进的U-net结构，结合离散小波变换和PCA变换，提升去噪效果。

Result: 模型在去噪效果上显著优于传统方法，提高了签名图像的清晰度和可读性。

Conclusion: 该模型为签名分析与识别提供了更可靠的技术支持。

Abstract: Handwriting signatures, as an important means of identity recognition, are
widely used in multiple fields such as financial transactions, commercial
contracts and personal affairs due to their legal effect and uniqueness. In
forensic science appraisals, the analysis of offline handwriting signatures
requires the appraiser to provide a certain number of signature samples, which
are usually derived from various historical contracts or archival materials.
However, the provided handwriting samples are often mixed with a large amount
of interfering information, which brings severe challenges to handwriting
identification work. This study proposes a signature handwriting denoising
model based on the improved U-net structure, aiming to enhance the robustness
of the signature recognition system. By introducing discrete wavelet transform
and PCA transform, the model's ability to suppress noise has been enhanced. The
experimental results show that this modelis significantly superior to the
traditional methods in denoising effect, can effectively improve the clarity
and readability of the signed images, and provide more reliable technical
support for signature analysis and recognition.

</details>


### [32] [Towards Open-World Human Action Segmentation Using Graph Convolutional Networks](https://arxiv.org/abs/2507.00756)
*Hao Xing,Kai Zhe Boey,Gordon Cheng*

Main category: cs.CV

TL;DR: 论文提出了一种开放世界动作分割框架，通过增强的金字塔图卷积网络、Mixup训练和时序聚类损失，显著提升了开放集分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在封闭世界动作分割表现良好，但难以泛化到开放世界场景，需要无需手动标注的模型来处理新动作。

Method: 提出EPGCN网络、Mixup训练合成数据、时序聚类损失，用于检测和分割未见动作。

Result: 在Bimanual Actions和H2O数据集上，开放集分割和分布外检测性能分别提升16.9%和34.6%。

Conclusion: 框架有效解决了开放世界动作分割问题，各组件通过消融实验验证了其重要性。

Abstract: Human-object interaction segmentation is a fundamental task of daily activity
understanding, which plays a crucial role in applications such as assistive
robotics, healthcare, and autonomous systems. Most existing learning-based
methods excel in closed-world action segmentation, they struggle to generalize
to open-world scenarios where novel actions emerge. Collecting exhaustive
action categories for training is impractical due to the dynamic diversity of
human activities, necessitating models that detect and segment
out-of-distribution actions without manual annotation. To address this issue,
we formally define the open-world action segmentation problem and propose a
structured framework for detecting and segmenting unseen actions. Our framework
introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional
Network (EPGCN) with a novel decoder module for robust spatiotemporal feature
upsampling. 2) Mixup-based training to synthesize out-of-distribution data,
eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss
that groups in-distribution actions while distancing out-of-distribution
samples.
  We evaluate our framework on two challenging human-object interaction
recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets.
Experimental results demonstrate significant improvements over state-of-the-art
action segmentation models across multiple open-set evaluation metrics,
achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and
out-of-distribution detection performances (AUROC), respectively. Additionally,
we conduct an in-depth ablation study to assess the impact of each proposed
component, identifying the optimal framework configuration for open-world
action segmentation.

</details>


### [33] [Out-of-Distribution Detection with Adaptive Top-K Logits Integration](https://arxiv.org/abs/2507.00368)
*Hikaru Shijo,Yutaka Yoshihama,Kenichi Yadani,Norifumi Murata*

Main category: cs.CV

TL;DR: 论文提出了一种新方法ATLI，通过自适应选择有效的top-k logits并结合最大logit，显著提高了OOD检测的性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络对分布外（OOD）样本的预测往往过于自信，检测OOD数据对提升机器学习安全性至关重要。

Method: 提出ATLI方法，自适应确定模型特定的top-k logits，并将最大logit与其他top-k logits结合。

Result: 在ImageNet-1K基准测试中，ATLI将FPR95降低了6.73%（相比MaxLogit）和2.67%（相比其他先进方法）。

Conclusion: ATLI通过利用更多logits信息，显著提升了OOD检测效果。

Abstract: Neural networks often make overconfident predictions from out-of-distribution
(OOD) samples. Detection of OOD data is therefore crucial to improve the safety
of machine learning. The simplest and most powerful method for OOD detection is
MaxLogit, which uses the model's maximum logit to provide an OOD score. We have
discovered that, in addition to the maximum logit, some other logits are also
useful for OOD detection. Based on this finding, we propose a new method called
ATLI (Adaptive Top-k Logits Integration), which adaptively determines effective
top-k logits that are specific to each model and combines the maximum logit
with the other top-k logits. In this study we evaluate our proposed method
using ImageNet-1K benchmark. Extensive experiments showed our proposed method
to reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit
approach, and decreased FPR95 by an additional 2.67% compared to other
state-of-the-art methods.

</details>


### [34] [GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond](https://arxiv.org/abs/2507.00886)
*Anna-Maria Halacheva,Jan-Nico Zaech,Xi Wang,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯点云的场景中心化视觉语言模型（VLM），通过语言和任务感知的场景表示，解决了现有方法对物体检测器的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉语言模型（VLMs）对物体检测器依赖性强，导致处理瓶颈和分类灵活性受限，需要一种更高效的方法。

Method: 采用语言和任务感知的场景表示，将语言特征直接嵌入3D高斯点云中，并通过双稀疏化器生成紧凑的任务相关令牌。

Result: 该方法显著提升了性能，在域外设置下将现有3D VLM的性能提高了五倍。

Conclusion: 提出的高斯点云VLM在3D场景理解中表现出色，具有强泛化能力。

Abstract: As multimodal language models advance, their application to 3D scene
understanding is a fast-growing frontier, driving the development of 3D
Vision-Language Models (VLMs). Current methods show strong dependence on object
detectors, introducing processing bottlenecks and limitations in taxonomic
flexibility. To address these limitations, we propose a scene-centric 3D VLM
for 3D Gaussian splat scenes that employs language- and task-aware scene
representations. Our approach directly embeds rich linguistic features into the
3D scene representation by associating language with each Gaussian primitive,
achieving early modality alignment. To process the resulting dense
representations, we introduce a dual sparsifier that distills them into
compact, task-relevant tokens via task-guided and location-guided pathways,
producing sparse, task-aware global and local scene tokens. Notably, we present
the first Gaussian splatting-based VLM, leveraging photorealistic 3D
representations derived from standard RGB images, demonstrating strong
generalization: it improves performance of prior 3D VLM five folds, in
out-of-the-domain settings.

</details>


### [35] [PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching](https://arxiv.org/abs/2507.00371)
*Xin Yang,Ruiming Du,Hanyang Huang,Jiayang Xie,Pengyao Xie,Leisen Fang,Ziyue Guo,Nanjun Jiang,Yu Jiang,Haiyan Cen*

Main category: cs.CV

TL;DR: PlantSegNeRF提出了一种新方法，通过多视角RGB图像序列直接生成高精度植物器官点云，显著提升了分割精度和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有植物点云器官分割技术在分辨率、精度和跨物种通用性上存在局限，需要一种更高效、高精度的解决方案。

Method: PlantSegNeRF结合2D实例分割、实例匹配模块和实例NeRF，从多视角图像生成包含语义和实例信息的隐式场景，并转换为点云。

Result: 在语义和实例分割任务中，PlantSegNeRF表现优异，各项指标平均提升显著，尤其在复杂数据集上优势明显。

Conclusion: PlantSegNeRF为植物表型研究提供了高质量3D数据，支持大规模模型开发。

Abstract: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.

</details>


### [36] [Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur](https://arxiv.org/abs/2507.00372)
*Xinge Yang,Chuong Nguyen,Wenbin Wang,Kaizhang Kang,Wolfgang Heidrich,Xiaoxing Li*

Main category: cs.CV

TL;DR: 提出一种高效、可扩展的数据集合成方法，解决大光圈相机因浅景深导致的模糊问题，无需依赖真实数据微调。


<details>
  <summary>Details</summary>
Motivation: 大光圈相机因浅景深导致焦外物体模糊，固定焦距相机（如智能眼镜）难以添加自动对焦机制，现有深度学习模型因域差距在真实场景中表现不佳。

Method: 同时建模深度相关的散焦和空间变化的光学像差，解决计算复杂性和高质量RGB-D数据集稀缺问题。

Result: 实验表明，用低分辨率合成图像训练的网络能有效泛化到高分辨率（12MP）真实世界图像。

Conclusion: 提出的方法在解决域差距和数据集稀缺问题上表现出色，适用于多样场景。

Abstract: Modern cameras with large apertures often suffer from a shallow depth of
field, resulting in blurry images of objects outside the focal plane. This
limitation is particularly problematic for fixed-focus cameras, such as those
used in smart glasses, where adding autofocus mechanisms is challenging due to
form factor and power constraints. Due to unmatched optical aberrations and
defocus properties unique to each camera system, deep learning models trained
on existing open-source datasets often face domain gaps and do not perform well
in real-world settings. In this paper, we propose an efficient and scalable
dataset synthesis approach that does not rely on fine-tuning with real-world
data. Our method simultaneously models depth-dependent defocus and spatially
varying optical aberrations, addressing both computational complexity and the
scarcity of high-quality RGB-D datasets. Experimental results demonstrate that
a network trained on our low resolution synthetic images generalizes
effectively to high resolution (12MP) real-world images across diverse scenes.

</details>


### [37] [Customizable ROI-Based Deep Image Compression](https://arxiv.org/abs/2507.00373)
*Ian Jin,Fanxin Xia,Feng Ding,Xinfeng Zhang,Meiqin Liu,Yao Zhao,Weisi Lin,Lili Meng*

Main category: cs.CV

TL;DR: 提出了一种可定制的基于ROI的深度图像压缩方法，通过文本控制ROI定义和可调节的非ROI掩码，优化了图像压缩的灵活性和质量平衡。


<details>
  <summary>Details</summary>
Motivation: 现有ROI图像压缩方法无法满足用户多样化的需求，如自定义ROI和灵活调整ROI与非ROI的质量权衡。

Method: 1. 文本控制掩码获取（TMA）模块，用户通过输入文本自定义ROI；2. 可定制值分配（CVA）机制，调节非ROI掩码程度；3. 潜在掩码注意力（LMA）模块，融合掩码和图像的潜在空间信息优化压缩。

Result: 实验证明该方法有效支持ROI自定义和质量权衡管理。

Conclusion: 该方法为ROI图像压缩提供了灵活性和高效性，满足多样化用户需求。

Abstract: Region of Interest (ROI)-based image compression optimizes bit allocation by
prioritizing ROI for higher-quality reconstruction. However, as the users
(including human clients and downstream machine tasks) become more diverse,
ROI-based image compression needs to be customizable to support various
preferences. For example, different users may define distinct ROI or require
different quality trade-offs between ROI and non-ROI. Existing ROI-based image
compression schemes predefine the ROI, making it unchangeable, and lack
effective mechanisms to balance reconstruction quality between ROI and non-ROI.
This work proposes a paradigm for customizable ROI-based deep image
compression. First, we develop a Text-controlled Mask Acquisition (TMA) module,
which allows users to easily customize their ROI for compression by just
inputting the corresponding semantic \emph{text}. It makes the encoder
controlled by text. Second, we design a Customizable Value Assign (CVA)
mechanism, which masks the non-ROI with a changeable extent decided by users
instead of a constant one to manage the reconstruction quality trade-off
between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)
module, where the latent spatial prior of the mask and the latent
Rate-Distortion Optimization (RDO) prior of the image are extracted and fused
in the latent space, and further used to optimize the latent representation of
the source image. Experimental results demonstrate that our proposed
customizable ROI-based deep image compression paradigm effectively addresses
the needs of customization for ROI definition and mask acquisition as well as
the reconstruction quality trade-off management between the ROI and non-ROI.

</details>


### [38] [MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis](https://arxiv.org/abs/2507.00377)
*Jianhao Xie,Ziang Zhang,Zhenyu Weng,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: MedDiff-FT是一种可控的医学图像生成方法，通过微调扩散基础模型，以数据高效的方式生成具有结构依赖性和领域特异性的医学图像，提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中高质量训练数据稀缺的问题，以及现有扩散模型在医学成像中依赖大规模数据集和高质量图像的限制。

Method: 提出MedDiff-FT方法，包括动态自适应引导掩码、轻量级随机掩码生成器和自动质量评估协议，以生成高质量的医学图像。

Result: 在五个医学分割数据集上，MedDiff-FT的合成图像-掩码对将SOTA方法的分割性能平均提高了1%的Dice分数。

Conclusion: MedDiff-FT在生成质量、多样性和计算效率之间取得了平衡，为医学数据增强提供了实用解决方案。

Abstract: Recent advancements in deep learning for medical image segmentation are often
limited by the scarcity of high-quality training data.While diffusion models
provide a potential solution by generating synthetic images, their
effectiveness in medical imaging remains constrained due to their reliance on
large-scale medical datasets and the need for higher image quality. To address
these challenges, we present MedDiff-FT, a controllable medical image
generation method that fine-tunes a diffusion foundation model to produce
medical images with structural dependency and domain specificity in a
data-efficient manner. During inference, a dynamic adaptive guiding mask
enforces spatial constraints to ensure anatomically coherent synthesis, while a
lightweight stochastic mask generator enhances diversity through hierarchical
randomness injection. Additionally, an automated quality assessment protocol
filters suboptimal outputs using feature-space metrics, followed by mask
corrosion to refine fidelity. Evaluated on five medical segmentation
datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's
segmentation performance by an average of 1% in Dice score. The framework
effectively balances generation quality, diversity, and computational
efficiency, offering a practical solution for medical data augmentation. The
code is available at https://github.com/JianhaoXie1/MedDiff-FT.

</details>


### [39] [Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space](https://arxiv.org/abs/2507.00392)
*Yingping Liang,Yutao Hu,Wenqi Shao,Ying Fu*

Main category: cs.CV

TL;DR: 提出了一种名为Lift to Match (L2M)的两阶段框架，通过将2D图像提升到3D空间，利用大规模单视图图像实现鲁棒的特征匹配。


<details>
  <summary>Details</summary>
Motivation: 现有特征匹配方法依赖稀缺且干净的多视图图像，限制了其在多样化场景中的泛化能力；传统特征编码器基于单视图2D图像训练，难以捕捉3D感知的对应关系。

Method: 第一阶段通过多视图图像合成和3D特征高斯表示学习3D感知特征编码器；第二阶段结合新视图渲染和大规模合成数据训练特征解码器。

Result: 在零样本评估基准上表现出卓越的泛化能力。

Conclusion: L2M框架通过3D感知和合成数据实现了鲁棒的特征匹配，具有广泛的应用潜力。

Abstract: Feature matching plays a fundamental role in many computer vision tasks, yet
existing methods heavily rely on scarce and clean multi-view image collections,
which constrains their generalization to diverse and challenging scenarios.
Moreover, conventional feature encoders are typically trained on single-view 2D
images, limiting their capacity to capture 3D-aware correspondences. In this
paper, we propose a novel two-stage framework that lifts 2D images to 3D space,
named as \textbf{Lift to Match (L2M)}, taking full advantage of large-scale and
diverse single-view images. To be specific, in the first stage, we learn a
3D-aware feature encoder using a combination of multi-view image synthesis and
3D feature Gaussian representation, which injects 3D geometry knowledge into
the encoder. In the second stage, a novel-view rendering strategy, combined
with large-scale synthetic data generation from single-view images, is employed
to learn a feature decoder for robust feature matching, thus achieving
generalization across diverse domains. Extensive experiments demonstrate that
our method achieves superior generalization across zero-shot evaluation
benchmarks, highlighting the effectiveness of the proposed framework for robust
feature matching.

</details>


### [40] [Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains](https://arxiv.org/abs/2507.00401)
*Xin Xu,Eibe Frank,Geoffrey Holmes*

Main category: cs.CV

TL;DR: 论文提出了一种名为“MIV-head”的新方法，用于解决在无法微调主干网络的情况下进行跨域小样本学习的问题。该方法通过多实例验证任务实现高效分类，性能优于现有适配器方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在实际应用中无法微调主干网络时，如何处理低质量静态嵌入的问题，特别是在跨域小样本学习场景下。

Method: 方法是将小样本分类问题转化为多实例验证任务，并设计了一个与主干网络无关的“MIV-head”分类头，通过目标域的少量数据进行训练。

Result: 实验表明，MIV-head在跨域小样本分类任务中表现优异，性能接近部分微调方法，且适应成本更低。

Conclusion: 结论是MIV-head是一种高效、无需微调主干网络的跨域小样本学习方法，性能优于传统分类头方法。

Abstract: We investigate cross-domain few-shot learning under the constraint that
fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible
-- a scenario that is increasingly common in practical use cases. Handling the
low-quality and static embeddings produced by frozen, "black-box" backbones
leads to a problem representation of few-shot classification as a series of
multiple instance verification (MIV) tasks. Inspired by this representation, we
introduce a novel approach to few-shot domain adaptation, named the "MIV-head",
akin to a classification head that is agnostic to any pretrained backbone and
computationally efficient. The core components designed for the MIV-head, when
trained on few-shot data from a target domain, collectively yield strong
performance on test data from that domain. Importantly, it does so without
fine-tuning the backbone, and within the "meta-testing" phase. Experimenting
under various settings and on an extension of the Meta-dataset benchmark for
cross-domain few-shot image classification, using representative off-the-shelf
convolutional neural network and vision transformer backbones pretrained on
ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when
compared to state-of-the-art "adapter" (or partially fine-tuning) methods
applied to the same backbones, while incurring substantially lower adaptation
cost. We also find well-known "classification head" approaches lag far behind
in terms of accuracy. Ablation study empirically justifies the core components
of our approach. We share our code at https://github.com/xxweka/MIV-head.

</details>


### [41] [DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting](https://arxiv.org/abs/2507.00429)
*Jingyi Pan,Dan Xu,Qiong Luo*

Main category: cs.CV

TL;DR: DiGA3D提出了一种统一的3D修复管道，通过扩散模型实现多视图一致的外观和几何修复。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D修复方法在远视图、外观和几何一致性方面的不足。

Method: 采用多参考视图选择、注意力特征传播机制和纹理-几何评分蒸馏采样损失。

Result: 实验证明DiGA3D在多种3D修复任务中表现优异。

Conclusion: DiGA3D为3D修复提供了一种高效且一致的解决方案。

Abstract: Developing a unified pipeline that enables users to remove, re-texture, or
replace objects in a versatile manner is crucial for text-guided 3D inpainting.
However, there are still challenges in performing multiple 3D inpainting tasks
within a unified framework: 1) Single reference inpainting methods lack
robustness when dealing with views that are far from the reference view. 2)
Appearance inconsistency arises when independently inpainting multi-view images
with 2D diffusion priors; 3) Geometry inconsistency limits performance when
there are significant geometric changes in the inpainting regions. To tackle
these challenges, we introduce DiGA3D, a novel and versatile 3D inpainting
pipeline that leverages diffusion models to propagate consistent appearance and
geometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy
for selecting multiple reference views to reduce errors during propagation.
Next, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that
propagates attention features from the selected reference views to other views
via diffusion models to maintain appearance consistency. Furthermore, DiGA3D
introduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to
further improve the geometric consistency of inpainted 3D scenes. Extensive
experiments on multiple 3D inpainting tasks demonstrate the effectiveness of
our method. The project page is available at https://rorisis.github.io/DiGA3D/.

</details>


### [42] [MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2507.00430)
*Huanxin Yang,Qiwen Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合频域分析的手写数学表达式识别方法（MFH），利用离散余弦变换（DCT）提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 手写数学表达式识别（HMER）因复杂的公式结构和字符布局而面临挑战，频域信息可能提供结构分析辅助。

Method: 提出MFH方法，将频域分析与HMER结合，利用DCT提取频域信息。

Result: 在多个基准模型上实现性能提升，MFH-CoMER在CROHME 2014/2016/2019测试集上分别达到61.66%/62.07%/63.72%的准确率。

Conclusion: 频域信息对数学公式识别有效，MFH方法显著提升了识别性能。

Abstract: Handwritten mathematical expression recognition (HMER) suffers from complex
formula structures and character layouts in sequence prediction. In this paper,
we incorporate frequency domain analysis into HMER and propose a method that
marries frequency domain with HMER (MFH), leveraging the discrete cosine
transform (DCT). We emphasize the structural analysis assistance of frequency
information for recognizing mathematical formulas. When implemented on various
baseline models, our network exhibits a consistent performance enhancement,
demonstrating the efficacy of frequency domain information. Experiments show
that our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on
the CROHME 2014/2016/2019 test sets. The source code is available at
https://github.com/Hryxyhe/MFH.

</details>


### [43] [Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration](https://arxiv.org/abs/2507.00447)
*Xin Luo,Menglin Zhang,Yunwei Lan,Tianyu Zhang,Rui Li,Chang Liu,Dong Liu*

Main category: cs.CV

TL;DR: Latent-PMRF通过将PMRF重构在VAE的潜在空间中，优化了感知-失真平衡，显著提升了人脸恢复的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统PMRF在像素空间建模，难以与人类感知对齐，因此提出Latent-PMRF以更好地优化感知质量。

Method: 在VAE的潜在空间中重新定义PMRF，利用潜在表示的最小失真估计，并通过VAE的重构误差限制失真。

Result: Latent-PMRF在盲人脸恢复中表现优异，实现了比现有方法更好的感知-失真平衡，且收敛效率显著提升（5.79倍加速）。

Conclusion: Latent-PMRF通过潜在空间建模有效解决了感知-失真平衡问题，并展示了VAE设计的重要性。

Abstract: The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face
restoration algorithms must balance perceptual quality and fidelity. To achieve
minimal distortion while maintaining perfect perceptual quality, Posterior-Mean
Rectified Flow (PMRF) proposes a flow based approach where source distribution
is minimum distortion estimations. Although PMRF is shown to be effective, its
pixel-space modeling approach limits its ability to align with human
perception, where human perception is defined as how humans distinguish between
two image distributions. In this work, we propose Latent-PMRF, which
reformulates PMRF in the latent space of a variational autoencoder (VAE),
facilitating better alignment with human perception during optimization. By
defining the source distribution on latent representations of minimum
distortion estimation, we bound the minimum distortion by the VAE's
reconstruction error. Moreover, we reveal the design of VAE is crucial, and our
proposed VAE significantly outperforms existing VAEs in both reconstruction and
restoration. Extensive experiments on blind face restoration demonstrate the
superiority of Latent-PMRF, offering an improved PD-tradeoff compared to
existing methods, along with remarkable convergence efficiency, achieving a
5.79X speedup over PMRF in terms of FID. Our code will be available as
open-source.

</details>


### [44] [ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales](https://arxiv.org/abs/2507.00454)
*Yihao Zhen,Qiang Wang,Yu Qiao,Liangqiong Qu,Huijie Fan*

Main category: cs.CV

TL;DR: 论文提出了一种名为ATSTrack的视觉语言跟踪方法，通过对齐视觉和语言输入的时间和空间尺度差异，提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言跟踪（VLT）中视觉输入与语言描述之间的不对齐问题，尤其是时间和空间尺度的差异，限制了现有方法的性能。

Method: ATSTrack通过将语言描述分解为具有不同属性的短语，并细粒度地调整其特征，同时引入视觉语言标记来减少空间尺度差异的影响。

Result: 实验结果表明，ATSTrack的性能与现有方法相当。

Conclusion: ATSTrack通过解决时间和空间尺度差异问题，为视觉语言跟踪提供了一种有效的方法。

Abstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment
between visual inputs and language descriptions caused by target movement.
Previous trackers have explored many effective feature modification methods to
preserve more aligned features. However, an important yet unexplored factor
ultimately hinders their capability, which is the inherent differences in the
temporal and spatial scale of information between visual and language inputs.
To address this issue, we propose a novel visual-language tracker that enhances
the effect of feature modification by \textbf{A}ligning \textbf{T}emporal and
\textbf{S}patial scale of different input components, named as
\textbf{ATSTrack}. Specifically, we decompose each language description into
phrases with different attributes based on their temporal and spatial
correspondence with visual inputs, and modify their features in a fine-grained
manner. Moreover, we introduce a Visual-Language token that comprises modified
linguistic information from the previous frame to guide the model to extract
visual features that are more relevant to language description, thereby
reducing the impact caused by the differences in spatial scale. Experimental
results show that our proposed ATSTrack achieves performance comparable to
existing methods. Our code will be released.

</details>


### [45] [Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation](https://arxiv.org/abs/2507.00462)
*Jizhou Han,Chenhao Ding,SongLin Dong,Yuhang He,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: MS-TTA是一种无需训练的测试时适应方法，通过kNN Mean-Shift增强CLIP的特征表示，提升分布偏移下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖高置信度样本，忽略了低置信度样本的潜力，导致在分布偏移下表现不佳。

Method: 使用单步kNN Mean-Shift优化所有测试样本的特征表示，增强特征紧凑性和类别可分性。

Result: 在OOD和跨数据集基准测试中，MS-TTA表现优于现有方法，实现稳定适应。

Conclusion: MS-TTA无需额外训练即可提升CLIP在分布偏移下的鲁棒性。

Abstract: Visual-language models (VLMs) like CLIP exhibit strong generalization but
struggle with distribution shifts at test time. Existing training-free
test-time adaptation (TTA) methods operate strictly within CLIP's original
feature space, relying on high-confidence samples while overlooking the
potential of low-confidence ones. We propose MS-TTA, a training-free approach
that enhances feature representations beyond CLIP's space using a single-step
k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA
improves feature compactness and class separability, leading to more stable
adaptation. Additionally, a cache of refined embeddings further enhances
inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD
and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms
state-of-the-art training-free TTA methods, achieving robust adaptation without
requiring additional training.

</details>


### [46] [Bisecle: Binding and Separation in Continual Learning for Video Language Understanding](https://arxiv.org/abs/2507.00469)
*Yue Tan,Xiaoqian Hu,Hao Xue,Celso De Melo,Flora D. Salim*

Main category: cs.CV

TL;DR: 论文提出Bisecle方法，通过模拟海马体的快速绑定和模式分离机制，解决视频语言持续学习中的遗忘和更新冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的视频是持续演化的数据流，现有视觉语言模型在持续学习时面临灾难性遗忘和更新冲突的挑战。

Method: 提出多方向监督模块捕捉跨模态关系，设计对比提示学习方案隔离任务特定知识。

Result: 在多个VideoQA基准测试中，Bisecle有效缓解遗忘并提升跨任务泛化能力。

Conclusion: Bisecle通过结合绑定和分离机制，增强了视觉语言模型在视频理解任务中的持续学习能力。

Abstract: Frontier vision-language models (VLMs) have made remarkable improvements in
video understanding tasks. However, real-world videos typically exist as
continuously evolving data streams (e.g., dynamic scenes captured by wearable
glasses), necessitating models to continually adapt to shifting data
distributions and novel scenarios. Considering the prohibitive computational
costs of fine-tuning models on new tasks, usually, a small subset of parameters
is updated while the bulk of the model remains frozen. This poses new
challenges to existing continual learning frameworks in the context of large
multimodal foundation models, i.e., catastrophic forgetting and update
conflict. While the foundation models struggle with parameter-efficient
continual learning, the hippocampus in the human brain has evolved highly
efficient mechanisms for memory formation and consolidation. Inspired by the
rapid Binding and pattern separation mechanisms in the hippocampus, in this
work, we propose Bisecle for video-language continual learning, where a
multi-directional supervision module is used to capture more cross-modal
relationships and a contrastive prompt learning scheme is designed to isolate
task-specific knowledge to facilitate efficient memory storage. Binding and
separation processes further strengthen the ability of VLMs to retain complex
experiences, enabling robust and efficient continual learning in video
understanding tasks. We perform a thorough evaluation of the proposed Bisecle,
demonstrating its ability to mitigate forgetting and enhance cross-task
generalization on several VideoQA benchmarks.

</details>


### [47] [ARIG: Autoregressive Interactive Head Generation for Real-time Conversations](https://arxiv.org/abs/2507.00472)
*Ying Guo,Xi Liu,Cheng Zhen,Pengfei Yan,Xiaoming Wei*

Main category: cs.CV

TL;DR: 提出了一种基于自回归（AR）的帧级框架ARIG，用于实时生成更真实的交互式头部运动，解决了传统方法的信号获取、上下文行为理解和切换平滑性问题。


<details>
  <summary>Details</summary>
Motivation: 面对面对话是常见的人类活动，但现有方法在实时性和真实性上存在局限，如信号获取困难、上下文理解不足和切换不流畅。

Method: 采用非向量量化的自回归过程建模运动预测，利用扩散过程表示运动分布，同时结合交互行为理解（IBU）和对话状态理解（CSU）提升交互真实性。

Result: 实验验证了ARIG模型的有效性，能够实现更准确的连续空间预测和更真实的交互效果。

Conclusion: ARIG框架通过创新的自回归和扩散方法，显著提升了实时交互式头部生成的准确性和真实性。

Abstract: Face-to-face communication, as a common human activity, motivates the
research on interactive head generation. A virtual agent can generate motion
responses with both listening and speaking capabilities based on the audio or
motion signals of the other user and itself. However, previous clip-wise
generation paradigm or explicit listener/speaker generator-switching methods
have limitations in future signal acquisition, contextual behavioral
understanding, and switching smoothness, making it challenging to be real-time
and realistic. In this paper, we propose an autoregressive (AR) based
frame-wise framework called ARIG to realize the real-time generation with
better interaction realism. To achieve real-time generation, we model motion
prediction as a non-vector-quantized AR process. Unlike discrete codebook-index
prediction, we represent motion distribution using diffusion procedure,
achieving more accurate predictions in continuous space. To improve interaction
realism, we emphasize interactive behavior understanding (IBU) and detailed
conversational state understanding (CSU). In IBU, based on dual-track
dual-modal signals, we summarize short-range behaviors through
bidirectional-integrated learning and perform contextual understanding over
long ranges. In CSU, we use voice activity signals and context features of IBU
to understand the various states (interruption, feedback, pause, etc.) that
exist in actual conversations. These serve as conditions for the final
progressive motion prediction. Extensive experiments have verified the
effectiveness of our model.

</details>


### [48] [ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis](https://arxiv.org/abs/2507.00474)
*Yaofei Duan,Yuhao Huang,Xin Yang,Luyi Han,Xinyu Xie,Zhiyuan Zhu,Ping He,Ka-Hou Chan,Ligang Cui,Sio-Kei Im,Dong Ni,Tao Tan*

Main category: cs.CV

TL;DR: 提出了一种名为ADAptation的无监督主动学习框架，通过扩散模型和双评分机制解决跨域数据分布差异问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在跨域数据分布变化时性能下降的问题，减少标注成本并提高适应性。

Method: 利用扩散模型进行分布对齐，结合紧凑特征聚类和双评分机制选择信息量大的样本。

Result: 在四个乳腺超声数据集上优于现有主动学习方法，验证了其有效性和泛化能力。

Conclusion: ADAptation框架在临床领域适应中表现出色，为跨域数据学习提供了高效解决方案。

Abstract: Deep learning-based diagnostic models often suffer performance drops due to
distribution shifts between training (source) and test (target) domains.
Collecting and labeling sufficient target domain data for model retraining
represents an optimal solution, yet is limited by time and scarce resources.
Active learning (AL) offers an efficient approach to reduce annotation costs
while maintaining performance, but struggles to handle the challenge posed by
distribution variations across different datasets. In this study, we propose a
novel unsupervised Active learning framework for Domain Adaptation, named
ADAptation, which efficiently selects informative samples from multi-domain
data pools under limited annotation budget. As a fundamental step, our method
first utilizes the distribution homogenization capabilities of diffusion models
to bridge cross-dataset gaps by translating target images into source-domain
style. We then introduce two key innovations: (a) a hypersphere-constrained
contrastive learning network for compact feature clustering, and (b) a
dual-scoring mechanism that quantifies and balances sample uncertainty and
representativeness. Extensive experiments on four breast ultrasound datasets
(three public and one in-house/multi-center) across five common deep
classifiers demonstrate that our method surpasses existing strong AL-based
competitors, validating its effectiveness and generalization for clinical
domain adaptation. The code is available at the anonymized link:
https://github.com/miccai25-966/ADAptation.

</details>


### [49] [Just Noticeable Difference for Large Multimodal Models](https://arxiv.org/abs/2507.00490)
*Zijian Chen,Yuan Tian,Yuze Sun,Wei Sun,Zicheng Zhang,Weisi Lin,Guangtao Zhai,Wenjun Zhang*

Main category: cs.CV

TL;DR: 论文提出了LMM-JND概念及其测定流程，揭示了当前大型多模态模型（LMMs）在视觉感知任务中的盲区，并构建了VPA-JND数据集以支持研究。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LMMs在多任务和刺激类型中的感知边界，尤其是其潜在的安全问题和效率不足。

Method: 通过构建VPA-JND数据集（包含21.5k参考图像和489k刺激），系统量化LMMs的视觉盲区，并分析视觉和语言主干的影响。

Result: 研究发现GPT-4o和InternVL2.5等先进LMMs在基础比较查询中表现不佳，远低于人类水平。

Conclusion: LMM-JND为研究LMMs提供了独特视角，其可预测性对安全性至关重要。

Abstract: Just noticeable difference (JND), the minimum change that the human visual
system (HVS) can perceive, has been studied for decades. Although recent work
has extended this line of research into machine vision, there has been a
scarcity of studies systematically exploring its perceptual boundaries across
multiple tasks and stimulus types, particularly in the current era of rapidly
advancing large multimodal models (LMMs), where studying the multifaceted
capabilities of models has become a mainstream focus. Moreover, the perceptual
defects of LMMs are not investigated thoroughly, resulting in potential
security issues and suboptimal response efficiency. In this paper, we take an
initial attempt and demonstrate that there exist significant visual blind spots
in current LMMs. To systemically quantify this characteristic, we propose a new
concept, {\bf LMM-JND}, together with its determination pipeline. Targeting
uncovering the behavior commonalities in HVS-aligned visual perception tasks,
we delve into several LMM families and construct a large-scale dataset, named
VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12
distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where
state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle
with basic comparison queries and fall significantly short of human-level
visual performance. We further explore the effects of vision and language
backbones and find a notable correlation between their design philosophy that
may instruct the future refinement of LMMs for their visual acuity. Together,
our research underscores the significance of LMM-JND as a unique perspective
for studying LMMs, and predictable LMM-JND is crucial for security concerns.
This work will be available at https://github.com/zijianchen98/LMM-JND.

</details>


### [50] [Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models](https://arxiv.org/abs/2507.00493)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: 论文提出了一种新的形状评估方法（CSS），用于衡量模型对全局配置形状的敏感性，发现自监督和语言对齐的Transformer模型表现最佳，并揭示了长程交互的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型主要依赖局部纹理线索，忽略了全局配置形状的重要性，导致特征脆弱且非组合性。论文旨在探索模型如何同时利用纹理和形状线索。

Method: 通过Configural Shape Score（CSS）评估模型对全局配置形状的敏感性，使用Object-Anagram对进行测试，并分析模型的注意力机制和表征相似性。

Result: 自监督和语言对齐的Transformer模型（如DINOv2、SigLIP2和EVA-CLIP）在CSS评估中表现最佳，且依赖长程交互。BagNet模型表现不佳。

Conclusion: 构建真正鲁棒且类人的视觉系统需要同时整合局部纹理和全局配置形状的架构和学习框架。

Abstract: Humans are able to recognize objects based on both local texture cues and the
configuration of object parts, yet contemporary vision models primarily harvest
local texture cues, yielding brittle, non-compositional features. Work on
shape-vs-texture bias has pitted shape and texture representations in
opposition, measuring shape relative to texture, ignoring the possibility that
models (and humans) can simultaneously rely on both types of cues, and
obscuring the absolute quality of both types of representation. We therefore
recast shape evaluation as a matter of absolute configural competence,
operationalized by the Configural Shape Score (CSS), which (i) measures the
ability to recognize both images in Object-Anagram pairs that preserve local
texture while permuting global part arrangement to depict different object
categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)
uncovers a broad spectrum of configural sensitivity with fully self-supervised
and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and
EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes
reveal that (iii) high-CSS networks depend on long-range interactions:
radius-controlled attention masks abolish performance showing a distinctive
U-shaped integration profile, and representational-similarity analyses expose a
mid-depth transition from local to global coding. A BagNet control remains at
chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that
configural shape score also predicts other shape-dependent evals. Overall, we
propose that the path toward truly robust, generalizable, and human-like vision
systems may not lie in forcing an artificial choice between shape and texture,
but rather in architectural and learning frameworks that seamlessly integrate
both local-texture and global configural shape.

</details>


### [51] [Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing](https://arxiv.org/abs/2507.00501)
*Yongzhen Wang,Liangliang Chen,Bingwen Hu,Heng Liu,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: Laplace-Mamba框架结合拉普拉斯频率先验和混合Mamba-CNN架构，用于高效图像去雾，通过双并行路径处理低频和高频分量，显著提升恢复质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于SSM的方法在重建局部结构和高维数据处理上表现不佳，导致细粒度图像特征恢复不理想。

Method: 采用拉普拉斯分解将图像分为低频和高频分量，分别通过SSM和CNN处理，结合信息保留的下采样技术。

Result: 在多个基准测试中，Laplace-Mamba在恢复质量和效率上均优于现有方法。

Conclusion: Laplace-Mamba通过双路径处理和拉普拉斯先验，有效解决了图像去雾中的局部结构和高维数据问题。

Abstract: Recent progress in image restoration has underscored Spatial State Models
(SSMs) as powerful tools for modeling long-range dependencies, owing to their
appealing linear complexity and computational efficiency. However, SSM-based
approaches exhibit limitations in reconstructing localized structures and tend
to be less effective when handling high-dimensional data, frequently resulting
in suboptimal recovery of fine image features. To tackle these challenges, we
introduce Laplace-Mamba, a novel framework that integrates Laplace frequency
prior with a hybrid Mamba-CNN architecture for efficient image dehazing.
Leveraging the Laplace decomposition, the image is disentangled into
low-frequency components capturing global texture and high-frequency components
representing edges and fine details. This decomposition enables specialized
processing via dual parallel pathways: the low-frequency branch employs SSMs
for global context modeling, while the high-frequency branch utilizes CNNs to
refine local structural details, effectively addressing diverse haze scenarios.
Notably, the Laplace transformation facilitates information-preserving
downsampling of low-frequency components in accordance with the Nyquist theory,
thereby significantly improving computational efficiency. Extensive evaluations
across multiple benchmarks demonstrate that our method outperforms
state-of-the-art approaches in both restoration quality and efficiency. The
source code and pretrained models are available at
https://github.com/yz-wang/Laplace-Mamba.

</details>


### [52] [ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation](https://arxiv.org/abs/2507.00502)
*JianChao Zhao,Songlin Dong*

Main category: cs.CV

TL;DR: ExPaMoE提出了一种基于可扩展并行混合专家架构的新框架，通过双分支专家设计和实时频谱感知域判别器，解决了持续测试时适应中的特征纠缠和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖共享模型参数，易受特征纠缠和灾难性遗忘影响，尤其是在大或非平稳域偏移时。

Method: ExPaMoE采用双分支专家设计分离领域通用和特定知识，并通过频谱感知在线域判别器动态扩展专家池。

Result: 在多个标准基准测试中，ExPaMoE表现优异，展示了强大的鲁棒性、可扩展性和抗遗忘能力。

Conclusion: ExPaMoE为CTTA提供了一种高效解决方案，适用于复杂域演化的长期适应。

Abstract: Continual Test-Time Adaptation (CTTA) aims to enable models to adapt
on-the-fly to a stream of unlabeled data under evolving distribution shifts.
However, existing CTTA methods typically rely on shared model parameters across
all domains, making them vulnerable to feature entanglement and catastrophic
forgetting in the presence of large or non-stationary domain shifts. To address
this limitation, we propose \textbf{ExPaMoE}, a novel framework based on an
\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples
domain-general and domain-specific knowledge via a dual-branch expert design
with token-guided feature separation, and dynamically expands its expert pool
based on a \emph{Spectral-Aware Online Domain Discriminator} (SODD) that
detects distribution changes in real-time using frequency-domain cues.
Extensive experiments demonstrate the superiority of ExPaMoE across diverse
CTTA scenarios. We evaluate our method on standard benchmarks including
CIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic
segmentation. Additionally, we introduce \textbf{ImageNet++}, a large-scale and
realistic CTTA benchmark built from multiple ImageNet-derived datasets, to
better reflect long-term adaptation under complex domain evolution. ExPaMoE
consistently outperforms prior arts, showing strong robustness, scalability,
and resistance to forgetting.

</details>


### [53] [LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs](https://arxiv.org/abs/2507.00505)
*Haoran Lou,Chunxiao Fan,Ziyan Liu,Yuexin Wu,Xinxiang Wang*

Main category: cs.CV

TL;DR: LLaVA-SP通过添加六个空间视觉标记增强视觉表示，提出新投影器和两种变体，显著提升多模态任务性能。


<details>
  <summary>Details</summary>
Motivation: CLIP-ViT在捕捉局部关系方面表现不佳，影响多模态大语言模型的详细理解能力。

Method: 提出LLaVA-SP，添加空间视觉标记，使用卷积核和交叉注意力机制，开发两种变体（Cropping和Pooling）。

Result: 在多个多模态基准测试中性能显著提升，优于LLaVA-1.5，推理延迟几乎相同。

Conclusion: LLaVA-SP通过简单改进显著提升视觉表示能力，适用于多样化视觉理解任务。

Abstract: The architecture of multimodal large language models (MLLMs) commonly
connects a vision encoder, often based on CLIP-ViT, to a large language model.
While CLIP-ViT works well for capturing global image features, it struggles to
model local relationships between adjacent patches, leading to weaker visual
representation, which in turn affects the detailed understanding ability of
MLLMs. To solve this, we propose LLaVA-SP, which \textbf{ only adds six spatial
visual tokens} to the original visual tokens to enhance the visual
representation. Our approach offers three key advantages: 1)We propose a novel
Projector, which uses convolutional kernels to derive visual spatial tokens
from ViT patch features, simulating two visual spatial ordering approaches:
``from central region to global" and ``from abstract to specific". Then, a
cross-attention mechanism is applied to fuse fine-grained visual information,
enriching the overall visual representation. 2) We present two model variants:
LLaVA-SP-Cropping, which focuses on detail features through progressive
cropping, and LLaVA-SP-Pooling, which captures global semantics through
adaptive pooling, enabling the model to handle diverse visual understanding
tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,
achieves significant performance improvements across various multimodal
benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple
tasks with nearly identical inference latency. The code and models are
available at
\href{https://github.com/CnFaker/LLaVA-SP}{\texttt{https://github.com/CnFaker/LLaVA-SP}}.

</details>


### [54] [SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning](https://arxiv.org/abs/2507.00506)
*Yunfei Xie,Yuxuan Cheng,Juncheng Wu,Haoyu Zhang,Yuyin Zhou,Shoudong Han*

Main category: cs.CV

TL;DR: 提出了一种名为SCING的框架，通过选择性视觉提示融合和扰动驱动一致性对齐，优化了跨模态对齐和鲁棒性，减少了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂适配器或模态特定调优，忽略了跨模态交互，导致高计算成本或对齐效果不佳。

Method: 引入选择性视觉提示融合（SVIP）和扰动驱动一致性对齐（PDCA），动态注入视觉特征并增强特征对齐。

Result: 在多个基准测试中表现优异，平衡了性能和计算开销。

Conclusion: SCING框架简化了适配器设计，提升了跨模态对齐效率和鲁棒性。

Abstract: Recent advancements in adapting vision-language pre-training models like CLIP
for person re-identification (ReID) tasks often rely on complex adapter design
or modality-specific tuning while neglecting cross-modal interaction, leading
to high computational costs or suboptimal alignment. To address these
limitations, we propose a simple yet effective framework named Selective
Cross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and
robustness against real-world perturbations. Our method introduces two key
innovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a
lightweight module that dynamically injects discriminative visual features into
text prompts via a cross-modal gating mechanism. Moreover, the proposed
Perturbation-Driven Consistency Alignment (PDCA) is a dual-path training
strategy that enforces invariant feature alignment under random image
perturbations by regularizing consistency between original and augmented
cross-modal embeddings. Extensive experiments are conducted on several popular
benchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,
and P-DukeMTMC, which demonstrate the impressive performance of the proposed
method. Notably, our framework eliminates heavy adapters while maintaining
efficient inference, achieving an optimal trade-off between performance and
computational overhead. The code will be released upon acceptance.

</details>


### [55] [Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection](https://arxiv.org/abs/2507.00519)
*Ruize Cui,Jiaan Zhang,Jialun Pei,Kai Wang,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

TL;DR: TopoNet是一种用于腹腔镜肝脏手术中标志物检测的新型拓扑约束学习框架，结合RGB和深度信息，通过边界感知拓扑融合模块提升边缘感知能力，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 肝脏标志物在腹腔镜手术中提供关键解剖引导，但其管状结构和动态变形对自动检测提出挑战。

Method: 采用蛇形-CNN双路径编码器捕获RGB纹理和深度拓扑结构，提出边界感知拓扑融合模块和拓扑约束损失函数。

Result: 在L3D和P2ILF数据集上表现出高精度和低计算复杂度。

Conclusion: TopoNet在临床腹腔镜肝脏手术中具有潜在应用价值。

Abstract: Liver landmarks provide crucial anatomical guidance to the surgeon during
laparoscopic liver surgery to minimize surgical risk. However, the tubular
structural properties of landmarks and dynamic intraoperative deformations pose
significant challenges for automatic landmark detection. In this study, we
introduce TopoNet, a novel topology-constrained learning framework for
laparoscopic liver landmark detection. Our framework adopts a snake-CNN
dual-path encoder to simultaneously capture detailed RGB texture information
and depth-informed topological structures. Meanwhile, we propose a
boundary-aware topology fusion (BTF) module, which adaptively merges RGB-D
features to enhance edge perception while preserving global topology.
Additionally, a topological constraint loss function is embedded, which
contains a center-line constraint loss and a topological persistence loss to
ensure homotopy equivalence between predictions and labels. Extensive
experiments on L3D and P2ILF datasets demonstrate that TopoNet achieves
outstanding accuracy and computational complexity, highlighting the potential
for clinical applications in laparoscopic liver surgery. Our code will be
available at https://github.com/cuiruize/TopoNet.

</details>


### [56] [Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving](https://arxiv.org/abs/2507.00525)
*Djamahl Etchegaray,Yuxia Fu,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: Box-QAymo是一个用于评估和微调视觉语言模型（VLMs）在用户指定对象上的时空推理能力的数据集和基准，通过边界框和层次化评估协议提升自动驾驶系统的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在真实场景中难以捕捉用户意图，现有数据集局限于全场景描述或路径预测，无法评估VLMs对局部用户驱动查询的响应能力。

Method: 提出Box-QAymo数据集，用户通过绘制边界框表达意图，采用层次化评估协议（包括属性预测、运动理解和时空推理），并通过众包和严格质量控制确保数据多样性。

Result: 评估显示当前VLMs在感知问题上的显著局限性，揭示了与现实世界性能的差距。

Conclusion: Box-QAymo为开发更鲁棒、可解释的自动驾驶系统奠定了基础，支持真实条件下的有效用户沟通。

Abstract: Interpretable communication is essential for safe and trustworthy autonomous
driving, yet current vision-language models (VLMs) often operate under
idealized assumptions and struggle to capture user intent in real-world
scenarios. Existing driving-oriented VQA datasets are limited to full-scene
descriptions or waypoint prediction, preventing the assessment of whether VLMs
can respond to localized user-driven queries. We introduce Box-QAymo, a
box-referring dataset and benchmark designed to both evaluate and finetune VLMs
on spatial and temporal reasoning over user-specified objects. Users express
intent by drawing bounding boxes, offering a fast and intuitive interface for
focused queries in complex scenes. Specifically, we propose a hierarchical
evaluation protocol that begins with binary sanity-check questions to assess
basic model capacities, and progresses to (1) attribute prediction for
box-referred objects, (2) motion understanding of target instances, and (3)
spatiotemporal motion reasoning over inter-object dynamics across frames. To
support this, we crowd-sourced fine-grained object classes and visual
attributes that reflect the complexity drivers encounter, and extract object
trajectories to construct temporally grounded QA pairs. Rigorous quality
control through negative sampling, temporal consistency checks, and
difficulty-aware balancing guarantee dataset robustness and diversity. Our
comprehensive evaluation reveals significant limitations in current VLMs when
queried about perception questions, highlighting the gap in achieving
real-world performance. This work provides a foundation for developing more
robust and interpretable autonomous driving systems that can communicate
effectively with users under real-world conditions. Project page and dataset
are available at https://djamahl99.github.io/qaymo-pages/.

</details>


### [57] [Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation](https://arxiv.org/abs/2507.00537)
*Feng Lin,Marco Chen,Haokui Zhang,Xiaotian Yu,Guangming Lu,Rong Xiao*

Main category: cs.CV

TL;DR: 本文研究了CLIP图像编码器中注意力头的作用，提出了一种名为注意力消融技术（AAT）的方法，通过操纵注意力权重来抑制有害头，从而提升下游任务性能。实验显示AAT显著提高了跨模态检索等任务的性能。


<details>
  <summary>Details</summary>
Motivation: CLIP在多种应用中表现稳健，但某些注意力头可能对最终表示产生负面影响，消融这些头可能提升性能。

Method: 提出AAT方法，通过操纵注意力权重抑制有害头的贡献，并针对不同场景整合两种策略。

Result: AAT在多种领域下游任务中均提升性能，跨模态检索召回率最高提升11.1%。

Conclusion: AAT能有效优化大规模视觉语言模型，且几乎不增加推理成本。

Abstract: This paper studies the role of attention heads in CLIP's image encoder. While
CLIP has exhibited robust performance across diverse applications, we
hypothesize that certain attention heads negatively affect final
representations and that ablating them can improve performance in downstream
tasks. To capitalize on this insight, we propose a simple yet effective method,
called Attention Ablation Technique (AAT), to suppress the contribution of
specific heads by manipulating attention weights. By integrating two
alternative strategies tailored for different application scenarios, AAT
systematically identifies and ablates detrimental attention heads to enhance
representation quality. Experiments demonstrate that AAT consistently improves
downstream task performance across various domains, boosting recall rate by up
to 11.1% on CLIP-family models for cross-modal retrieval. The results highlight
the potential of AAT to effectively refine large-scale vision-language models
with virtually no increase in inference cost.

</details>


### [58] [LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing](https://arxiv.org/abs/2507.00554)
*Zhenya Yang,Bingchen Gong,Kai Chen,Qi Dou*

Main category: cs.CV

TL;DR: LOD-GS提出了一种动态预测3D高斯原语最优滤波强度的框架，解决了3D高斯溅射中的混叠问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖低通滤波来缓解混叠，但对采样率不敏感，导致渲染效果不佳。

Method: 引入一组基函数，以采样率为输入建模外观变化，实现采样率敏感滤波，并与3D高斯联合优化。

Result: 在公开数据集和新合成数据集上，LOD-GS实现了SOTA渲染质量，有效消除混叠。

Conclusion: LOD-GS框架显著提升了3D高斯溅射的渲染质量，代码和数据集已开源。

Abstract: Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dynamically predicts the optimal filtering
strength for each 3D Gaussian primitive. Specifically, we introduce a set of
basis functions to each Gaussian, which take the sampling rate as input to
model appearance variations, enabling sampling-rate-sensitive filtering. These
basis function parameters are jointly optimized with the 3D Gaussian in an
end-to-end manner. The sampling rate is influenced by both focal length and
camera distance. However, existing methods and datasets rely solely on
down-sampling to simulate focal length changes for anti-aliasing evaluation,
overlooking the impact of camera distance. To enable a more comprehensive
assessment, we introduce a new synthetic dataset featuring objects rendered at
varying camera distances. Extensive experiments on both public datasets and our
newly collected dataset demonstrate that our method achieves SOTA rendering
quality while effectively eliminating aliasing. The code and dataset have been
open-sourced.

</details>


### [59] [Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment](https://arxiv.org/abs/2507.00566)
*Kai Zhou,Shuhai Zhang,Zeng You,Jinwu Hu,Mingkui Tan,Fei Liu*

Main category: cs.CV

TL;DR: PGFA提出了一种原型引导的特征对齐范式，用于零样本骨架动作识别，通过端到端跨模态对比训练框架和原型引导的文本特征对齐策略，显著提升了识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在骨架特征区分度和骨架-文本对齐偏差方面的不足，提升零样本骨架动作识别的效果。

Method: 采用端到端跨模态对比训练框架增强骨架-文本对齐，并提出原型引导的文本特征对齐策略以减少测试时的分布差异影响。

Result: 在NTU-60、NTU-120和PKU-MMD数据集上，PGFA相比SMIE方法分别提升了22.96%、12.53%和18.54%的准确率。

Conclusion: PGFA通过改进骨架特征区分度和对齐策略，显著提升了零样本骨架动作识别的性能。

Abstract: Zero-shot skeleton-based action recognition aims to classify unseen
skeleton-based human actions without prior exposure to such categories during
training. This task is extremely challenging due to the difficulty in
generalizing from known to unknown actions. Previous studies typically use
two-stage training: pre-training skeleton encoders on seen action categories
using cross-entropy loss and then aligning pre-extracted skeleton and text
features, enabling knowledge transfer to unseen classes through skeleton-text
alignment and language models' generalization. However, their efficacy is
hindered by 1) insufficient discrimination for skeleton features, as the fixed
skeleton encoder fails to capture necessary alignment information for effective
skeleton-text alignment; 2) the neglect of alignment bias between skeleton and
unseen text features during testing. To this end, we propose a prototype-guided
feature alignment paradigm for zero-shot skeleton-based action recognition,
termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive
training framework to improve skeleton-text alignment, ensuring sufficient
discrimination for skeleton features. Additionally, we introduce a
prototype-guided text feature alignment strategy to mitigate the adverse impact
of the distribution discrepancy during testing. We provide a theoretical
analysis to support our prototype-guided text feature alignment strategy and
empirically evaluate our overall PGFA on three well-known datasets. Compared
with the top competitor SMIE method, our PGFA achieves absolute accuracy
improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD
datasets, respectively.

</details>


### [60] [Out-of-distribution detection in 3D applications: a review](https://arxiv.org/abs/2507.00570)
*Zizhao Li,Xueyang Kang,Joseph West,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 本文综述了OOD（Out-of-Distribution）检测在可信赖和不确定性AI中的重要性，介绍了关键用例、基准数据集、评估指标，并比较了不同方法。最后提出了未来研究方向，如对抗性鲁棒的OOD检测和3D视觉集成。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据中未见的物体在现实世界中的识别问题，提升AI系统的可靠性和安全性。

Method: 通过比较OOD检测方法，包括模型结构、不确定性指标和分布距离分类，以及不确定性校准技术。

Result: 提供了理论和实践见解，展示了3D视觉集成等新兴研究方向。

Conclusion: OOD检测是发展可靠AI系统的关键，未来研究应关注对抗性鲁棒性和3D应用。

Abstract: The ability to detect objects that are not prevalent in the training set is a
critical capability in many 3D applications, including autonomous driving.
Machine learning methods for object recognition often assume that all object
categories encountered during inference belong to a closed set of classes
present in the training data. This assumption limits generalization to the real
world, as objects not seen during training may be misclassified or entirely
ignored. As part of reliable AI, OOD detection identifies inputs that deviate
significantly from the training distribution. This paper provides a
comprehensive overview of OOD detection within the broader scope of trustworthy
and uncertain AI. We begin with key use cases across diverse domains, introduce
benchmark datasets spanning multiple modalities, and discuss evaluation
metrics. Next, we present a comparative analysis of OOD detection
methodologies, exploring model structures, uncertainty indicators, and
distributional distance taxonomies, alongside uncertainty calibration
techniques. Finally, we highlight promising research directions, including
adversarially robust OOD detection and failure identification, particularly
relevant to 3D applications. The paper offers both theoretical and practical
insights into OOD detection, showcasing emerging research opportunities such as
3D vision integration. These insights help new researchers navigate the field
more effectively, contributing to the development of reliable, safe, and robust
AI systems.

</details>


### [61] [AI-Generated Video Detection via Perceptual Straightening](https://arxiv.org/abs/2507.00583)
*Christian Internò,Robert Geirhos,Markus Olhofer,Sunny Liu,Barbara Hammer,David Klindt*

Main category: cs.CV

TL;DR: ReStraV是一种基于神经表示几何的新方法，通过分析视频在DINOv2表示域中的时间曲率和步长距离，有效区分真实视频与AI生成视频。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展导致合成视频高度逼真，现有检测方法难以泛化且难以捕捉时间不一致性。

Method: 利用预训练的DINOv2模型量化视频表示域的时间曲率和步长距离，统计这些指标并训练分类器。

Result: 在VidProM基准测试中达到97.17%准确率和98.63% AUROC，显著优于现有方法。

Conclusion: ReStraV提供了一种高效、低成本的检测方案，为AI生成视频检测提供了新思路。

Abstract: The rapid advancement of generative AI enables highly realistic synthetic
videos, posing significant challenges for content authentication and raising
urgent concerns about misuse. Existing detection methods often struggle with
generalization and capturing subtle temporal inconsistencies. We propose
ReStraV(Representation Straightening Video), a novel approach to distinguish
natural from AI-generated videos. Inspired by the "perceptual straightening"
hypothesis -- which suggests real-world video trajectories become more straight
in neural representation domain -- we analyze deviations from this expected
geometric property. Using a pre-trained self-supervised vision transformer
(DINOv2), we quantify the temporal curvature and stepwise distance in the
model's representation domain. We aggregate statistics of these measures for
each video and train a classifier. Our analysis shows that AI-generated videos
exhibit significantly different curvature and distance patterns compared to
real videos. A lightweight classifier achieves state-of-the-art detection
performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),
substantially outperforming existing image- and video-based methods. ReStraV is
computationally efficient, it is offering a low-cost and effective detection
solution. This work provides new insights into using neural representation
geometry for AI-generated video detection.

</details>


### [62] [Similarity Memory Prior is All You Need for Medical Image Segmentation](https://arxiv.org/abs/2507.00585)
*Tang Hao,Guo ZhiQing,Wang LieJun,Liu Chao*

Main category: cs.CV

TL;DR: 论文提出了一种基于相似性记忆先验的医学图像分割网络Sim-MPNet，通过动态记忆权重损失注意力和双相似性全局内部增强模块，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 受猕猴初级视觉皮层中“祖母细胞”直接识别复杂形状的启发，探索这些细胞在医学图像分割研究中的价值。

Method: 设计了Sim-MPNet网络，包括动态记忆权重损失注意力（DMW-LA）和双相似性全局内部增强模块（DS-GIM），通过相似性记忆先验和动态更新策略优化特征提取。

Result: 在四个公开数据集上的实验表明，Sim-MPNet优于其他最先进方法。

Conclusion: Sim-MPNet通过创新的记忆和相似性机制，有效提升了医学图像分割的精度和性能。

Abstract: In recent years, it has been found that "grandmother cells" in the primary
visual cortex (V1) of macaques can directly recognize visual input with complex
shapes. This inspires us to examine the value of these cells in promoting the
research of medical image segmentation. In this paper, we design a Similarity
Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,
we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and
remembers the category features of specific lesions or organs in medical images
through the similarity memory prior in the prototype memory bank, thus helping
the network to learn subtle texture changes between categories. DMW-LA also
dynamically updates the similarity memory prior in reverse through Weight-Loss
Dynamic (W-LD) update strategy, effectively assisting the network directly
extract category features. In addition, we propose the Double-Similarity Global
Internal Enhancement Module (DS-GIM) to deeply explore the internal differences
in the feature distribution of input data through cosine similarity and
euclidean distance. Extensive experiments on four public datasets show that
Sim-MPNet has better segmentation performance than other state-of-the-art
methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.

</details>


### [63] [Context-Aware Academic Emotion Dataset and Benchmark](https://arxiv.org/abs/2507.00586)
*Luming Zhao,Jingwen Xuan,Jiamin Lou,Yonghui Yu,Wenwu Yang*

Main category: cs.CV

TL;DR: 论文提出了一种基于CLIP的上下文感知学术情绪识别方法（CLIP-CAER），并发布了首个涵盖多样化自然学习场景的学术情绪数据集RAER。


<details>
  <summary>Details</summary>
Motivation: 学术情绪分析对评估学生学习状态至关重要，但现有研究多集中于基本情绪，学术情绪识别因缺乏公开数据集而研究不足。

Method: 通过构建RAER数据集，并利用CLIP模型结合可学习文本提示，整合面部表情和上下文线索进行情绪识别。

Result: CLIP-CAER在学术情绪识别任务中显著优于现有视频面部表情识别方法。

Conclusion: 上下文信息对学术情绪识别至关重要，CLIP-CAER为未来研究提供了新方向。

Abstract: Academic emotion analysis plays a crucial role in evaluating students'
engagement and cognitive states during the learning process. This paper
addresses the challenge of automatically recognizing academic emotions through
facial expressions in real-world learning environments. While significant
progress has been made in facial expression recognition for basic emotions,
academic emotion recognition remains underexplored, largely due to the scarcity
of publicly available datasets. To bridge this gap, we introduce RAER, a novel
dataset comprising approximately 2,700 video clips collected from around 140
students in diverse, natural learning contexts such as classrooms, libraries,
laboratories, and dormitories, covering both classroom sessions and individual
study. Each clip was annotated independently by approximately ten annotators
using two distinct sets of academic emotion labels with varying granularity,
enhancing annotation consistency and reliability. To our knowledge, RAER is the
first dataset capturing diverse natural learning scenarios. Observing that
annotators naturally consider context cues-such as whether a student is looking
at a phone or reading a book-alongside facial expressions, we propose CLIP-CAER
(CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes
learnable text prompts within the vision-language model CLIP to effectively
integrate facial expression and context cues from videos. Experimental results
demonstrate that CLIP-CAER substantially outperforms state-of-the-art
video-based facial expression recognition methods, which are primarily designed
for basic emotions, emphasizing the crucial role of context in accurately
recognizing academic emotions. Project page: https://zgsfer.github.io/CAER

</details>


### [64] [Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods](https://arxiv.org/abs/2507.00593)
*Fernando Alonso-Fernandez,Talha Hanif Butt,Prayag Tiwari*

Main category: cs.CV

TL;DR: 研究通过分析卡车CAN总线数据，评估三种分类器（ANN、RF、SVM）用于超车检测的性能，发现数据多样性和多车训练对提升分类准确性至关重要，最终通过分数级融合策略实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 卡车安全超车对防止事故和交通效率至关重要，需通过ADAS系统准确预测超车行为。

Method: 使用Volvo提供的卡车CAN总线数据，评估ANN、RF、SVM三种分类器，分析预处理配置对性能的影响，并应用分数级融合策略。

Result: 多车训练提升泛化能力，分数级融合策略实现TNR=93%和TPR=86.5%的准确率。

Conclusion: 数据多样性和多车训练是提高超车检测性能的关键，分数级融合策略效果最佳。

Abstract: Safe overtaking manoeuvres in trucks are vital for preventing accidents and
ensuring efficient traffic flow. Accurate prediction of such manoeuvres is
essential for Advanced Driver Assistance Systems (ADAS) to make timely and
informed decisions. In this study, we focus on overtake detection using
Controller Area Network (CAN) bus data collected from five in-service trucks
provided by the Volvo Group. We evaluate three common classifiers for vehicle
manoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and
Support Vector Machines (SVM), and analyse how different preprocessing
configurations affect performance. We find that variability in traffic
conditions strongly influences the signal patterns, particularly in the
no-overtake class, affecting classification performance if training data lacks
adequate diversity. Since the data were collected under unconstrained,
real-world conditions, class diversity cannot be guaranteed a priori. However,
training with data from multiple vehicles improves generalisation and reduces
condition-specific bias. Our pertruck analysis also reveals that classification
accuracy, especially for overtakes, depends on the amount of training data per
vehicle. To address this, we apply a score-level fusion strategy, which yields
the best per-truck performance across most cases. Overall, we achieve an
accuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True
Positive Rate). This research has been part of the BIG FUN project, which
explores how Artificial Intelligence can be applied to logged vehicle data to
understand and predict driver behaviour, particularly in relation to Camera
Monitor Systems (CMS), being introduced as digital replacements for traditional
exterior mirrors.

</details>


### [65] [World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model](https://arxiv.org/abs/2507.00603)
*Yupeng Zheng,Pengxuan Yang,Zebin Xing,Qichao Zhang,Yuhang Zheng,Yinfeng Gao,Pengfei Li,Teng Zhang,Zhongpu Xia,Peng Jia,Dongbin Zhao*

Main category: cs.CV

TL;DR: World4Drive是一个端到端自动驾驶框架，利用视觉基础模型构建潜在世界模型，实现无感知标注的规划轨迹生成与评估。


<details>
  <summary>Details</summary>
Motivation: 解决端到端自动驾驶依赖昂贵感知监督的问题，通过自监督学习构建信息丰富的驾驶世界模型。

Method: 提取场景特征（包括驾驶意图和潜在表示），生成多模态规划轨迹，并通过世界模型选择器评估最佳轨迹。

Result: 在nuScenes和NavSim基准测试中表现优异，L2误差降低18.1%，碰撞率减少46.7%，训练速度提升3.75倍。

Conclusion: World4Drive通过自监督学习实现了无感知标注的高效自动驾驶规划。

Abstract: End-to-end autonomous driving directly generates planning trajectories from
raw sensor data, yet it typically relies on costly perception supervision to
extract scene information. A critical research challenge arises: constructing
an informative driving world model to enable perception annotation-free,
end-to-end planning via self-supervised learning. In this paper, we present
World4Drive, an end-to-end autonomous driving framework that employs vision
foundation models to build latent world models for generating and evaluating
multi-modal planning trajectories. Specifically, World4Drive first extracts
scene features, including driving intention and world latent representations
enriched with spatial-semantic priors provided by vision foundation models. It
then generates multi-modal planning trajectories based on current scene
features and driving intentions and predicts multiple intention-driven future
states within the latent space. Finally, it introduces a world model selector
module to evaluate and select the best trajectory. We achieve perception
annotation-free, end-to-end planning through self-supervised alignment between
actual future observations and predicted observations reconstructed from the
latent space. World4Drive achieves state-of-the-art performance without manual
perception annotations on both the open-loop nuScenes and closed-loop NavSim
benchmarks, demonstrating an 18.1\% relative reduction in L2 error, 46.7% lower
collision rate, and 3.75 faster training convergence. Codes will be accessed at
https://github.com/ucaszyp/World4Drive.

</details>


### [66] [De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection](https://arxiv.org/abs/2507.00608)
*Zehua Fu,Chenguang Liu,Yuyu Chen,Jiaqi Zhou,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: 论文提出DeSimPL方法，通过减少训练中简单样本的比例，提升自标记检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 交通场景中目标检测需要大量高质量标注数据，无监督域适应（UDA）方法受到关注。自标记方法虽简单高效，但性能不及域对齐方法，原因是训练中简单样本比例过高（简单标签偏差）。

Method: 提出DeSimPL方法，利用实例级记忆库更新伪标签策略，并引入对抗样本增加样本多样性，同时采用自适应加权损失避免后期训练中伪标签的假阳性问题。

Result: 实验表明DeSimPL有效减少简单样本比例，显著提升自标记检测器性能。

Conclusion: DeSimPL通过解决简单标签偏差问题，为自标记检测器提供了性能提升的有效途径。

Abstract: Despite its significant success, object detection in traffic and
transportation scenarios requires time-consuming and laborious efforts in
acquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation
(UDA) for object detection has recently gained increasing research attention.
UDA for object detection has been dominated by domain alignment methods, which
achieve top performance. Recently, self-labeling methods have gained popularity
due to their simplicity and efficiency. In this paper, we investigate the
limitations that prevent self-labeling detectors from achieving commensurate
performance with domain alignment methods. Specifically, we identify the high
proportion of simple samples during training, i.e., the simple-label bias, as
the central cause. We propose a novel approach called De-Simplifying Pseudo
Labels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level
memory bank to implement an innovative pseudo label updating strategy. Then,
adversarial samples are introduced during training to enhance the proportion.
Furthermore, we propose an adaptive weighted loss to avoid the model suffering
from an abundance of false positive pseudo labels in the late training period.
Experimental results demonstrate that DeSimPL effectively reduces the
proportion of simple samples during training, leading to a significant
performance improvement for self-labeling detectors. Extensive experiments
conducted on four benchmarks validate our analysis and conclusions.

</details>


### [67] [UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions](https://arxiv.org/abs/2507.00648)
*Siyuan Yao,Rui Zhu,Ziqi Wang,Wenqi Ren,Yanyang Yan,Xiaochun Cao*

Main category: cs.CV

TL;DR: UMDATrack提出了一种统一域适应框架，能够在恶劣天气条件下保持高质量的目标跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在白天条件下学习目标表示，而在恶劣天气条件下性能显著下降。

Method: 使用可控场景生成器合成少量未标记视频，设计域定制适配器（DCA）和目标感知置信对齐模块（TCA）。

Result: UMDATrack显著超越现有先进视觉跟踪器，达到新的最先进性能。

Conclusion: UMDATrack为恶劣天气条件下的目标跟踪提供了有效解决方案。

Abstract: Visual object tracking has gained promising progress in past decades. Most of
the existing approaches focus on learning target representation in
well-conditioned daytime data, while for the unconstrained real-world scenarios
with adverse weather conditions, e.g. nighttime or foggy environment, the
tremendous domain shift leads to significant performance degradation. In this
paper, we propose UMDATrack, which is capable of maintaining high-quality
target state prediction under various adverse weather conditions within a
unified domain adaptation framework. Specifically, we first use a controllable
scenario generator to synthesize a small amount of unlabeled videos (less than
2% frames in source daytime datasets) in multiple weather conditions under the
guidance of different text prompts. Afterwards, we design a simple yet
effective domain-customized adapter (DCA), allowing the target objects'
representation to rapidly adapt to various weather conditions without redundant
model updating. Furthermore, to enhance the localization consistency between
source and target domains, we propose a target-aware confidence alignment
module (TCA) following optimal transport theorem. Extensive experiments
demonstrate that UMDATrack can surpass existing advanced visual trackers and
lead new state-of-the-art performance by a significant margin. Our code is
available at https://github.com/Z-Z188/UMDATrack.

</details>


### [68] [LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment](https://arxiv.org/abs/2507.00659)
*Juelin Zhu,Shuaibang Peng,Long Wang,Hanlin Tan,Yu Liu,Maojun Zhang,Shen Yan*

Main category: cs.CV

TL;DR: 提出了一种基于低细节城市模型（LoD1）的空中视觉定位新方法LoD-Loc v2，通过粗到细的策略实现高精度定位。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖高细节模型（LoD2/3），但全球多数可用或规划中的模型为低细节（LoD1），限制了无人机在城市定位中的应用潜力。

Method: 采用粗到细策略：1）通过建筑分割网络提取轮廓；2）粗姿态选择阶段构建姿态成本体积；3）细姿态估计阶段使用粒子滤波和多光束跟踪。

Result: LoD-Loc v2首次实现了低细节模型的定位，并在高细节模型上提升了精度，显著优于现有方法。

Conclusion: LoD-Loc v2扩展了定位模型的适用范围，提升了精度和鲁棒性，为无人机全球城市定位提供了新可能。

Abstract: We propose a novel method for aerial visual localization over low
Level-of-Detail (LoD) city models. Previous wireframe-alignment-based method
LoD-Loc has shown promising localization results leveraging LoD models.
However, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the
majority of available models and those many countries plan to construct
nationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD
city models could unlock drones' potential for global urban localization. To
address these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine
strategy using explicit silhouette alignment to achieve accurate localization
over low-LoD city models in the air. Specifically, given a query image, LoD-Loc
v2 first applies a building segmentation network to shape building silhouettes.
Then, in the coarse pose selection stage, we construct a pose cost volume by
uniformly sampling pose hypotheses around a prior pose to represent the pose
probability distribution. Each cost of the volume measures the degree of
alignment between the projected and predicted silhouettes. We select the pose
with maximum value as the coarse pose. In the fine pose estimation stage, a
particle filtering method incorporating a multi-beam tracking approach is used
to efficiently explore the hypothesis space and obtain the final pose
estimation. To further facilitate research in this field, we release two
datasets with LoD1 city models covering 10.7 km , along with real RGB queries
and ground-truth pose annotations. Experimental results show that LoD-Loc v2
improves estimation accuracy with high-LoD models and enables localization with
low-LoD models for the first time. Moreover, it outperforms state-of-the-art
baselines by large margins, even surpassing texture-model-based methods, and
broadens the convergence basin to accommodate larger prior errors.

</details>


### [69] [A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation](https://arxiv.org/abs/2507.00676)
*Edward Effendy,Kuan-Wei Tseng,Rei Kawakami*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的框架，用于全身抓取，解决了姿态生成和运动填充问题，实现了真实稳定的物体交互。


<details>
  <summary>Details</summary>
Motivation: 解决全身抓取任务中姿态生成和运动填充的挑战，并克服手-物体交互数据稀缺的问题。

Method: 采用三阶段流程：抓取姿态生成、时间填充和LiftUp Transformer，并通过广义预训练提高数据效率。

Result: 在GRAB数据集上实验表明，该方法在连贯性、稳定性和视觉真实性上优于现有基线。

Conclusion: 模块化设计易于适应其他人体运动应用，展现了广泛的应用潜力。

Abstract: Accepted in the ICIP 2025
  We present a novel transformer-based framework for whole-body grasping that
addresses both pose generation and motion infilling, enabling realistic and
stable object interactions. Our pipeline comprises three stages: Grasp Pose
Generation for full-body grasp generation, Temporal Infilling for smooth motion
continuity, and a LiftUp Transformer that refines downsampled joints back to
high-resolution markers. To overcome the scarcity of hand-object interaction
data, we introduce a data-efficient Generalized Pretraining stage on large,
diverse motion datasets, yielding robust spatio-temporal representations
transferable to grasping tasks. Experiments on the GRAB dataset show that our
method outperforms state-of-the-art baselines in terms of coherence, stability,
and visual realism. The modular design also supports easy adaptation to other
human-motion applications.

</details>


### [70] [Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack](https://arxiv.org/abs/2507.00690)
*Keke Tang,Ziyong Du,Weilong Peng,Xiaofei Wang,Peican Zhu,Ligang Liu,Zhihong Tian*

Main category: cs.CV

TL;DR: CageAttack提出了一种基于笼形变形的对抗攻击方法，通过结构化变形生成自然的对抗点云，平衡了可转移性、不可防御性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法在点云上施加严格几何约束，限制了可转移性和不可防御性，而变形方法可能导致不自然的扭曲。

Method: CageAttack通过构建目标对象的笼形结构，对笼顶点施加扰动，实现平滑自然的变形。

Result: 在三个数据集上的七种3D深度神经网络分类器实验中，CageAttack在可转移性、不可防御性和真实性上优于现有方法。

Conclusion: CageAttack提供了一种有效的对抗攻击框架，代码将在接受后公开。

Abstract: Adversarial attacks on point clouds often impose strict geometric constraints
to preserve plausibility; however, such constraints inherently limit
transferability and undefendability. While deformation offers an alternative,
existing unstructured approaches may introduce unnatural distortions, making
adversarial point clouds conspicuous and undermining their plausibility. In
this paper, we propose CageAttack, a cage-based deformation framework that
produces natural adversarial point clouds. It first constructs a cage around
the target object, providing a structured basis for smooth, natural-looking
deformation. Perturbations are then applied to the cage vertices, which
seamlessly propagate to the point cloud, ensuring that the resulting
deformations remain intrinsic to the object and preserve plausibility.
Extensive experiments on seven 3D deep neural network classifiers across three
datasets show that CageAttack achieves a superior balance among
transferability, undefendability, and plausibility, outperforming
state-of-the-art methods. Codes will be made public upon acceptance.

</details>


### [71] [Rectifying Magnitude Neglect in Linear Attention](https://arxiv.org/abs/2507.00698)
*Qihang Fan,Huaibo Huang,Yuang Ai,ran He*

Main category: cs.CV

TL;DR: 论文分析了线性注意力（Linear Attention）性能下降的原因，并提出了一种改进方法MALA，通过引入查询（Query）的幅度信息，使其性能接近标准Softmax Attention。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然复杂度低，但性能显著低于标准Softmax Attention。本文旨在分析原因并提出改进方法。

Method: 基于线性注意力的公式分析，发现其忽略了查询的幅度信息，导致注意力分布无法动态调整。提出MALA方法，引入查询幅度信息。

Result: MALA在图像分类、目标检测、自然语言处理等多项任务中表现优异，性能接近Softmax Attention。

Conclusion: MALA通过引入查询幅度信息，显著提升了线性注意力的性能，同时保持了低复杂度的优势。

Abstract: As the core operator of Transformers, Softmax Attention exhibits excellent
global modeling capabilities. However, its quadratic complexity limits its
applicability to vision tasks. In contrast, Linear Attention shares a similar
formulation with Softmax Attention while achieving linear complexity, enabling
efficient global information modeling. Nevertheless, Linear Attention suffers
from a significant performance degradation compared to standard Softmax
Attention. In this paper, we analyze the underlying causes of this issue based
on the formulation of Linear Attention. We find that, unlike Softmax Attention,
Linear Attention entirely disregards the magnitude information of the Query.
This prevents the attention score distribution from dynamically adapting as the
Query scales. As a result, despite its structural similarity to Softmax
Attention, Linear Attention exhibits a significantly different attention score
distribution. Based on this observation, we propose Magnitude-Aware Linear
Attention (MALA), which modifies the computation of Linear Attention to fully
incorporate the Query's magnitude. This adjustment allows MALA to generate an
attention score distribution that closely resembles Softmax Attention while
exhibiting a more well-balanced structure. We evaluate the effectiveness of
MALA on multiple tasks, including image classification, object detection,
instance segmentation, semantic segmentation, natural language processing,
speech recognition, and image generation. Our MALA achieves strong results on
all of these tasks. Code will be available at https://github.com/qhfan/MALA

</details>


### [72] [BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving](https://arxiv.org/abs/2507.00707)
*Zeming Chen,Hang Zhao*

Main category: cs.CV

TL;DR: 论文提出BEV-VAE方法，通过多视图图像变分自编码器和潜在扩散变换器，实现自动驾驶中一致且可控的多视图图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法多将多视图图像生成视为2D任务，缺乏显式3D建模，而结构化表示对自动驾驶场景生成至关重要。

Method: BEV-VAE首先训练多视图图像变分自编码器，构建紧凑统一的BEV潜在空间，再通过潜在扩散变换器生成场景。

Result: 在nuScenes和Argoverse 2数据集上，BEV-VAE在3D一致重建和生成方面表现优异。

Conclusion: BEV-VAE通过显式3D建模和结构化表示，实现了自动驾驶场景中一致且可控的多视图生成。

Abstract: Multi-view image generation in autonomous driving demands consistent 3D scene
understanding across camera views. Most existing methods treat this problem as
a 2D image set generation task, lacking explicit 3D modeling. However, we argue
that a structured representation is crucial for scene generation, especially
for autonomous driving applications. This paper proposes BEV-VAE for consistent
and controllable view synthesis. BEV-VAE first trains a multi-view image
variational autoencoder for a compact and unified BEV latent space and then
generates the scene with a latent diffusion transformer. BEV-VAE supports
arbitrary view generation given camera configurations, and optionally 3D
layouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance
in both 3D consistent reconstruction and generation. The code is available at:
https://github.com/Czm369/bev-vae.

</details>


### [73] [TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving](https://arxiv.org/abs/2507.00709)
*Yiming Yang,Yueru Luo,Bingkun He,Hongbin Lin,Suzhong Fu,Chao Yan,Kun Tang,Xinrui Yan,Chao Zheng,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: TopoStreamer提出了一种端到端的时序感知模型，用于车道段拓扑推理，通过流式属性约束、动态车道边界位置编码和车道段去噪显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在一致位置嵌入和时序多属性学习上的局限性阻碍了准确的路网重建，因此需要改进。

Method: TopoStreamer引入了流式属性约束、动态车道边界位置编码和车道段去噪三项关键技术。

Result: 在OpenLane-V2数据集上，TopoStreamer在车道段感知和中心线感知任务中分别实现了+3.4% mAP和+2.1% OLS的性能提升。

Conclusion: TopoStreamer通过改进时序一致性和多样性学习，显著提升了车道段拓扑推理的准确性。

Abstract: Lane segment topology reasoning constructs a comprehensive road network by
capturing the topological relationships between lane segments and their
semantic types. This enables end-to-end autonomous driving systems to perform
road-dependent maneuvers such as turning and lane changing. However, the
limitations in consistent positional embedding and temporal multiple attribute
learning in existing methods hinder accurate roadnet reconstruction. To address
these issues, we propose TopoStreamer, an end-to-end temporal perception model
for lane segment topology reasoning. Specifically, TopoStreamer introduces
three key improvements: streaming attribute constraints, dynamic lane boundary
positional encoding, and lane segment denoising. The streaming attribute
constraints enforce temporal consistency in both centerline and boundary
coordinates, along with their classifications. Meanwhile, dynamic lane boundary
positional encoding enhances the learning of up-to-date positional information
within queries, while lane segment denoising helps capture diverse lane segment
patterns, ultimately improving model performance. Additionally, we assess the
accuracy of existing models using a lane boundary classification metric, which
serves as a crucial measure for lane-changing scenarios in autonomous driving.
On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements
over state-of-the-art methods, achieving substantial performance gains of +3.4%
mAP in lane segment perception and +2.1% OLS in centerline perception tasks.

</details>


### [74] [UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement](https://arxiv.org/abs/2507.00721)
*Xiao Zhang,Fei Wei,Yong Wang,Wenda Zhao,Feiyi Li,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 论文提出UPRE框架，通过联合优化文本提示和视觉表示，解决零样本域适应中的任务与视觉语言模型不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 零样本域适应（ZSDA）因目标域缺乏图像而具有挑战性，现有方法依赖手动设计的提示，忽略了检测任务与视觉语言模型的不匹配。

Method: 提出UPRE框架，结合多视角域提示和视觉表示增强模块，并采用多级增强策略对齐多模态表示。

Result: 在九个基准数据集上的实验表明，UPRE在ZSDA检测场景中表现优异。

Conclusion: UPRE通过联合优化提示和表示，有效解决了零样本域适应中的任务不匹配问题。

Abstract: Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the
lack of images in the target domain. Previous approaches leverage
Vision-Language Models (VLMs) to tackle this challenge, exploiting their
zero-shot learning capabilities. However, these methods primarily address
domain distribution shifts and overlook the misalignment between the detection
task and VLMs, which rely on manually crafted prompts. To overcome these
limitations, we propose the unified prompt and representation enhancement
(UPRE) framework, which jointly optimizes both textual prompts and visual
representations. Specifically, our approach introduces a multi-view domain
prompt that combines linguistic domain priors with detection-specific
knowledge, and a visual representation enhancement module that produces domain
style variations. Furthermore, we introduce multi-level enhancement strategies,
including relative domain distance and positive-negative separation, which
align multi-modal representations at the image level and capture diverse visual
representations at the instance level, respectively. Extensive experiments
conducted on nine benchmark datasets demonstrate the superior performance of
our framework in ZSDA detection scenarios. Code is available at
https://github.com/AMAP-ML/UPRE.

</details>


### [75] [Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features](https://arxiv.org/abs/2507.00724)
*Linghui Zhu,Yiming Li,Haiqin Weng,Yan Liu,Tianwei Zhang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种无害的模型所有权验证方法，通过解耦相似共同特征来保护个性化模型免受模型窃取攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对微调模型无效或引入风险，需开发新方法保护个性化模型的知识产权。

Method: 分三阶段：创建保留共同特征的影子模型；训练元分类器识别被盗模型；通过假设检验验证所有权。

Result: 在基准数据集上的实验验证了该方法能有效检测多种模型窃取类型。

Conclusion: 该方法为个性化模型提供了一种安全、有效的所有权验证解决方案。

Abstract: Large vision models achieve remarkable performance in various downstream
tasks, primarily by personalizing pre-trained models through fine-tuning with
private and valuable local data, which makes the personalized model a valuable
intellectual property for its owner. Similar to the era of traditional DNNs,
model stealing attacks also pose significant risks to these personalized
models. However, in this paper, we reveal that most existing defense methods
(developed for traditional DNNs), typically designed for models trained from
scratch, either introduce additional security risks, are prone to misjudgment,
or are even ineffective for fine-tuned models. To alleviate these problems,
this paper proposes a harmless model ownership verification method for
personalized models by decoupling similar common features. In general, our
method consists of three main stages. In the first stage, we create shadow
models that retain common features of the victim model while disrupting
dataset-specific features. We represent the dataset-specific features of the
victim model by the output differences between the shadow and victim models.
After that, a meta-classifier is trained to identify stolen models by
determining whether suspicious models contain the dataset-specific features of
the victim. In the third stage, we conduct model ownership verification by
hypothesis test to mitigate randomness and enhance robustness. Extensive
experiments on benchmark datasets verify the effectiveness of the proposed
method in detecting different types of model stealing simultaneously.

</details>


### [76] [Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network](https://arxiv.org/abs/2507.00739)
*An Le,Hung Nguyen,Sungbal Seo,You-Suk Bae,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种基于提升方案的新型双正交可调小波单元，放宽了正交性和滤波器长度限制，提升了CNN中的卷积、池化和下采样操作，显著提高了图像分类和异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统小波单元的正交性和滤波器长度限制限制了设计的灵活性，影响了CNN的性能。

Method: 通过提升方案构建双正交可调小波单元，放宽正交性和滤波器长度约束，优化卷积、池化和下采样操作。

Result: 在CIFAR-10和DTD数据集上分类准确率分别提升2.12%和9.73%；在MVTec异常检测任务中表现优于现有方法。

Conclusion: 该方法显著提升了CNN的性能，尤其在图像分类和异常检测任务中表现优异。

Abstract: This work introduces a novel biorthogonal tunable wavelet unit constructed
using a lifting scheme that relaxes both the orthogonality and equal filter
length constraints, providing greater flexibility in filter design. The
proposed unit enhances convolution, pooling, and downsampling operations,
leading to improved image classification and anomaly detection in convolutional
neural networks (CNN). When integrated into an 18-layer residual neural network
(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%
and on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its
effectiveness in capturing fine-grained details. Similar improvements were
observed in ResNet-34. For anomaly detection in the hazelnut category of the
MVTec Anomaly Detection dataset, the proposed method achieved competitive and
wellbalanced performance in both segmentation and detection tasks,
outperforming existing approaches in terms of accuracy and robustness.

</details>


### [77] [Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning](https://arxiv.org/abs/2507.00748)
*Bob Zhang,Haoran Li,Tao Zhang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Yanbin Hao*

Main category: cs.CV

TL;DR: 论文提出了一种基于强化学习的后训练策略，提升多模态大语言模型在多图像任务中的推理能力，通过合成高质量思维链数据和监督微调，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在单图像任务中表现优异，但在多图像组合和多模态指令的实际应用中性能下降，需要改进跨图像推理和泛化能力。

Method: 采用强化学习后训练策略，包括合成高质量思维链数据、监督微调（LoRA）、拒绝采样和基于规则的强化学习。

Result: 在多图像基准测试中性能显著提升（MIG-Bench +9.04%，其他基准 +4.98%），并在多图像感知中表现出强泛化能力（BLINK +3.1%，MMIU +2.4%）。

Conclusion: 该方法有效提升了多模态大语言模型在多图像任务中的推理和泛化能力，为实际应用提供了有力支持。

Abstract: Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding
in single-image scenarios with textual references. However, their performance
degrades when handling real-world applications involving complex multi-image
compositions and multimodal instructions, which reveals limitations in
cross-image reasoning and generalization. To address these challenges, we adopt
a Reinforcement Learning (RL) based post-training strategy to improve the
reasoning performance of MLLMs in multi-image grounding tasks. Our approach
begins with synthesizing high-quality chain-of-thought (CoT) data for
cold-start initialization, followed by supervised fine-tuning (SFT) using
low-rank adaptation (LoRA). The cold-start training stage enables the model to
identify correct solutions. Subsequently, we perform rejection sampling using
the merged SFT model to curate high-quality RL data and leverage rule-based RL
to guide the model toward optimal reasoning paths. Extensive experimental
results demonstrate the effectiveness of our approach, achieving +9.04\%
improvements on MIG-Bench and +4.98\% improvements on several out-of-domain
reasoning grounding benchmarks over the SFT baseline. Furthermore, our approach
exhibits strong generalization in multi-image perception, with gains of +3.1\%
and +2.4\% over the base model on subsets of the BLINK and MMIU benchmarks,
respectively.

</details>


### [78] [Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs](https://arxiv.org/abs/2507.00754)
*Selim Kuzucu,Muhammad Ferjad Naeem,Anna Kukleva,Federico Tombari,Bernt Schiele*

Main category: cs.CV

TL;DR: LUViT通过联合预训练策略解决LLM与ViT的模态不匹配问题，提升视觉任务性能。


<details>
  <summary>Details</summary>
Motivation: 利用LLM的语义知识和推理能力增强视觉任务，但直接融合因模态不匹配导致效果不佳。

Method: 采用MAE预训练ViT，同时用LoRA层训练LLM块，实现联合优化。

Result: LUViT显著提升下游视觉任务性能。

Conclusion: LUViT为利用LLM知识进行视觉理解提供了更有效和高效的途径。

Abstract: The integration of Large Language Model (LLMs) blocks with Vision
Transformers (ViTs) holds immense promise for vision-only tasks by leveraging
the rich semantic knowledge and reasoning capabilities of LLMs. However, a
fundamental challenge lies in the inherent modality mismatch between
text-centric pretraining of LLMs and vision-centric training of ViTs. Direct
fusion often fails to fully exploit the LLM's potential and suffers from
unstable finetuning. As a result, LLM blocks are kept frozen while only the
vision components are learned. As a remedy to these challenges, we introduce
Language-Unlocked Vision Transformers (LUViT), a novel approach that bridges
this modality mismatch through a synergistic pre-training strategy. LUViT
co-adapts a ViT backbone and an LLM fusion block by (1) employing Masked
Auto-Encoding (MAE) to pre-train the ViT for richer visual representations, and
(2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM
block using the MAE objective. This joint optimization guides the ViT to
produce LLM-aligned features and the LLM to effectively interpret visual
information. We demonstrate through extensive experiments that LUViT
significantly improves performance on various downstream vision tasks,
showcasing a more effective and efficient pathway to harness LLM knowledge for
visual understanding.

</details>


### [79] [OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection](https://arxiv.org/abs/2507.00789)
*Ziji Lu*

Main category: cs.CV

TL;DR: OptiPrune是一个结合分布感知噪声优化和基于相似性的token剪枝的统一框架，旨在提升文本到图像扩散模型的语义对齐和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声优化或token剪枝中难以平衡语义对齐和计算效率，导致性能或效率的妥协。

Method: OptiPrune通过分布感知噪声优化模块和硬件高效的token剪枝策略，同时优化初始噪声和减少计算开销。

Result: 在基准数据集上，OptiPrune实现了最先进的提示-图像一致性，并显著降低了计算成本。

Conclusion: OptiPrune有效解决了语义对齐和计算效率的挑战，为资源受限硬件上的部署提供了可行方案。

Abstract: Text-to-image diffusion models often struggle to achieve accurate semantic
alignment between generated images and text prompts while maintaining
efficiency for deployment on resource-constrained hardware. Existing approaches
either incur substantial computational overhead through noise optimization or
compromise semantic fidelity by aggressively pruning tokens. In this work, we
propose OptiPrune, a unified framework that combines distribution-aware initial
noise optimization with similarity-based token pruning to address both
challenges simultaneously. Specifically, (1) we introduce a distribution-aware
noise optimization module guided by attention scores to steer the initial
latent noise toward semantically meaningful regions, mitigating issues such as
subject neglect and feature entanglement; (2) we design a hardware-efficient
token pruning strategy that selects representative base tokens via patch-wise
similarity, injects randomness to enhance generalization, and recovers pruned
tokens using maximum similarity copying before attention operations. Our method
preserves the Gaussian prior during noise optimization and enables efficient
inference without sacrificing alignment quality. Experiments on benchmark
datasets, including Animal-Animal, demonstrate that OptiPrune achieves
state-of-the-art prompt-image consistency with significantly reduced
computational cost.

</details>


### [80] [LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling](https://arxiv.org/abs/2507.00790)
*Huaqiu Li,Yong Wang,Tongwen Huang,Hailang Huang,Haoqian Wang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出一种基于预训练潜在扩散模型的无数据集统一图像恢复方法，通过循环后验采样和多模态理解模型提升泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法针对特定任务设计，泛化性差，或依赖配对数据集，受限于闭集约束。

Method: 利用预训练潜在扩散模型，结合多模态理解模型提供语义先验，通过轻量级模块对齐退化输入，并采用循环细化后验采样。

Result: 实验表明，该方法优于现有技术，验证了其有效性和鲁棒性。

Conclusion: 该方法为统一图像恢复提供了一种高效、泛化性强的解决方案。

Abstract: Unified image restoration is a significantly challenging task in low-level
vision. Existing methods either make tailored designs for specific tasks,
limiting their generalizability across various types of degradation, or rely on
training with paired datasets, thereby suffering from closed-set constraints.
To address these issues, we propose a novel, dataset-free, and unified approach
through recurrent posterior sampling utilizing a pretrained latent diffusion
model. Our method incorporates the multimodal understanding model to provide
sematic priors for the generative model under a task-blind condition.
Furthermore, it utilizes a lightweight module to align the degraded input with
the generated preference of the diffusion model, and employs recurrent
refinement for posterior sampling. Extensive experiments demonstrate that our
method outperforms state-of-the-art methods, validating its effectiveness and
robustness. Our code and data will be available at
https://github.com/AMAP-ML/LD-RPS.

</details>


### [81] [TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency](https://arxiv.org/abs/2507.00802)
*Minye Shao,Xingyu Miao,Haoran Duan,Zeyu Wang,Jingkun Chen,Yawen Huang,Xian Wu,Jingjing Deng,Yang Long,Yefeng Zheng*

Main category: cs.CV

TL;DR: TRACE是一个基于2D多模态条件扩散的框架，用于生成具有时空对齐的3D医学图像，解决了现有方法在解剖保真度、轴向长度和计算成本上的限制。


<details>
  <summary>Details</summary>
Motivation: 当前3D医学图像生成方法存在解剖保真度低、轴向长度受限和计算成本高的问题，限制了其在资源有限地区的应用。

Method: TRACE通过将2D切片建模为视频帧对，结合分割先验和放射学报告进行解剖对齐，并使用光流保持时间一致性。推理时采用重叠帧策略生成灵活长度的序列。

Result: 实验表明，TRACE在计算效率和保持解剖保真度与时空一致性之间取得了平衡。

Conclusion: TRACE为3D医学图像生成提供了一种高效且可靠的解决方案，适用于临床实践。

Abstract: 3D medical image generation is essential for data augmentation and patient
privacy, calling for reliable and efficient models suited for clinical
practice. However, current methods suffer from limited anatomical fidelity,
restricted axial length, and substantial computational cost, placing them
beyond reach for regions with limited resources and infrastructure. We
introduce TRACE, a framework that generates 3D medical images with
spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach.
TRACE models sequential 2D slices as video frame pairs, combining segmentation
priors and radiology reports for anatomical alignment, incorporating optical
flow to sustain temporal coherence. During inference, an overlapping-frame
strategy links frame pairs into a flexible length sequence, reconstructed into
a spatiotemporally and anatomically aligned 3D volume. Experimental results
demonstrate that TRACE effectively balances computational efficiency with
preserving anatomical fidelity and spatiotemporal consistency. Code is
available at: https://github.com/VinyehShaw/TRACE.

</details>


### [82] [CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs](https://arxiv.org/abs/2507.00817)
*Jiaming Zhang,Rui Hu,Qing Guo,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: CAVALRY-V是一个针对视频多模态大语言模型（V-MLLMs）的对抗攻击框架，通过双目标语义-视觉损失函数和高效的两阶段生成器，显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 探索V-MLLMs在对抗攻击中的脆弱性，解决跨模态推理、时间依赖性和计算限制的挑战。

Method: 提出双目标语义-视觉损失函数和两阶段生成器框架，结合大规模预训练和微调。

Result: 在多个视频理解基准测试中，CAVALRY-V比现有攻击方法平均提升22.8%，并在图像理解任务中表现优异。

Conclusion: CAVALRY-V为多模态系统的对抗研究提供了基础性方法。

Abstract: Video Multimodal Large Language Models (V-MLLMs) have shown impressive
capabilities in temporal reasoning and cross-modal understanding, yet their
vulnerability to adversarial attacks remains underexplored due to unique
challenges: complex cross-modal reasoning mechanisms, temporal dependencies,
and computational constraints. We present CAVALRY-V (Cross-modal
Language-Vision Adversarial Yielding for Videos), a novel framework that
directly targets the critical interface between visual perception and language
generation in V-MLLMs. Our approach introduces two key innovations: (1) a
dual-objective semantic-visual loss function that simultaneously disrupts the
model's text generation logits and visual representations to undermine
cross-modal integration, and (2) a computationally efficient two-stage
generator framework that combines large-scale pre-training for cross-model
transferability with specialized fine-tuning for spatiotemporal coherence.
Empirical evaluation on comprehensive video understanding benchmarks
demonstrates that CAVALRY-V significantly outperforms existing attack methods,
achieving 22.8% average improvement over the best baseline attacks on both
commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,
InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves
flexibility through implicit temporal coherence modeling rather than explicit
regularization, enabling significant performance improvements even on image
understanding (34.4% average gain). This capability demonstrates CAVALRY-V's
potential as a foundational approach for adversarial research across multimodal
systems.

</details>


### [83] [Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data](https://arxiv.org/abs/2507.00822)
*Yasser El Jarida,Youssef Iraqi,Loubna Mekouar*

Main category: cs.CV

TL;DR: 提出了一种基于CNN的方法，利用Blender生成的合成粒子图像进行PSD测量，评估了三种模型，EfficientNet-B0表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统PSD测量方法耗时且受限于粒子重叠，需要自动化实时解决方案。

Method: 使用Blender生成合成粒子图像训练CNN模型（ResNet-50、InceptionV3、EfficientNet-B0），预测PSD参数。

Result: 三种模型精度相当，EfficientNet-B0计算效率最高，适合工业实时部署。

Conclusion: 合成数据训练CNN在工业PSD监测中具有潜力，代码已开源。

Abstract: Accurate particle size distribution (PSD) measurement is important in
industries such as mining, pharmaceuticals, and fertilizer manufacturing,
significantly influencing product quality and operational efficiency.
Traditional PSD methods like sieve analysis and laser diffraction are manual,
time-consuming, and limited by particle overlap. Recent developments in
convolutional neural networks (CNNs) enable automated, real-time PSD estimation
directly from particle images. In this work, we present a CNN-based methodology
trained on realistic synthetic particle imagery generated using Blender's
advanced rendering capabilities. Synthetic data sets using this method can
replicate various industrial scenarios by systematically varying particle
shapes, textures, lighting, and spatial arrangements that closely resemble the
actual configurations. We evaluated three CNN-based architectures, ResNet-50,
InceptionV3, and EfficientNet-B0, for predicting critical PSD parameters (d10,
d50, d90). Results demonstrated comparable accuracy across models, with
EfficientNet-B0 achieving the best computational efficiency suitable for
real-time industrial deployment. This approach shows the effectiveness of
realistic synthetic data for robust CNN training, which offers significant
potential for automated industrial PSD monitoring. The code is released at :
https://github.com/YasserElj/Synthetic-Granular-Gen

</details>


### [84] [High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery](https://arxiv.org/abs/2507.00825)
*Hongxing Peng,Lide Chen,Hui Zhu,Yan Chen*

Main category: cs.CV

TL;DR: HEGS-DETR是一种专为无人机设计的检测Transformer框架，通过高频增强语义网络、高效小目标金字塔策略等模块，显著提升了小目标和密集目标的检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的目标检测面临小目标、高密度分布和复杂背景等挑战，现有算法依赖手工组件且泛化能力有限。

Method: 提出HFESNet作为骨干网络，保留高频细节；ESOP策略融合高分辨率特征；SQR和GAPE模块增强解码器稳定性和定位精度。

Result: 在VisDrone数据集上，AP$_{50}$提升5.1%，AP提升3.8%，同时保持实时速度和减少参数。

Conclusion: HEGS-DETR有效解决了无人机目标检测的挑战，性能显著优于基线方法。

Abstract: Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial
challenges, including small target sizes, high-density distributions, and
cluttered backgrounds in UAV imagery. Current algorithms often depend on
hand-crafted components like anchor boxes, which demand fine-tuning and exhibit
limited generalization, and Non-Maximum Suppression (NMS), which is
threshold-sensitive and prone to misclassifying dense objects. These generic
architectures thus struggle to adapt to aerial imaging characteristics,
resulting in performance limitations. Moreover, emerging end-to-end frameworks
have yet to effectively mitigate these aerial-specific challenges.To address
these issues, we propose HEGS-DETR, a comprehensively enhanced, real-time
Detection Transformer framework tailored for UAVs. First, we introduce the
High-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone.
HFESNet preserves critical high-frequency spatial details to extract robust
semantic features, thereby improving discriminative capability for small and
occluded targets in complex backgrounds. Second, our Efficient Small Object
Pyramid (ESOP) strategy strategically fuses high-resolution feature maps with
minimal computational overhead, significantly boosting small object detection.
Finally, the proposed Selective Query Recollection (SQR) and Geometry-Aware
Positional Encoding (GAPE) modules enhance the detector's decoder stability and
localization accuracy, effectively optimizing bounding boxes and providing
explicit spatial priors for dense scenes. Experiments on the VisDrone dataset
demonstrate that HEGS-DETR achieves a 5.1\% AP$_{50}$ and 3.8\% AP increase
over the baseline, while maintaining real-time speed and reducing parameter
count by 4M.

</details>


### [85] [Do Echo Top Heights Improve Deep Learning Nowcasts?](https://arxiv.org/abs/2507.00845)
*Peter Pavlík,Marc Schleiss,Anna Bou Ezzeddine,Viera Rozinajová*

Main category: cs.CV

TL;DR: 论文探讨了在深度学习降水临近预报中使用Echo Top Height（ETH）作为辅助输入变量的潜力，发现ETH在低雨强时能提升预报能力，但在高雨强时效果不一致。


<details>
  <summary>Details</summary>
Motivation: 降水临近预报对天气敏感行业至关重要，但现有深度学习模型多依赖2D雷达反射率，忽略了3D雷达数据的垂直信息。ETH作为2D投影可能提供额外信息。

Method: 研究分析了ETH与雷达反射率的关系，并设计了一个单通道3D U-Net模型，同时处理雷达反射率和ETH作为输入。

Result: ETH在低雨强阈值下能提升预报能力，但在高雨强时效果不稳定，且模型会系统性低估降水强度。

Conclusion: ETH在某些情况下有帮助，但也可能增加误差方差，研究为评估额外变量对临近预报的贡献提供了基础。

Abstract: Precipitation nowcasting -- the short-term prediction of rainfall using
recent radar observations -- is critical for weather-sensitive sectors such as
transportation, agriculture, and disaster mitigation. While recent deep
learning models have shown promise in improving nowcasting skill, most
approaches rely solely on 2D radar reflectivity fields, discarding valuable
vertical information available in the full 3D radar volume. In this work, we
explore the use of Echo Top Height (ETH), a 2D projection indicating the
maximum altitude of radar reflectivity above a given threshold, as an auxiliary
input variable for deep learning-based nowcasting. We examine the relationship
between ETH and radar reflectivity, confirming its relevance for predicting
rainfall intensity. We implement a single-pass 3D U-Net that processes both the
radar reflectivity and ETH as separate input channels. While our models are
able to leverage ETH to improve skill at low rain-rate thresholds, results are
inconsistent at higher intensities and the models with ETH systematically
underestimate precipitation intensity. Three case studies are used to
illustrate how ETH can help in some cases, but also confuse the models and
increase the error variance. Nonetheless, the study serves as a foundation for
critically assessing the potential contribution of additional variables to
nowcasting performance.

</details>


### [86] [UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection](https://arxiv.org/abs/2507.00849)
*Wei Li,Jiaman Tang,Yang Li,Beihao Xia,Ligang Tan,Hongmao Qin*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba架构的多模态无人机目标检测框架UAVD-Mamba，通过可变形令牌Mamba块（DTMB）和多尺度特征提取，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测面临遮挡、小目标和形状不规则等挑战，需要一种鲁棒且高效的多模态方法。

Method: 设计了DTMB模块，结合可变形卷积和普通卷积生成令牌；采用RGB和红外模态的双DTMB结构，并通过融合Mamba块优化特征互补；堆叠多尺度DTMB并引入检测颈部模块（DNM）提升小目标检测。

Result: 在DroneVehicle数据集上，mAP指标比基线OAFA方法提高了3.6%。

Conclusion: UAVD-Mamba通过多模态和多尺度设计，有效解决了无人机目标检测中的关键问题，性能显著提升。

Abstract: Unmanned Aerial Vehicle (UAV) object detection has been widely used in
traffic management, agriculture, emergency rescue, etc. However, it faces
significant challenges, including occlusions, small object sizes, and irregular
shapes. These challenges highlight the necessity for a robust and efficient
multimodal UAV object detection method. Mamba has demonstrated considerable
potential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a
multimodal UAV object detection framework based on Mamba architectures. To
improve geometric adaptability, we propose the Deformable Token Mamba Block
(DTMB) to generate deformable tokens by incorporating adaptive patches from
deformable convolutions alongside normal patches from normal convolutions,
which serve as the inputs to the Mamba Block. To optimize the multimodal
feature complementarity, we design two separate DTMBs for the RGB and infrared
(IR) modalities, with the outputs from both DTMBs integrated into the Mamba
Block for feature extraction and into the Fusion Mamba Block for feature
fusion. Additionally, to improve multiscale object detection, especially for
small objects, we stack four DTMBs at different scales to produce multiscale
feature representations, which are then sent to the Detection Neck for Mamba
(DNM). The DNM module, inspired by the YOLO series, includes modifications to
the SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In
particular, we employ cross-enhanced spatial attention before the DTMB and
cross-channel attention after the Fusion Mamba Block to extract more
discriminative features. Experimental results on the DroneVehicle dataset show
that our method outperforms the baseline OAFA method by 3.6% in the mAP metric.
Codes will be released at https://github.com/GreatPlum-hnu/UAVD-Mamba.git.

</details>


### [87] [Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting](https://arxiv.org/abs/2507.00852)
*Fatemeh Sadat Daneshmand*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mask R-CNN的计算机视觉系统，使工业机器人能够在非结构化环境中检测和抓取笔组件，无需固定位置约束，并在不同光照条件下保持稳定性能。


<details>
  <summary>Details</summary>
Motivation: 工业4.0中的柔性制造系统需要机器人能够在非结构化环境中处理物体，而无需严格的定位约束。

Method: 采用Mask R-CNN方法，在ZHAW的完整笔制造线上实现和评估，解决了无位置约束的物体检测、极端光照变化的鲁棒性以及低成本相机的可靠性能三大挑战。

Result: 系统在多样化光照条件下实现了95%的检测准确率，消除了结构化组件放置的需求，减少了30%的设置时间，显著提高了制造灵活性。

Conclusion: 通过四种不同光照场景的广泛测试验证了该系统的实用性，展示了其在现实工业部署中的应用潜力。

Abstract: Flexible manufacturing systems in Industry 4.0 require robots capable of
handling objects in unstructured environments without rigid positioning
constraints. This paper presents a computer vision system that enables
industrial robots to detect and grasp pen components in arbitrary orientations
without requiring structured trays, while maintaining robust performance under
varying lighting conditions. We implement and evaluate a Mask R-CNN-based
approach on a complete pen manufacturing line at ZHAW, addressing three
critical challenges: object detection without positional constraints,
robustness to extreme lighting variations, and reliable performance with
cost-effective cameras. Our system achieves 95% detection accuracy across
diverse lighting conditions while eliminating the need for structured component
placement, demonstrating a 30% reduction in setup time and significant
improvement in manufacturing flexibility. The approach is validated through
extensive testing under four distinct lighting scenarios, showing practical
applicability for real-world industrial deployment.

</details>


### [88] [SafeMap: Robust HD Map Construction from Incomplete Observations](https://arxiv.org/abs/2507.00861)
*Xiaoshuai Hao,Lingdong Kong,Rong Yin,Pengwei Wang,Jing Zhang,Yunfeng Diao,Shu Zhao*

Main category: cs.CV

TL;DR: SafeMap是一种新型框架，通过G-PVR和D-BEVC模块，解决了多视角相机数据不完整时的高清地图构建问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角相机数据不完整时表现不佳，影响自动驾驶的高清地图构建准确性。

Method: SafeMap包含G-PVR模块（基于高斯分布的视角重建）和D-BEVC模块（基于蒸馏的鸟瞰图校正），动态优先处理信息丰富区域并校正不完整观测的BEV表示。

Result: 实验表明，SafeMap在完整和不完整数据场景下均显著优于现有方法。

Conclusion: SafeMap提供了一种即插即用的解决方案，提升了高清地图构建的鲁棒性和可靠性。

Abstract: Robust high-definition (HD) map construction is vital for autonomous driving,
yet existing methods often struggle with incomplete multi-view camera data.
This paper presents SafeMap, a novel framework specifically designed to secure
accuracy even when certain camera views are missing. SafeMap integrates two key
components: the Gaussian-based Perspective View Reconstruction (G-PVR) module
and the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.
G-PVR leverages prior knowledge of view importance to dynamically prioritize
the most informative regions based on the relationships among available camera
views. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV
representations derived from incomplete observations. Together, these
components facilitate the end-to-end map reconstruction and robust HD map
generation. SafeMap is easy to implement and integrates seamlessly into
existing systems, offering a plug-and-play solution for enhanced robustness.
Experimental results demonstrate that SafeMap significantly outperforms
previous methods in both complete and incomplete scenarios, highlighting its
superior performance and reliability.

</details>


### [89] [Is Visual in-Context Learning for Compositional Medical Tasks within Reach?](https://arxiv.org/abs/2507.00868)
*Simon Reiß,Zdravko Marinov,Alexander Jaus,Constantin Seibold,M. Saquib Sarfraz,Erik Rodner,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文探讨了视觉上下文学习的潜力，使单一模型能处理多任务并在测试时适应新任务，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 旨在解决复杂任务，通过单一模型灵活定义视觉流程，适应任务序列而非单个任务。

Method: 研究了视觉上下文学习架构的特性与限制，提出基于合成组合任务生成引擎的新训练方法，并探索掩码训练目标。

Result: 为多模态医疗任务序列提供了重要见解，同时指出了需解决的挑战。

Conclusion: 视觉上下文学习在适应任务序列和解决复杂组合任务方面具有潜力，但仍需进一步研究。

Abstract: In this paper, we explore the potential of visual in-context learning to
enable a single model to handle multiple tasks and adapt to new tasks during
test time without re-training. Unlike previous approaches, our focus is on
training in-context learners to adapt to sequences of tasks, rather than
individual tasks. Our goal is to solve complex tasks that involve multiple
intermediate steps using a single model, allowing users to define entire vision
pipelines flexibly at test time. To achieve this, we first examine the
properties and limitations of visual in-context learning architectures, with a
particular focus on the role of codebooks. We then introduce a novel method for
training in-context learners using a synthetic compositional task generation
engine. This engine bootstraps task sequences from arbitrary segmentation
datasets, enabling the training of visual in-context learners for compositional
tasks. Additionally, we investigate different masking-based training objectives
to gather insights into how to train models better for solving complex,
compositional tasks. Our exploration not only provides important insights
especially for multi-modal medical task sequences but also highlights
challenges that need to be addressed.

</details>


### [90] [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2507.00898)
*Zifu Wan,Ce Zhang,Silong Yong,Martin Q. Ma,Simon Stepputtis,Louis-Philippe Morency,Deva Ramanan,Katia Sycara,Yaqi Xie*

Main category: cs.CV

TL;DR: ONLY是一种无需训练的实时解码方法，通过单次查询和单层干预减少视觉语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型在多模态任务中表现优异，但其幻觉问题限制了实际应用的可靠性。现有方法需要多次查询，影响实时性。

Method: 提出ONLY方法，通过文本到视觉熵比选择性放大关键文本信息，仅需单次查询和单层干预。

Result: 实验表明，ONLY在多个基准测试中优于现有方法，且实现简单、计算成本低。

Conclusion: ONLY为视觉语言模型的实时部署提供了一种高效解决方案。

Abstract: Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm
for understanding and reasoning about image input through textual responses.
Although they have achieved remarkable performance across a range of
multi-modal tasks, they face the persistent challenge of hallucination, which
introduces practical weaknesses and raises concerns about their reliable
deployment in real-world applications. Existing work has explored contrastive
decoding approaches to mitigate this issue, where the output of the original
LVLM is compared and contrasted with that of a perturbed version. However,
these methods require two or more queries that slow down LVLM response
generation, making them less suitable for real-time applications. To overcome
this limitation, we propose ONLY, a training-free decoding approach that
requires only a single query and a one-layer intervention during decoding,
enabling efficient real-time deployment. Specifically, we enhance textual
outputs by selectively amplifying crucial textual information using a
text-to-visual entropy ratio for each token. Extensive experimental results
demonstrate that our proposed ONLY consistently outperforms state-of-the-art
methods across various benchmarks while requiring minimal implementation effort
and computational cost. Code is available at https://github.com/zifuwan/ONLY.

</details>


### [91] [Masks make discriminative models great again!](https://arxiv.org/abs/2507.00916)
*Tianshi Cao,Marie-Julie Rakotosaona,Ben Poole,Federico Tombari,Michael Niemeyer*

Main category: cs.CV

TL;DR: Image2GS提出了一种从单图像重建3D场景的新方法，专注于图像到3D的提升部分，通过分离提升问题和补全问题，提高了重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决从单图像重建3D场景的挑战，特别是提升部分（将图像转换为可见的3D模型），避免补全问题（生成输入中未出现的内容）带来的不确定性。

Method: 使用优化的3D高斯斑点生成的可见性掩码，在训练中排除源视角不可见的区域，采用掩码训练策略。

Result: 在可见区域的重建质量显著优于基线方法，尽管仅训练掩码区域，但在完整场景评估中仍与最先进的判别模型竞争。

Conclusion: 判别模型在处理未见区域时存在困难，将图像到3D提升作为独立问题并采用专门技术具有优势。

Abstract: We present Image2GS, a novel approach that addresses the challenging problem
of reconstructing photorealistic 3D scenes from a single image by focusing
specifically on the image-to-3D lifting component of the reconstruction
process. By decoupling the lifting problem (converting an image to a 3D model
representing what is visible) from the completion problem (hallucinating
content not present in the input), we create a more deterministic task suitable
for discriminative models. Our method employs visibility masks derived from
optimized 3D Gaussian splats to exclude areas not visible from the source view
during training. This masked training strategy significantly improves
reconstruction quality in visible regions compared to strong baselines.
Notably, despite being trained only on masked regions, Image2GS remains
competitive with state-of-the-art discriminative models trained on full target
images when evaluated on complete scenes. Our findings highlight the
fundamental struggle discriminative models face when fitting unseen regions and
demonstrate the advantages of addressing image-to-3D lifting as a distinct
problem with specialized techniques.

</details>


### [92] [MVP: Winning Solution to SMP Challenge 2025 Video Track](https://arxiv.org/abs/2507.00950)
*Liliang Ye,Yunyao Zhang,Yafeng Wu,Yi-Ping Phoebe Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.CV

TL;DR: MVP模型通过整合视频特征、用户元数据和上下文信息，成功预测社交媒体视频的流行度，并在SMP Challenge 2025中获胜。


<details>
  <summary>Details</summary>
Motivation: 社交媒体视频流行度预测对内容推荐、趋势检测和用户参与具有重要价值。

Method: MVP整合预训练模型的视频特征、用户元数据和上下文信息，采用梯度提升回归模型进行预测。

Result: MVP在SMP Challenge 2025的Video Track中排名第一。

Conclusion: MVP是一种有效且可靠的多模态视频流行度预测方法。

Abstract: Social media platforms serve as central hubs for content dissemination,
opinion expression, and public engagement across diverse modalities. Accurately
predicting the popularity of social media videos enables valuable applications
in content recommendation, trend detection, and audience engagement. In this
paper, we present Multimodal Video Predictor (MVP), our winning solution to the
Video Track of the SMP Challenge 2025. MVP constructs expressive post
representations by integrating deep video features extracted from pretrained
models with user metadata and contextual information. The framework applies
systematic preprocessing techniques, including log-transformations and outlier
removal, to improve model robustness. A gradient-boosted regression model is
trained to capture complex patterns across modalities. Our approach ranked
first in the official evaluation of the Video Track, demonstrating its
effectiveness and reliability for multimodal video popularity prediction on
social platforms. The source code is available at
https://anonymous.4open.science/r/SMPDVideo.

</details>


### [93] [Surgical Neural Radiance Fields from One Image](https://arxiv.org/abs/2507.00969)
*Alberto Neri,Maximilan Fehrentz,Veronica Penza,Leonardo S. Mattos,Nazim Haouchine*

Main category: cs.CV

TL;DR: 该论文提出了一种利用单张术中图像和术前数据快速训练NeRF的方法，解决了手术场景中多视图数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: NeRF在3D重建和视图合成中表现优异，但在手术场景中，由于时间限制无法获取足够的多视图数据。本文旨在通过单张术中图像和术前数据实现高效训练。

Method: 结合术前MRI数据定义相机视角和图像，通过神经风格迁移（WTC2和STROTSS）将术中图像外观转移到预构建的训练集，实现单图像快速训练。

Result: 在四个神经外科案例中验证，与真实手术显微镜图像训练的NeRF模型相比，重建保真度和风格对齐度高，结构相似性接近真实数据。

Conclusion: 该方法证明了在手术场景中单图像训练NeRF的可行性，突破了传统多视图方法的限制。

Abstract: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgical scenarios.
  Methods: We leverage preoperative MRI data to define the set of camera
viewpoints and images needed for robust and unobstructed training.
Intraoperatively, the appearance of the surgical image is transferred to the
pre-constructed training set through neural style transfer, specifically
combining WTC2 and STROTSS to prevent over-stylization. This process enables
the creation of a dataset for instant and fast single-image NeRF training.
  Results: The method is evaluated with four clinical neurosurgical cases.
Quantitative comparisons to NeRF models trained on real surgical microscope
images demonstrate strong synthesis agreement, with similarity metrics
indicating high reconstruction fidelity and stylistic alignment. When compared
with ground truth, our method demonstrates high structural similarity,
confirming good reconstruction quality and texture preservation.
  Conclusion: Our approach demonstrates the feasibility of single-image NeRF
training in surgical settings, overcoming the limitations of traditional
multi-view methods.

</details>


### [94] [RTMap: Real-Time Recursive Mapping with Change Detection and Localization](https://arxiv.org/abs/2507.00980)
*Yuheng Du,Sheng Yang,Lingxuan Wang,Zhenghua Hou,Chengying Cai,Zhitao Tan,Mingxia Chen,Shi-Sheng Huang,Qiang Li*

Main category: cs.CV

TL;DR: RTMap通过多轨迹众包高精地图，提升单次遍历方法的准确性，解决感知误差、遮挡和多智能体观测融合问题。


<details>
  <summary>Details</summary>
Motivation: 现有在线高精地图方法存在感知误差、交通密集遮挡和多智能体观测融合不足的问题。

Method: RTMap采用不确定性感知的位置建模、概率感知的定位和实时道路结构变化检测，以端到端方式解决核心挑战。

Result: 在多个自动驾驶数据集上，RTMap在先验地图质量和定位精度上表现优异，支持下游预测和规划模块。

Conclusion: RTMap通过异步更新众包地图，持续提升地图准确性和新鲜度，代码将开源。

Abstract: While recent online HD mapping methods relieve burdened offline pipelines and
solve map freshness, they remain limited by perceptual inaccuracies, occlusion
in dense traffic, and an inability to fuse multi-agent observations. We propose
RTMap to enhance these single-traversal methods by persistently crowdsourcing a
multi-traversal HD map as a self-evolutional memory. On onboard agents, RTMap
simultaneously addresses three core challenges in an end-to-end fashion: (1)
Uncertainty-aware positional modeling for HD map elements, (2)
probabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3)
real-time detection for possible road structural changes. Experiments on
several public autonomous driving datasets demonstrate our solid performance on
both the prior-aided map quality and the localization accuracy, demonstrating
our effectiveness of robustly serving downstream prediction and planning
modules while gradually improving the accuracy and freshness of the
crowdsourced prior-map asynchronously. Our source-code will be made publicly
available at https://github.com/CN-ADLab/RTMap (Camera ready version
incorporating reviewer suggestions will be updated soon).

</details>


### [95] [Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations](https://arxiv.org/abs/2507.00981)
*Jack Nugent,Siyang Wu,Zeyu Ma,Beining Han,Meenal Parakh,Abhishek Joshi,Lingjie Mei,Alexander Raistrick,Xinyuan Li,Jia Deng*

Main category: cs.CV

TL;DR: PDE（Procedural Depth Evaluation）是一个新的基准测试，用于系统评估单目深度估计模型的鲁棒性，通过程序生成3D场景测试多种扰动。


<details>
  <summary>Details</summary>
Motivation: 现有标准基准测试主要评估准确性而非鲁棒性，PDE填补了这一空白。

Method: 使用程序生成3D场景，测试模型对物体、相机、材质和光照变化的鲁棒性。

Result: 揭示了当前先进深度模型在特定扰动下的挑战。

Conclusion: PDE为深度估计研究提供了新的鲁棒性评估工具，有助于未来研究。

Abstract: Recent years have witnessed substantial progress on monocular depth
estimation, particularly as measured by the success of large models on standard
benchmarks. However, performance on standard benchmarks does not offer a
complete assessment, because most evaluate accuracy but not robustness. In this
work, we introduce PDE (Procedural Depth Evaluation), a new benchmark which
enables systematic robustness evaluation. PDE uses procedural generation to
create 3D scenes that test robustness to various controlled perturbations,
including object, camera, material and lighting changes. Our analysis yields
interesting findings on what perturbations are challenging for state-of-the-art
depth models, which we hope will inform further research. Code and data are
available at https://github.com/princeton-vl/proc-depth-eval.

</details>


### [96] [UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis](https://arxiv.org/abs/2507.00992)
*Yuanrui Wang,Cong Han,YafeiLi,Zhipeng Jin,Xiawei Li,SiNan Du,Wen Tao,Yi Yang,shuanglong li,Chun Yuan,Liu Lin*

Main category: cs.CV

TL;DR: 提出了一种基于分割引导的文本生成框架，通过像素级视觉文本掩码作为统一条件输入，解决了现有方法在字体风格和颜色保留上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成方法在渲染视觉文本时存在模糊字形、语义漂移和风格控制有限的问题，且依赖预渲染字形图像导致模型复杂性和灵活性下降。

Method: 采用分割引导框架，包含双语分割模型提取精确文本掩码，以及优化的扩散模型结合自适应字形条件和区域特定损失。

Result: 在AnyText基准测试中表现最优，显著优于现有方法，并在新提出的GlyphMM和MiniText基准测试中大幅领先。

Conclusion: 该方法在文本内容和风格保真度上表现出色，特别是在小文本渲染和复杂布局保留方面，验证了其强泛化能力和部署准备性。

Abstract: Text-to-image generation has greatly advanced content creation, yet
accurately rendering visual text remains a key challenge due to blurred glyphs,
semantic drift, and limited style control. Existing methods often rely on
pre-rendered glyph images as conditions, but these struggle to retain original
font styles and color cues, necessitating complex multi-branch designs that
increase model overhead and reduce flexibility. To address these issues, we
propose a segmentation-guided framework that uses pixel-level visual text masks
-- rich in glyph shape, color, and spatial detail -- as unified conditional
inputs. Our method introduces two core components: (1) a fine-tuned bilingual
segmentation model for precise text mask extraction, and (2) a streamlined
diffusion model augmented with adaptive glyph conditioning and a
region-specific loss to preserve textual fidelity in both content and style.
Our approach achieves state-of-the-art performance on the AnyText benchmark,
significantly surpassing prior methods in both Chinese and English settings. To
enable more rigorous evaluation, we also introduce two new benchmarks:
GlyphMM-benchmark for testing layout and glyph consistency in complex
typesetting, and MiniText-benchmark for assessing generation quality in
small-scale text regions. Experimental results show that our model outperforms
existing methods by a large margin in both scenarios, particularly excelling at
small text rendering and complex layout preservation, validating its strong
generalization and deployment readiness.

</details>


### [97] [GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://arxiv.org/abs/2507.01006)
*Wenyi Hong,Wenmeng Yu,Xiaotao Gu,Guo Wang,Guobing Gan,Haomiao Tang,Jiale Cheng,Ji Qi,Junhui Ji,Lihang Pan,Shuaiqi Duan,Weihan Wang,Yan Wang,Yean Cheng,Zehai He,Zhe Su,Zhen Yang,Ziyang Pan,Aohan Zeng,Baoxu Wang,Boyan Shi,Changyu Pang,Chenhui Zhang,Da Yin,Fan Yang,Guoqing Chen,Jiazheng Xu,Jiali Chen,Jing Chen,Jinhao Chen,Jinghao Lin,Jinjiang Wang,Junjie Chen,Leqi Lei,Leyi Pan,Mingzhi Zhang,Qinkai Zheng,Sheng Yang,Shi Zhong,Shiyu Huang,Shuyuan Zhao,Siyan Xue,Shangqin Tu,Shengbiao Meng,Tianshu Zhang,Tianwei Luo,Tianxiang Hao,Tianle Gong,Wenkai Li,Wei Jia,Xin Lyu,Xuancheng Huang,Yanling Wang,Yadong Xue,Yanfeng Wang,Yifan An,Yifan Du,Yiming Shi,Yiheng Huang,Yilin Niu,Yuan Wang,Yuanchang Yue,Yuchen Li,Yutao Zhang,Yuxuan Zhang,Zhanxiao Du,Zhenyu Hou,Zhao Xue,Zhengxiao Du,Zihan Wang,Peng Zhang,Debing Liu,Bin Xu,Juanzi Li,Minlie Huang,Yuxiao Dong,Jie Tang*

Main category: cs.CV

TL;DR: GLM-4.1V-Thinking是一个先进的视觉语言模型，通过强化学习与课程采样（RLCS）提升多模态推理能力，在多个任务上表现优异，甚至超越更大规模的模型和闭源模型。


<details>
  <summary>Details</summary>
Motivation: 开发一个通用的多模态推理模型，提升在STEM问题解决、视频理解、内容识别等多样化任务上的性能。

Method: 采用大规模预训练构建视觉基础模型，并通过RLCS进一步优化模型能力。

Result: 在28个公共基准测试中表现优异，超越同类规模模型，并在部分任务上优于更大规模模型和闭源模型。

Conclusion: GLM-4.1V-Thinking展示了强大的多模态推理能力，为相关研究提供了开源资源。

Abstract: We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to
advance general-purpose multimodal reasoning. In this report, we share our key
findings in the development of the reasoning-centric training framework. We
first develop a capable vision foundation model with significant potential
through large-scale pre-training, which arguably sets the upper bound for the
final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then
unlocks the full potential of the model, leading to comprehensive capability
enhancement across a diverse range of tasks, including STEM problem solving,
video understanding, content recognition, coding, grounding, GUI-based agents,
and long document understanding, among others. To facilitate research in this
field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art
performance among models of comparable size. In a comprehensive evaluation
across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all
tasks and achieves comparable or even superior performance on 18 benchmarks
relative to the significantly larger Qwen2.5-VL-72B. Notably,
GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance
compared to closed-source models such as GPT-4o on challenging tasks including
long document understanding and STEM reasoning, further underscoring its strong
capabilities. Code, models and more information are released at
https://github.com/THUDM/GLM-4.1V-Thinking.

</details>


### [98] [ShapeEmbed: a self-supervised learning framework for 2D contour quantification](https://arxiv.org/abs/2507.01009)
*Anna Foix Romero,Craig Russell,Alexander Krull,Virginie Uhlmann*

Main category: cs.CV

TL;DR: ShapeEmbed是一种自监督表示学习框架，用于编码2D图像中物体的轮廓，生成对平移、缩放、旋转、反射和点索引不变的形状描述符，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决形状量化中确保测量对固有几何变换不变的核心挑战。

Method: 提出ShapeEmbed框架，通过自监督学习将欧几里得距离矩阵编码为形状描述符。

Result: 在自然和生物图像形状分类任务中表现优于现有方法。

Conclusion: ShapeEmbed在生物成像应用中具有潜在重要性。

Abstract: The shape of objects is an important source of visual information in a wide
range of applications. One of the core challenges of shape quantification is to
ensure that the extracted measurements remain invariant to transformations that
preserve an object's intrinsic geometry, such as changing its size,
orientation, and position in the image. In this work, we introduce ShapeEmbed,
a self-supervised representation learning framework designed to encode the
contour of objects in 2D images, represented as a Euclidean distance matrix,
into a shape descriptor that is invariant to translation, scaling, rotation,
reflection, and point indexing. Our approach overcomes the limitations of
traditional shape descriptors while improving upon existing state-of-the-art
autoencoder-based approaches. We demonstrate that the descriptors learned by
our framework outperform their competitors in shape classification tasks on
natural and biological images. We envision our approach to be of particular
relevance to biological imaging applications.

</details>


### [99] [DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution](https://arxiv.org/abs/2507.01012)
*Zhe Kong,Le Li,Yong Zhang,Feng Gao,Shaoshu Yang,Tao Wang,Kaihao Zhang,Zhuoliang Kang,Xiaoming Wei,Guanying Chen,Wenhan Luo*

Main category: cs.CV

TL;DR: DAM-VSR是一个用于视频超分辨率（VSR）的框架，通过分离外观增强和运动控制问题，结合视频扩散模型和图像超分辨率模型，解决了时间一致性和细节生成的挑战。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的视频超分辨率面临复杂且不可预测的退化问题，现有方法难以生成时间一致的帧。

Method: 提出DAM-VSR框架，将VSR分解为外观增强（通过参考图像超分辨率）和运动控制（通过视频ControlNet），并采用运动对齐的双向采样策略。

Result: DAM-VSR在真实世界数据和AIGC数据上实现了最先进的性能，展示了强大的细节生成能力。

Conclusion: DAM-VSR通过分离外观和运动问题，充分利用生成先验和细节生成能力，有效解决了VSR的挑战。

Abstract: Real-world video super-resolution (VSR) presents significant challenges due
to complex and unpredictable degradations. Although some recent methods utilize
image diffusion models for VSR and have shown improved detail generation
capabilities, they still struggle to produce temporally consistent frames. We
attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address
this issue. However, due to the intrinsic image-animation characteristics of
SVD, it is challenging to generate fine details using only low-quality videos.
To tackle this problem, we propose DAM-VSR, an appearance and motion
disentanglement framework for VSR. This framework disentangles VSR into
appearance enhancement and motion control problems. Specifically, appearance
enhancement is achieved through reference image super-resolution, while motion
control is achieved through video ControlNet. This disentanglement fully
leverages the generative prior of video diffusion models and the detail
generation capabilities of image super-resolution models. Furthermore, equipped
with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can
conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art
performance on real-world data and AIGC data, demonstrating its powerful detail
generation capabilities.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [100] [EMSpice 2.1: A Coupled EM and IR Drop Analysis Tool with Joule Heating and Thermal Map Integration for VLSI Reliability](https://arxiv.org/abs/2507.00270)
*Subed Lamichhane,Haotian Lu,Sheldon X. -D. Tan*

Main category: eess.SY

TL;DR: EMSpice 2.1是一个增强工具，用于铜基VLSI电路中的电迁移（EM）和IR压降分析，首次整合了焦耳热效应和实际热图，显著提升了分析速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着技术尺寸缩小，电迁移引起的IR压降问题日益严重，现有工具很少考虑温度分布对EM和IR压降的实际影响。

Method: 在EMSpice 2.0基础上，EMSpice 2.1整合了焦耳热效应和实际热图，并改进了与商业EDA工具的互操作性。

Result: 研究发现热点模式显著影响互连寿命和芯片可靠性，工具与COMSOL等工业标准工具一致，速度提升200倍以上。

Conclusion: EMSpice 2.1为EM和IR压降分析提供了更高效、准确的解决方案，适用于未来VLSI电路设计。

Abstract: Electromigration (EM) remains a critical reliability concern in current and
future copper-based VLSI circuits. As technology scales down, EM-induced IR
drop becomes increasingly severe. While several EM-aware IR drop analysis tools
have been proposed, few incorporate the real impact of temperature distribution
on both EM and IR drop effects. In this work, we introduce EMSpice 2.1, an
enhanced tool built upon the existing coupled IR-EM analysis framework, EMSpice
2.0, for EM-aware IR drop analysis. For the first time, EMSpice 2.1 uniquely
integrates Joule heating effects and practical thermal maps derived from actual
chip conditions. Additionally, it features improved interoperability with
commercial EDA tools, facilitating more comprehensive EM and IR drop sign-off
analysis. Our findings demonstrate that specific hotspot patterns significantly
impact the lifetime of interconnects and overall chip reliability due to EM
failures. Furthermore, our tool exhibits strong agreement with
industry-standard tools such as COMSOL, achieving a speedup of over 200 times
while maintaining high accuracy.

</details>


### [101] [Iteratively Saturated Kalman Filtering](https://arxiv.org/abs/2507.00272)
*Alan Yang,Stephen Boyd*

Main category: eess.SY

TL;DR: 提出了一种迭代饱和卡尔曼滤波器（ISKF），用于解决线性高斯系统中卡尔曼滤波器对异常值的脆弱性问题，同时保持低计算成本和实现简单性。


<details>
  <summary>Details</summary>
Motivation: 卡尔曼滤波器（KF）在测量和过程噪声中存在异常值时表现不佳，需要一种鲁棒性更强的替代方法。

Method: 通过求解凸鲁棒估计问题的缩放梯度方法，推导出ISKF，通常仅需1-2次迭代即可实现良好性能。

Result: ISKF在保持KF低计算成本和实现简单性的同时，显著提高了对异常值的鲁棒性。

Conclusion: ISKF是一种适用于实时系统的鲁棒卡尔曼滤波器变体，尤其适合需要快速迭代和稳态性能的应用。

Abstract: The Kalman filter (KF) provides optimal recursive state estimates for
linear-Gaussian systems and underpins applications in control, signal
processing, and others. However, it is vulnerable to outliers in the
measurements and process noise. We introduce the iteratively saturated Kalman
filter (ISKF), which is derived as a scaled gradient method for solving a
convex robust estimation problem. It achieves outlier robustness while
preserving the KF's low per-step cost and implementation simplicity, since in
practice it typically requires only one or two iterations to achieve good
performance. The ISKF also admits a steady-state variant that, like the
standard steady-state KF, does not require linear system solves in each time
step, making it well-suited for real-time systems.

</details>


### [102] [Augmented Physics-Based Li-ion Battery Model via Adaptive Ensemble Sparse Learning and Conformal Prediction](https://arxiv.org/abs/2507.00353)
*Samuel Filgueira da Silva,Mehmet Fatih Ozkan,Faissal El Idrissi,Marcello Canova*

Main category: eess.SY

TL;DR: 提出了一种自适应集成稀疏识别（AESI）框架，通过补偿不可预测的动态行为，提高了锂离子电池降阶模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池在实际应用中需要高精度电化学模型以确保安全和效率，但现有降阶模型难以捕捉复杂非线性行为。

Method: 结合扩展单粒子模型（ESPM）与进化集成稀疏学习策略，并采用保形预测方法量化不确定性。

Result: 混合模型（ESPM + AESI）显著提高了电压预测精度，均方误差降低达46%，保形预测提供了高覆盖率的预测区间。

Conclusion: AESI框架有效提升了模型精度和可靠性，适用于复杂工况下的锂离子电池建模。

Abstract: Accurate electrochemical models are essential for the safe and efficient
operation of lithium-ion batteries in real-world applications such as
electrified vehicles and grid storage. Reduced-order models (ROM) offer a
balance between fidelity and computational efficiency but often struggle to
capture complex and nonlinear behaviors, such as the dynamics in the cell
voltage response under high C-rate conditions. To address these limitations,
this study proposes an Adaptive Ensemble Sparse Identification (AESI) framework
that enhances the accuracy of reduced-order li-ion battery models by
compensating for unpredictable dynamics. The approach integrates an Extended
Single Particle Model (ESPM) with an evolutionary ensemble sparse learning
strategy to construct a robust hybrid model. In addition, the AESI framework
incorporates a conformal prediction method to provide theoretically guaranteed
uncertainty quantification for voltage error dynamics, thereby improving the
reliability of the model's predictions. Evaluation across diverse operating
conditions shows that the hybrid model (ESPM + AESI) improves the voltage
prediction accuracy, achieving mean squared error reductions of up to 46% on
unseen data. Prediction reliability is further supported by conformal
prediction, yielding statistically valid prediction intervals with coverage
ratios of 96.85% and 97.41% for the ensemble models based on bagging and
stability selection, respectively.

</details>


### [103] [Minimal Construction of Graphs with Maximum Robustness](https://arxiv.org/abs/2507.00415)
*Haejoon Lee,Dimitra Panagou*

Main category: eess.SY

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The notions of network $r$-robustness and $(r,s)$-robustness have been
earlier introduced in the literature to achieve resilient control in the
presence of misbehaving agents. However, while higher robustness levels provide
networks with higher tolerances against the misbehaving agents, they also
require dense communication structures, which are not always desirable for
systems with limited capabilities and energy capacities. Therefore, this paper
studies the fundamental structures behind $r$-robustness and $(r,s)$-
robustness properties in two different ways. (a) We first explore and establish
the tight necessary conditions on the number of edges for undirected graphs
with any nodes must satisfy to achieve maximum $r$- and $(r,s)$-robustness. (b)
We then use these conditions to construct two classes of undirected graphs,
referred as to $\gamma$- and $(\gamma,\gamma)$-Minimal Edge Robust Graphs
(MERGs), that provably achieve maximum robustness with minimal numbers of
edges. We finally validate our work through some sets of simulations.

</details>


### [104] [Multi-Agent Coordination under Poisson Observations: A Global Game Approach](https://arxiv.org/abs/2507.00424)
*Marcos M. Vasconcelos,Behrouz Touri*

Main category: eess.SY

TL;DR: 研究了基于不完全信息博弈的协调模型，证明了在特定条件下存在贝叶斯纳什均衡，并通过数值示例验证了潜在函数的单峰性。


<details>
  <summary>Details</summary>
Motivation: 探索在噪声信号和有限代理下战略协调的均衡存在性及其应用。

Method: 假设信号服从泊松分布，状态服从伽马先验，分析无限代理下的潜在函数。

Result: 证明了阈值策略下的均衡存在性，潜在函数为单峰。

Conclusion: 结果适用于细菌群体感应系统的建模。

Abstract: We study a model of strategic coordination based on a class of games with
incomplete information known as Global Games. Under the assumption of
Poisson-distributed signals and a Gamma prior distribution on state of the
system, we demonstrate the existence of a Bayesian Nash equilibrium within the
class of threshold policies for utility functions that are linear in the
agents' actions. Although computing the exact threshold that constitutes an
equilibrium in a system with finitely many agents is a highly non-trivial task,
the problem becomes tractable by analyzing the game's potential function with
countably infinitely many agents. Through numerical examples, we provide
evidence that the resulting potential function is unimodal, exhibiting a
well-defined maximum. Our results are applicable to the modeling of bacterial
Quorum Sensing systems, whose noisy observation signals are often
well-approximated using Poisson processes.

</details>


### [105] [The impact of the following vehicles behaviors on the car following behaviors of the ego-vehicle](https://arxiv.org/abs/2507.00452)
*Yang Liu,Jiahao Zhang,Yuxuan Ouyang,Huan Yu,Dengbo He*

Main category: eess.SY

TL;DR: 论文研究了后车状态对前车跟驰行为的影响，发现后车压力会导致前车驾驶员调整行为，保持更近距离但更谨慎驾驶。


<details>
  <summary>Details</summary>
Motivation: 传统跟驰行为模型通常忽略周围道路使用者（包括后车）的同伴压力，研究旨在填补这一空白。

Method: 利用highD数据集提取两类跟驰事件（紧跟前车和保持距离），使用动态时间规整和逆强化学习分析驾驶员行为。

Result: 结果显示，后车压力下前车驾驶员会调整跟驰行为，保持更近距离但更谨慎，同时仍能根据交通流速度和距离调整策略。

Conclusion: 研究为更精确的交通流建模提供了新视角，强调了考虑周围道路使用者压力的重要性。

Abstract: Among all types of crashes, rear-end crashes dominate, which are closely
related to the car-following (CF) behaviors. Traditional CF behavior models
focused on the influence of the vehicle in front, but usually ignored the peer
pressure from the surrounding road users, including the following vehicle (FV).
Based on an open dataset, the highD dataset, we investigated whether the FV's
states can affect the CF behavior of the ego-vehicle in CF events. Two types of
CF events were extracted from highD database, including the tailgated events,
where the time headway between the FV and the ego-vehicle (i.e., time gap) was
smaller than 1 second, and the gapped events, where the time gap was larger
than 3 seconds. The dynamic time warping was used to extract CF pairs with
similar speed profiles of the leading vehicle (LV). Statistical analyses were
conducted to compare the CF-performance metrics in tailgated and gapped events.
Then, the inverse reinforcement learning was used to recover the reward
function of the ego-vehicle drivers in different CF events. The results showed
that the ego-driver would adjust their CF behavior in response to the pressure
from a tailgating FV, by maintaining a closer distance to the LV, but at the
same time, driving more cautiously. Further, drivers were still able to adjust
their CF strategies based on the speed of traffic flow and the distance to the
LV, even when being tailgated. These findings provide insights regarding more
accurate modelling of traffic flow by considering the peer pressure from
surrounding road users.

</details>


### [106] [Price Aware Power Split Control in Heterogeneous Battery Storage Systems](https://arxiv.org/abs/2507.00628)
*Sheng Yin,Vivek Teja Tanjavooru,Thomas Hamacher,Christoph Goebel,Holger Hesse*

Main category: eess.SY

TL;DR: 本文提出了一种统一框架，用于优化电池储能系统（BESS）的调度和内部功率分配，结合市场信号和物理约束，比较了线性规划（LP）和强化学习（RL）方法。


<details>
  <summary>Details</summary>
Motivation: 优化BESS的经济价值和性能，同时管理外部能源调度和内部异质性。

Method: 使用基于Gym的仿真环境，比较LP和RL方法在不同预测假设下的表现。

Result: RL在系统效率上优于LP（高10%），而LP在累积节省上更优（高33%）。LP在SOC平衡上表现更好，而RL在温度平衡上更优。

Conclusion: LP适用于稳定条件，RL在动态环境中更具适应性，适合实时BESS控制。

Abstract: This paper presents a unified framework for the optimal scheduling of battery
dispatch and internal power allocation in Battery energy storage systems
(BESS). This novel approach integrates both market-based (price-aware) signals
and physical system constraints to simultaneously optimize (1) external energy
dispatch and (2) internal heterogeneity management of BESS, enhancing its
operational economic value and performance. This work compares both model-based
Linear Programming (LP) and model-free Reinforcement Learning (RL) approaches
for optimization under varying forecast assumptions, using a custom Gym-based
simulation environment. The evaluation considers both long-term and short-term
performance, focusing on economic savings, State of Charge (SOC) and
temperature balancing, and overall system efficiency. In summary, the long-term
results show that the RL approach achieved 10% higher system efficiency
compared to LP, whereas the latter yielded 33% greater cumulative savings. In
terms of internal heterogeneity, the LP approach resulted in lower mean SOC
imbalance, while the RL approach achieved better temperature balance between
strings. This behavior is further examined in the short-term evaluation, which
indicates that LP delivers strong optimization under known and stable
conditions, whereas RL demonstrates higher adaptability in dynamic
environments, offering potential advantages for real-time BESS control.

</details>


### [107] [Getting Dynamic Line Ratings into Markets](https://arxiv.org/abs/2507.00826)
*Zhiyi Zhou,Christoph Graf,Yury Dvorkin*

Main category: eess.SY

TL;DR: 论文提出了一种动态线路评级（DLR）方法，通过实时调整输电线路容量以提高利用率，解决了静态评级的保守性问题。


<details>
  <summary>Details</summary>
Motivation: 静态输电线路评级因保守假设导致容量利用不足，动态线路评级（DLR）是一种经济可行的替代方案，但缺乏有效的操作工具限制了其应用。

Method: 将DLR建模为具有时间依赖性的类库存资源，通过近似线路温度演化过程解耦天气和功率流对容量的影响，并将其整合到多周期DC最优潮流问题中。

Result: 通过线性化将非凸问题转化为可处理的凸形式，并在11区和1814节点的NYISO系统中验证了其性能。

Conclusion: DLR方法能有效提高线路利用率，并对调度、定价和边际碳排放产生积极影响。

Abstract: Static transmission line ratings may lead to underutilization of line
capacity due to overly conservative (worst-case) assumptions. Grid-enhancing
technologies (GETs) such as dynamic line ratings (DLRs), which adjust line
capacity based on real-time conditions, are a techno-economically viable
alternative to increase the utilization of existing power lines. Nonetheless,
their adoption has been slow, partly due to the absence of operational tools
that effectively account for simultaneous impacts on dispatch and pricing. In
this paper, we represent transmission capacity with DLRs as a stock-like
resource with time-variant interdependency, which is modeled via an
approximation of line temperature evolution process, decoupling the impacts of
ambient weather conditions and power flow on transmission line temperature and
thus capacity. We integrate DLRs into a multi-period DC optimal power flow
problem, with chance constrains addressing correlated uncertainty in DLRs and
renewable generation. This yields non-convex problems that we transform into a
tractable convex form by linearization. We derive locational marginal energy
and ancillary services prices consistent with a competitive equilibrium.
Numerical experiments on the 11-zone and 1814-node NYISO systems demonstrate
its performance, including impacts on dispatch, pricing, and marginal carbon
emissions.

</details>


### [108] [Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite](https://arxiv.org/abs/2507.00877)
*William H English,Chase Walker,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: eess.SY

TL;DR: 论文提出了VLTL-Bench基准，用于评估自然语言到线性时序逻辑（NL-to-LTL）翻译系统的验证能力，弥补现有基准仅关注翻译准确性的不足。


<details>
  <summary>Details</summary>
Motivation: 现有NL-to-TL翻译系统在基准测试中表现优异，但忽略了原子命题在新场景中的落地能力，导致性能指标虚高，缺乏领域通用性。

Method: 引入VLTL-Bench，包含多样化的自然语言规范、形式化时序逻辑规范及验证样本，支持端到端评估和分步改进。

Result: VLTL-Bench提供了三个独特状态空间和验证样本，支持翻译、落地、验证等子步骤的评估。

Conclusion: VLTL-Bench为NL-to-LTL翻译系统的验证能力提供了统一基准，促进方法学上的进步。

Abstract: Empirical evaluation of state-of-the-art natural-language (NL) to
temporal-logic (TL) translation systems reveals near-perfect performance on
existing benchmarks. However, current studies measure only the accuracy of the
translation of NL logic into formal TL, ignoring a system's capacity to ground
atomic propositions into new scenarios or environments. This is a critical
feature, necessary for the verification of resulting formulas in a concrete
state space. Consequently, most NL-to-TL translation frameworks propose their
own bespoke dataset in which the correct grounding is known a-priori, inflating
performance metrics and neglecting the need for extensible, domain-general
systems. In this paper, we introduce the Verifiable Linear Temporal Logic
Benchmark ( VLTL-Bench), a unifying benchmark that measures verification and
verifiability of automated NL-to-LTL translation. The dataset consists of three
unique state spaces and thousands of diverse natural language specifications
and corresponding formal specifications in temporal logic. Moreover, the
benchmark contains sample traces to validate the temporal logic expressions.
While the benchmark directly supports end-to-end evaluation, we observe that
many frameworks decompose the process into i) lifting, ii) grounding, iii)
translation, and iv) verification. The benchmark provides ground truths after
each of these steps to enable researches to improve and evaluate different
substeps of the overall problem. To encourage methodologically sound advances
in verifiable NL-to-LTL translation approaches, we release VLTL-Bench here:
https://www.kaggle.com/datasets/dubascudes/vltl bench.

</details>


### [109] [Constellation as a Service: Tailored Connectivity Management in Direct-Satellite-to-Device Networks](https://arxiv.org/abs/2507.00902)
*Feng Wang,Shengyu Zhang,Een-Kee Hong,Tony Q. S. Quek*

Main category: eess.SY

TL;DR: 论文提出了一种名为CaaS的框架，用于优化多星座环境下的DS2D通信，通过动态子星座和AI技术提升服务性能。


<details>
  <summary>Details</summary>
Motivation: 解决多星座DS2D通信中的高干扰和频繁切换问题，充分利用多星座资源。

Method: 提出CaaS框架，动态生成子星座，结合GenAI预测波束成形和预配置切换路径。

Result: 仿真显示CaaS显著提升服务率并减少切换开销。

Conclusion: CaaS是管理多星座DS2D连接的高效可持续解决方案。

Abstract: Direct-satellite-to-device (DS2D) communication is emerging as a promising
solution for global mobile service extension, leveraging the deployment of
satellite constellations. However, the challenge of managing DS2D connectivity
for multi-constellations becomes outstanding, including high interference and
frequent handovers caused by multi-coverage overlap and rapid satellite
movement. Moreover, existing approaches primarily operate within
single-constellation shell, which inherently limits the ability to exploit the
vast potential of multi-constellation connectivity provision, resulting in
suboptimal DS2D service performances. To address these challenges, this article
proposes a Constellation as a Service (CaaS) framework, which treats the entire
multi-constellation infrastructure as a shared resource pool and dynamically
forms optimal sub-constellations (SCs) for each DS2D service region. The
formation of each SC integrates satellites from various orbits to provide
tailored connectivity based on user demands, guided by two innovative
strategies: predictive satellite beamforming using generative artificial
intelligence (GenAI) and pre-configured handover path for efficient satellite
access and mobility management. Simulation results demonstrate that CaaS
significantly improves satellite service rates while reducing handover
overhead, making it an efficient and continuable solution for managing DS2D
connectivity in multi-constellation environments.

</details>


### [110] [Geometrization of Higher-Order Linear Control Laws for Attitude Control on $\mathsf{SO(3)}$](https://arxiv.org/abs/2507.00997)
*Farooq Aslam,Hafiz Zeeshan Iqbal Khan,Muhammad Farooq Haydar,Suhail Akhtar,Jamshed Riaz*

Main category: eess.SY

TL;DR: 本文提出了一个理论框架，用于分析高阶几何非线性控制律在SO(3)群上的稳定性，扩展了PID型控制律的分析结果，并提出了基于LMI的稳定性条件。


<details>
  <summary>Details</summary>
Motivation: 研究动机是扩展现有PID型几何非线性控制律的分析方法，以适用于更一般的高阶动态状态反馈补偿器，并利用LMI工具减少保守性。

Method: 方法包括两步：首先确定Lyapunov函数的正定性和Lyapunov速率的负定性条件，确保AGAS稳定性；其次通过凸松弛得到LMI形式的充分条件。

Result: 结果表明，该方法能够设计并分析一个21状态的几何非线性姿态控制律，适用于多旋翼飞行器。

Conclusion: 结论表明，提出的框架有效扩展了高阶控制律的稳定性分析，并通过LMI工具减少了保守性，适用于实际应用。

Abstract: This paper presents a theoretical framework for analyzing the stability of
higher-order geometric nonlinear control laws for attitude control on the
Special Orthogonal Group $\mathrm{SO(3)}$. In particular, the paper extends
existing results on the analysis of PID-type geometric nonlinear control laws
to more general higher-order dynamic state-feedback compensators on
$\mathrm{SO(3)}$. The candidate Lyapunov function is motivated by quadratic
Lyapunov functions of the form $V(x)=x^{\top}Px$ typically considered in the
analysis of linear time-invariant (LTI) systems. The stability analysis is
carried out in two steps. In the first step, a sufficient condition is obtained
for the positive definiteness of the candidate Lyapunov function, and a
necessary and sufficient condition for the negative definiteness of the
corresponding Lyapunov rate. These conditions ensure that the desired
equilibrium is almost globally asymptotically stable (AGAS). In the second
step, a convex relaxation of the proposed conditions is used to obtain
sufficient conditions in the form of linear matrix inequalities (LMIs).
Overall, the approach is motivated by the widespread use of LMI-based analysis
and design tools for LTI systems. To reduce conservatism, matrix gains are
considered for the controller gains as well as the Lyapunov function
coefficients. The applicability of the approach to practical problems is
illustrated by designing and analyzing a 21-state geometric nonlinear attitude
control law for a multicopter.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [111] [MVGBench: Comprehensive Benchmark for Multi-view Generation Models](https://arxiv.org/abs/2507.00006)
*Xianghui Xie,Chuhang Zou,Meher Gitika Karumuri,Jan Eric Lenssen,Gerard Pons-Moll*

Main category: cs.GR

TL;DR: MVGBench是一个用于评估多视图图像生成模型（MVGs）的基准测试，重点关注3D几何和纹理一致性、图像质量和语义。它通过引入3D自一致性指标和系统比较12种现有MVGs，揭示了现有方法的局限性，并提出了ViFiGen方法，显著提升了3D一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的MVGs评估方法通常将生成图像与真实目标视图比较，这在生成任务中不适用，因为存在多种可能的解决方案。此外，不同MVGs的训练数据（如视角、合成数据和光照）差异较大，缺乏对这些因素的鲁棒性和对真实数据的泛化能力的全面评估。

Method: MVGBench通过评估最佳配置性能、泛化到真实数据的能力和鲁棒性三个方面来解决问题。它引入了一种新颖的3D自一致性指标，通过比较从不相交生成多视图中重建的3D模型来评估一致性。

Result: 在4个真实和合成数据集上系统比较了12种MVGs，发现现有方法在鲁棒性和泛化能力方面存在显著局限性。基于分析，提出的ViFiGen方法在3D一致性上优于所有评估的MVGs。

Conclusion: MVGBench为MVGs的评估提供了更全面的框架，揭示了关键设计选择的重要性，并通过ViFiGen展示了改进方向。代码、模型和基准测试套件将公开。

Abstract: We propose MVGBench, a comprehensive benchmark for multi-view image
generation models (MVGs) that evaluates 3D consistency in geometry and texture,
image quality, and semantics (using vision language models). Recently, MVGs
have been the main driving force in 3D object creation. However, existing
metrics compare generated images against ground truth target views, which is
not suitable for generative tasks where multiple solutions exist while
differing from ground truth. Furthermore, different MVGs are trained on
different view angles, synthetic data and specific lightings -- robustness to
these factors and generalization to real data are rarely evaluated thoroughly.
Without a rigorous evaluation protocol, it is also unclear what design choices
contribute to the progress of MVGs. MVGBench evaluates three different aspects:
best setup performance, generalization to real data and robustness. Instead of
comparing against ground truth, we introduce a novel 3D self-consistency metric
which compares 3D reconstructions from disjoint generated multi-views. We
systematically compare 12 existing MVGs on 4 different curated real and
synthetic datasets. With our analysis, we identify important limitations of
existing methods specially in terms of robustness and generalization, and we
find the most critical design choices. Using the discovered best practices, we
propose ViFiGen, a method that outperforms all evaluated MVGs on 3D
consistency. Our code, model, and benchmark suite will be publicly released.

</details>


### [112] [ViscoReg: Neural Signed Distance Functions via Viscosity Solutions](https://arxiv.org/abs/2507.00412)
*Meenakshi Krishnan,Ramani Duraiswami*

Main category: cs.GR

TL;DR: 论文提出了一种名为ViscoReg的新损失函数，用于稳定神经SDF训练，并在理论和实验上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 尽管Eikonal方程在神经SDF训练中存在不稳定性，但缺乏有效的正则化方法。

Method: 利用粘性理论提出ViscoReg损失函数，增强训练稳定性。

Result: ViscoReg在实验中优于SIREN、DiGS和StEik等方法，且计算成本低。

Conclusion: ViscoReg为神经SDF训练提供了一种高效且稳定的解决方案。

Abstract: Implicit Neural Representations (INRs) that learn a Signed Distance Function
(SDF) are a powerful tool for continuous 3D scene reconstruction. These models
are trained by enforcing the Eikonal equation. We demonstrate theoretically
that despite the ill-posedness of the Eikonal equation, generalization error
estimates may be obtained for Neural SDFs in terms of the training error.
However, training with the Eikonal loss can lead to unstable gradient flows,
necessitating alternate stabilization techniques. Traditional numerical solvers
for the equation have relied on viscosity approaches for regularization. We
enhance Neural SDF training using this well-developed theory, and introduce a
new loss formulation we call ViscoReg. We theoretically demonstrate the
stability of the gradient flow equation of our proposed loss term. Empirically,
ViscoReg outperforms state-of-the-art approaches such as SIREN, DiGS, and StEik
without adding significant computational cost.

</details>


### [113] [FreNBRDF: A Frequency-Rectified Neural Material Representation](https://arxiv.org/abs/2507.00476)
*Chenliang Zhou,Zheyuan Hu,Cengiz Oztireli*

Main category: cs.GR

TL;DR: FreNBRDF是一种基于频率校正的神经材料表示方法，通过球谐函数将频域考虑融入神经BRDF建模，提升了材料外观重建和编辑的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖表格化的BRDF数据，而神经隐式表示虽灵活但频域行为理解不足，因此需要一种频域校正的神经材料表示方法。

Method: 利用球谐函数提出频率校正损失，并将其融入可泛化和自适应的重建与编辑流程中。

Result: 实验表明，FreNBRDF在材料外观重建和编辑的准确性与鲁棒性上优于现有基线方法。

Conclusion: FreNBRDF通过频域校正提升了神经材料建模的保真度、适应性和效率，为下游任务提供了更结构化和可解释的框架。

Abstract: Accurate material modeling is crucial for achieving photorealistic rendering,
bridging the gap between computer-generated imagery and real-world photographs.
While traditional approaches rely on tabulated BRDF data, recent work has
shifted towards implicit neural representations, which offer compact and
flexible frameworks for a range of tasks. However, their behavior in the
frequency domain remains poorly understood. To address this, we introduce
FreNBRDF, a frequency-rectified neural material representation. By leveraging
spherical harmonics, we integrate frequency-domain considerations into neural
BRDF modeling. We propose a novel frequency-rectified loss, derived from a
frequency analysis of neural materials, and incorporate it into a generalizable
and adaptive reconstruction and editing pipeline. This framework enhances
fidelity, adaptability, and efficiency. Extensive experiments demonstrate that
\ours improves the accuracy and robustness of material appearance
reconstruction and editing compared to state-of-the-art baselines, enabling
more structured and interpretable downstream tasks and applications.

</details>


### [114] [Analyzing Time-Varying Scalar Fields using Piecewise-Linear Morse-Cerf Theory](https://arxiv.org/abs/2507.00725)
*Amritendu Dhar,Apratim Chakraborty,Vijay Natarajan*

Main category: cs.GR

TL;DR: 本文提出了一种将Morse-Cerf理论应用于分段线性（PL）函数的方法，引入了顶点图和Cerf图来表示PL函数临界点的演化，并定义了时间变化标量场的拓扑描述符。


<details>
  <summary>Details</summary>
Motivation: 将Morse-Cerf理论扩展到PL函数，以研究时间变化标量场的拓扑特征。

Method: 引入顶点图和Cerf图，基于顶点下链同调定义拓扑描述符，并开发算法计算Cerf图及其比较方法。

Result: 实验结果表明该方法适用于时间变化标量场的分析。

Conclusion: 该方法为PL函数的时间变化拓扑分析提供了有效工具。

Abstract: Morse-Cerf theory considers a one-parameter family of smooth functions
defined on a manifold and studies the evolution of their critical points with
the parameter. This paper presents an adaptation of Morse-Cerf theory to a
family of piecewise-linear (PL) functions. The vertex diagram and Cerf diagram
are introduced as representations of the evolution of critical points of the PL
function. The characterization of a crossing in the vertex diagram based on the
homology of the lower links of vertices leads to the definition of a
topological descriptor for time-varying scalar fields. An algorithm for
computing the Cerf diagram and a measure for comparing two Cerf diagrams are
also described together with experimental results on time-varying scalar
fields.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [115] [A High-Fidelity Speech Super Resolution Network using a Complex Global Attention Module with Spectro-Temporal Loss](https://arxiv.org/abs/2507.00229)
*Tarikul Islam Tamiti,Biraj Joshi,Rida Hasan,Rashedul Hasan,Taieba Athay,Nursad Mamun,Anomadarshi Barua*

Main category: cs.SD

TL;DR: CTFT-Net是一种用于语音超分辨率的网络，通过同时重建幅度和相位，结合全局注意力和复杂结构，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音超分辨率方法主要关注幅度重建，而相位重建对感知质量至关重要，因此提出CTFT-Net以改进任务。

Method: CTFT-Net结合复杂全局注意力块和复杂Conformer，建模音素间和频率间依赖关系，同时使用时域和多分辨率频域损失函数。

Result: 在VCTK数据集上，CTFT-Net优于NU-Wave等先进模型，尤其在极端上采样（2 kHz至48 kHz）中表现突出。

Conclusion: CTFT-Net通过复杂域重建和噪声鲁棒性设计，有效提升了语音超分辨率的性能。

Abstract: Speech super-resolution (SSR) enhances low-resolution speech by increasing
the sampling rate. While most SSR methods focus on magnitude reconstruction,
recent research highlights the importance of phase reconstruction for improved
perceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency
Transformation Network that reconstructs both magnitude and phase in complex
domains for improved SSR tasks. It incorporates a complex global attention
block to model inter-phoneme and inter-frequency dependencies and a complex
conformer to capture long-range and local features, improving frequency
reconstruction and noise robustness. CTFT-Net employs time-domain and
multi-resolution frequency-domain loss functions for better generalization.
Experiments show CTFT-Net outperforms state-of-the-art models (NU-Wave,
WSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling
(2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy
artifacts.

</details>


### [116] [Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture](https://arxiv.org/abs/2507.00466)
*Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: 本文提出了一种基于Transformer的端到端模型，用于MIDI表演中的节拍和强拍跟踪，通过新颖的数据预处理技术和实验验证，其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注基于音频的节拍跟踪，而MIDI表演中的节拍跟踪是一个重要但具有挑战性的任务，尤其在音乐转录和节奏分析中。

Method: 采用编码器-解码器架构的Transformer模型，结合动态增强和优化的标记化策略进行数据预处理。

Result: 在多个数据集上的实验表明，该模型优于隐马尔可夫模型和深度学习方法，F1分数表现优异。

Conclusion: Transformer架构在符号节拍跟踪中具有潜力，未来可结合自动音乐转录系统以增强音乐分析和乐谱生成。

Abstract: Beat tracking in musical performance MIDI is a challenging and important task
for notation-level music transcription and rhythmical analysis, yet existing
methods primarily focus on audio-based approaches. This paper proposes an
end-to-end transformer-based model for beat and downbeat tracking in
performance MIDI, leveraging an encoder-decoder architecture for
sequence-to-sequence translation of MIDI input to beat annotations. Our
approach introduces novel data preprocessing techniques, including dynamic
augmentation and optimized tokenization strategies, to improve accuracy and
generalizability across different datasets. We conduct extensive experiments
using the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model
against state-of-the-art hidden Markov models (HMMs) and deep learning-based
beat tracking methods. The results demonstrate that our model outperforms
existing symbolic music beat tracking approaches, achieving competitive
F1-scores across various musical styles and instruments. Our findings highlight
the potential of transformer architectures for symbolic beat tracking and
suggest future integration with automatic music transcription systems for
enhanced music analysis and score generation.

</details>


### [117] [AudioBERTScore: Objective Evaluation of Environmental Sound Synthesis Based on Similarity of Audio embedding Sequences](https://arxiv.org/abs/2507.00475)
*Minoru Kishi,Ryosuke Sakai,Shinnosuke Takamichi,Yusuke Kanamori,Yuki Okamoto*

Main category: cs.SD

TL;DR: 提出了一种新的TTA合成音频客观评价指标AudioBERTScore，通过嵌入相似性计算提高与主观评价的相关性。


<details>
  <summary>Details</summary>
Motivation: TTA中主观评价成本高，现有客观指标与主观评价相关性弱，需改进。

Method: 基于BERTScore，结合max-norm和p-norm计算合成与参考音频嵌入的相似性。

Result: 实验表明，新指标与主观评价的相关性优于传统方法。

Conclusion: AudioBERTScore为TTA提供更可靠的客观评价工具。

Abstract: We propose a novel objective evaluation metric for synthesized audio in
text-to-audio (TTA), aiming to improve the performance of TTA models. In TTA,
subjective evaluation of the synthesized sound is an important, but its
implementation requires monetary costs. Therefore, objective evaluation such as
mel-cepstral distortion are used, but the correlation between these objective
metrics and subjective evaluation values is weak. Our proposed objective
evaluation metric, AudioBERTScore, calculates the similarity between embedding
of the synthesized and reference sounds. The method is based not only on the
max-norm used in conventional BERTScore but also on the $p$-norm to reflect the
non-local nature of environmental sounds. Experimental results show that scores
obtained by the proposed method have a higher correlation with subjective
evaluation values than conventional metrics.

</details>


### [118] [MuteSwap: Silent Face-based Voice Conversion](https://arxiv.org/abs/2507.00498)
*Yifan Liu,Yu Fang,Zhouhan Lin*

Main category: cs.SD

TL;DR: 论文提出了一种基于视觉输入的无声人脸语音转换（SFVC）任务，并介绍了MuteSwap框架，通过对比学习和最小化互信息实现跨模态对齐，实验表明其在噪声环境下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统语音转换依赖音频输入，但在无声视频或噪声环境下不可行。SFVC任务旨在仅通过视觉输入实现语音转换，解决这一问题。

Method: 提出MuteSwap框架，利用对比学习对齐跨模态身份，最小化互信息分离共享视觉特征。

Result: MuteSwap在语音合成和身份转换中表现优异，尤其在噪声环境下优于依赖音频输入的方法。

Conclusion: MuteSwap证明了SFVC的可行性，其训练方法有效，适用于无音频输入的场景。

Abstract: Conventional voice conversion modifies voice characteristics from a source
speaker to a target speaker, relying on audio input from both sides. However,
this process becomes infeasible when clean audio is unavailable, such as in
silent videos or noisy environments. In this work, we focus on the task of
Silent Face-based Voice Conversion (SFVC), which does voice conversion entirely
from visual inputs. i.e., given images of a target speaker and a silent video
of a source speaker containing lip motion, SFVC generates speech aligning the
identity of the target speaker while preserving the speech content in the
source silent video. As this task requires generating intelligible speech and
converting identity using only visual cues, it is particularly challenging. To
address this, we introduce MuteSwap, a novel framework that employs contrastive
learning to align cross-modality identities and minimize mutual information to
separate shared visual features. Experimental results show that MuteSwap
achieves impressive performance in both speech synthesis and identity
conversion, especially under noisy conditions where methods dependent on audio
input fail to produce intelligible results, demonstrating both the
effectiveness of our training approach and the feasibility of SFVC.

</details>


### [119] [Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection](https://arxiv.org/abs/2507.00693)
*Yifan Gao,Jiao Fu,Long Guo,Hong Liu*

Main category: cs.SD

TL;DR: 论文提出了一种基于大型语言模型（LLM）的方法，结合声学和语义特征，用于通过语音识别青少年自杀风险，准确率达74%，在SW1挑战赛中排名第一。


<details>
  <summary>Details</summary>
Motivation: 早期识别自杀风险对预防自杀行为至关重要，语音作为一种非侵入性且易于获取的指标，具有研究价值。

Method: 利用大型语言模型（LLM）进行特征提取，并结合传统的声学和语义特征。

Result: 在测试集上达到74%的准确率，在SW1挑战赛中排名第一。

Conclusion: LLM方法在语音分析中具有潜力，可用于自杀风险评估。

Abstract: Early identification of suicide risk is crucial for preventing suicidal
behaviors. As a result, the identification and study of patterns and markers
related to suicide risk have become a key focus of current research. In this
paper, we present the results of our work in the 1st SpeechWellness Challenge
(SW1), which aims to explore speech as a non-invasive and easily accessible
mental health indicator for identifying adolescents at risk of suicide.Our
approach leverages large language model (LLM) as the primary tool for feature
extraction, alongside conventional acoustic and semantic features. The proposed
method achieves an accuracy of 74\% on the test set, ranking first in the SW1
challenge. These findings demonstrate the potential of LLM-based methods for
analyzing speech in the context of suicide risk assessment.

</details>


### [120] [Multi-interaction TTS toward professional recording reproduction](https://arxiv.org/abs/2507.00808)
*Hiroki Kanagawa,Kenichi Fujita,Aya Watanabe,Yusuke Ijima*

Main category: cs.SD

TL;DR: 提出了一种支持多步交互的TTS方法，允许用户直观快速地细化合成语音，模拟配音演员与导演的互动关系。


<details>
  <summary>Details</summary>
Motivation: 现有TTS系统缺乏迭代反馈机制，无法实现语音风格的精细调整，导致合成语音与用户预期风格不符。

Method: 提出了一种多步交互的TTS方法，通过模拟配音演员与导演的互动关系，支持用户对合成语音进行迭代风格调整。

Result: 实验表明，该方法能够根据用户指令实现迭代风格调整，展示了其多步交互能力。

Conclusion: 该方法为TTS系统引入了迭代反馈机制，提升了语音合成的灵活性和用户控制能力。

Abstract: Voice directors often iteratively refine voice actors' performances by
providing feedback to achieve the desired outcome. While this iterative
feedback-based refinement process is important in actual recordings, it has
been overlooked in text-to-speech synthesis (TTS). As a result, fine-grained
style refinement after the initial synthesis is not possible, even though the
synthesized speech often deviates from the user's intended style. To address
this issue, we propose a TTS method with multi-step interaction that allows
users to intuitively and rapidly refine synthetized speech. Our approach models
the interaction between the TTS model and its user to emulate the relationship
between voice actors and voice directors. Experiments show that the proposed
model with its corresponding dataset enable iterative style refinements in
accordance with users' directions, thus demonstrating its multi-interaction
capability. Sample audios are available: https://ntt-hilab-gensp.
github.io/ssw13multiinteraction_tts/

</details>


### [121] [MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement](https://arxiv.org/abs/2507.00966)
*Nikolai Lund Kühne,Jesper Jensen,Jan Østergaard,Zheng-Hua Tan*

Main category: cs.SD

TL;DR: 提出了一种结合Mamba和共享时频多头注意力模块的新架构MambAttention，用于单通道语音增强，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决序列模型（如LSTM和Mamba）在语音增强任务中容易过拟合的问题，并探索混合Mamba与时频注意力模型的潜力。

Method: 提出MambAttention架构，结合Mamba和共享时频多头注意力模块，并使用扩展数据集VB-DemandEx进行训练。

Result: 在VB-DemandEx上训练的MambAttention模型在多个指标上显著优于现有方法，尤其是在域外数据集上表现优异。

Conclusion: MambAttention在语音增强任务中表现出色，共享时频多头注意力模块对泛化性能至关重要。

Abstract: With the advent of new sequence models like Mamba and xLSTM, several studies
have shown that these models match or outperform state-of-the-art models in
single-channel speech enhancement, automatic speech recognition, and
self-supervised audio representation learning. However, prior research has
demonstrated that sequence models like LSTM and Mamba tend to overfit to the
training set. To address this issue, previous works have shown that adding
self-attention to LSTMs substantially improves generalization performance for
single-channel speech enhancement. Nevertheless, neither the concept of hybrid
Mamba and time-frequency attention models nor their generalization performance
have been explored for speech enhancement. In this paper, we propose a novel
hybrid architecture, MambAttention, which combines Mamba and shared time- and
frequency-multi-head attention modules for generalizable single-channel speech
enhancement. To train our model, we introduce VoiceBank+Demand Extended
(VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging
noise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our
proposed MambAttention model significantly outperforms existing
state-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar
complexity across all reported metrics on two out-of-domain datasets: DNS 2020
and EARS-WHAM_v2, while matching their performance on the in-domain dataset
VB-DemandEx. Ablation studies highlight the role of weight sharing between the
time- and frequency-multi-head attention modules for generalization
performance. Finally, we explore integrating the shared time- and
frequency-multi-head attention modules with LSTM and xLSTM, which yields a
notable performance improvement on the out-of-domain datasets. However, our
MambAttention model remains superior on both out-of-domain datasets across all
reported evaluation metrics.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [122] [State and Memory is All You Need for Robust and Reliable AI Agents](https://arxiv.org/abs/2507.00081)
*Matthew Muhoberac,Atharva Parikh,Nirvi Vakharia,Saniya Virani,Aco Radujevic,Savannah Wood,Meghav Verma,Dimitri Metaxotos,Jeyaraman Soundararajan,Thierry Masquelin,Alexander G. Godfrey,Sean Gardner,Dobrila Rudnicki,Sam Michael,Gaurav Chopra*

Main category: cs.MA

TL;DR: SciBORG是一个模块化框架，利用LLM和有限状态自动机（FSA）实现自主规划和任务执行，适用于复杂科学工作流。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂科学工作流中内存、规划和工具集成方面的局限性。

Method: 动态构建基于源代码文档的代理，结合FSA内存实现状态跟踪和上下文感知决策。

Result: 在物理和虚拟硬件中验证了可靠执行、自适应规划和可解释状态转换。

Conclusion: 内存和状态感知是代理规划和可靠性的关键，为复杂环境中的AI代理部署提供了通用基础。

Abstract: Large language models (LLMs) have enabled powerful advances in natural
language understanding and generation. Yet their application to complex,
real-world scientific workflows remain limited by challenges in memory,
planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke
Artificial Intelligence Agents Optimized for Research Goals), a modular agentic
framework that allows LLM-based agents to autonomously plan, reason, and
achieve robust and reliable domain-specific task execution. Agents are
constructed dynamically from source code documentation and augmented with
finite-state automata (FSA) memory, enabling persistent state tracking and
context-aware decision-making. This approach eliminates the need for manual
prompt engineering and allows for robust, scalable deployment across diverse
applications via maintaining context across extended workflows and to recover
from tool or execution failures. We validate SciBORG through integration with
both physical and virtual hardware, such as microwave synthesizers for
executing user-specified reactions, with context-aware decision making and
demonstrate its use in autonomous multi-step bioassay retrieval from the
PubChem database utilizing multi-step planning, reasoning, agent-to-agent
communication and coordination for execution of exploratory tasks. Systematic
benchmarking shows that SciBORG agents achieve reliable execution, adaptive
planning, and interpretable state transitions. Our results show that memory and
state awareness are critical enablers of agentic planning and reliability,
offering a generalizable foundation for deploying AI agents in complex
environments.

</details>


### [123] [Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms](https://arxiv.org/abs/2507.00491)
*Zain Taufique,Aman Vyas,Antonio Miele,Pasi Liljeberg,Anil Kanduri*

Main category: cs.MA

TL;DR: Twill是一个运行时框架，用于在异构移动边缘平台上调度复合AI（cAI）系统的并发推理任务，通过任务亲和性感知的集群映射和迁移、优先级感知的任务冻结/解冻以及DVFS，显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有移动边缘AI推理策略无法处理cAI系统中DNN和Transformer的并发推理需求，亟需一种新的调度方法。

Method: 提出Twill框架，结合任务亲和性感知的集群映射和迁移、优先级感知的任务冻结/解冻以及DVFS技术。

Result: 在Nvidia Jetson Orin NX平台上实现Twill，相比现有技术，平均降低54%的推理延迟，同时满足功耗预算。

Conclusion: Twill有效解决了cAI系统在移动边缘平台上的调度挑战，显著提升了性能。

Abstract: Compound AI (cAI) systems chain multiple AI models to solve complex problems.
cAI systems are typically composed of deep neural networks (DNNs),
transformers, and large language models (LLMs), exhibiting a high degree of
computational diversity and dynamic workload variation. Deploying cAI services
on mobile edge platforms poses a significant challenge in scheduling concurrent
DNN-transformer inference tasks, which arrive dynamically in an unknown
sequence. Existing mobile edge AI inference strategies manage multi-DNN or
transformer-only workloads, relying on design-time profiling, and cannot handle
concurrent inference of DNNs and transformers required by cAI systems. In this
work, we address the challenge of scheduling cAI systems on heterogeneous
mobile edge platforms. We present Twill, a run-time framework to handle
concurrent inference requests of cAI workloads through task affinity-aware
cluster mapping and migration, priority-aware task freezing/unfreezing, and
DVFS, while minimizing inference latency within power budgets. We implement and
deploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate
Twill against state-of-the-art edge AI inference techniques over contemporary
DNNs and LLMs, reducing inference latency by 54% on average, while honoring
power budgets.

</details>


### [124] [Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications](https://arxiv.org/abs/2507.00914)
*Jindong Han,Yansong Ning,Zirui Yuan,Hang Ni,Fan Liu,Tengfei Lyu,Hao Liu*

Main category: cs.MA

TL;DR: 本文介绍了基于大型语言模型（LLM）的智能城市代理（Urban LLM Agents），探讨其能力、工作流程、应用领域及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 利用LLM的语义理解和推理能力，实现智能城市的高效决策和可持续发展。

Method: 提出Urban LLM Agents的概念，分类其工作流程（感知、记忆、推理、执行、学习），并总结五大应用领域。

Result: 展示了Urban LLM Agents在城市规划、交通、环境、公共安全和城市社会中的代表性应用。

Conclusion: 讨论了可信度和评估问题，并指出未来研究方向，为LLM与城市智能的交叉领域奠定基础。

Abstract: The long-standing vision of intelligent cities is to create efficient,
livable, and sustainable urban environments using big data and artificial
intelligence technologies. Recently, the advent of Large Language Models (LLMs)
has opened new ways toward realizing this vision. With powerful semantic
understanding and reasoning capabilities, LLMs can be deployed as intelligent
agents capable of autonomously solving complex problems across domains. In this
article, we focus on Urban LLM Agents, which are LLM-powered agents that are
semi-embodied within the hybrid cyber-physical-social space of cities and used
for system-level urban decision-making. First, we introduce the concept of
urban LLM agents, discussing their unique capabilities and features. Second, we
survey the current research landscape from the perspective of agent workflows,
encompassing urban sensing, memory management, reasoning, execution, and
learning. Third, we categorize the application domains of urban LLM agents into
five groups: urban planning, transportation, environment, public safety, and
urban society, presenting representative works in each group. Finally, we
discuss trustworthiness and evaluation issues that are critical for real-world
deployment, and identify several open problems for future research. This survey
aims to establish a foundation for the emerging field of urban LLM agents and
to provide a roadmap for advancing the intersection of LLMs and urban
intelligence. A curated list of relevant papers and open-source resources is
maintained and continuously updated at
https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [125] [SwarmFusion: Revolutionizing Disaster Response with Swarm Intelligence and Deep Learning](https://arxiv.org/abs/2507.00005)
*Vasavi Lankipalle*

Main category: cs.NE

TL;DR: SwarmFusion结合粒子群优化和卷积神经网络，优化实时资源分配和路径规划，提升灾害响应速度和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 灾害响应需要在混乱环境中快速决策，现有方法效率不足。

Method: 集成粒子群优化和卷积神经网络，处理实时卫星、无人机和传感器数据。

Result: 在DisasterSim2025数据集上，响应时间提升40%，幸存者覆盖率达90%。

Conclusion: SwarmFusion为时间紧迫的灾害管理提供了可扩展的数据驱动解决方案。

Abstract: Disaster response requires rapid, adaptive decision-making in chaotic
environments. SwarmFusion, a novel hybrid framework, integrates particle swarm
optimization with convolutional neural networks to optimize real-time resource
allocation and path planning. By processing live satellite, drone, and sensor
data, SwarmFusion enhances situational awareness and operational efficiency in
flood and wildfire scenarios. Simulations using the DisasterSim2025 dataset
demonstrate up to 40 percentage faster response times and 90 percentage
survivor coverage compared to baseline methods. This scalable, data-driven
approach offers a transformative solution for time-critical disaster
management, with potential applications across diverse crisis scenarios.

</details>


### [126] [A Review on Zeroing Neural Networks](https://arxiv.org/abs/2507.00387)
*Chengze Jiang,Jie Gui,Long Jin,Shuai Li*

Main category: cs.NE

TL;DR: 本文综述了零化神经网络（ZNNs）在实现方法、分析理论和实际应用方面的进展。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对不同ZNNs之间关系及其推导的系统性研究，因此有必要对该领域进行综述以促进理解。

Method: 通过调查ZNNs的文献，总结其实现方法、理论分析和应用案例。

Result: 综述了ZNNs在时间变化优化和控制问题中的卓越表现及其相关进展。

Conclusion: 系统梳理ZNNs的研究有助于推动该领域的进一步发展和应用。

Abstract: Zeroing neural networks (ZNNs) have demonstrated outstanding performance on
time-varying optimization and control problems. Nonetheless, few studies are
committed to illustrating the relationship among different ZNNs and the
derivation of them. Therefore, reviewing the advances for a systematical
understanding of this field is desirable. This paper provides a survey of ZNNs'
progress regarding implementing methods, analysis theory, and practical
applications.

</details>


### [127] [Novel Complex-Valued Hopfield Neural Networks with Phase and Magnitude Quantization](https://arxiv.org/abs/2507.00461)
*Garimella Ramamurthy,Marcos Eduardo Valle,Tata Jagannadha Swamy*

Main category: cs.NE

TL;DR: 论文提出了两种新型复值Hopfield神经网络（CvHNNs），通过引入相位和幅度量化，显著增加了状态数量，扩展了应用范围。


<details>
  <summary>Details</summary>
Motivation: 现有复值Hopfield神经网络的状态数量有限，限制了其应用潜力。

Method: 第一种CvHNN基于直角坐标表示，使用天花板型激活函数；第二种基于极坐标表示，同样量化相位和幅度。

Result: 提出的CvHNNs显著增加了状态数量，扩展了潜在应用范围。

Conclusion: 新型CvHNNs通过量化相位和幅度，提升了模型性能和应用多样性。

Abstract: This research paper introduces two novel complex-valued Hopfield neural
networks (CvHNNs) that incorporate phase and magnitude quantization. The first
CvHNN employs a ceiling-type activation function that operates on the
rectangular coordinate representation of the complex net contribution. The
second CvHNN similarly incorporates phase and magnitude quantization but
utilizes a ceiling-type activation function based on the polar coordinate
representation of the complex net contribution. The proposed CvHNNs, with their
phase and magnitude quantization, significantly increase the number of states
compared to existing models in the literature, thereby expanding the range of
potential applications for CvHNNs.

</details>


### [128] [High-resolution spatial memory requires grid-cell-like neural codes](https://arxiv.org/abs/2507.00598)
*Madison Cotteret,Christopher J. Kymn,Hugh Greatorex,Martin Ziegler,Elisabetta Chicca,Friedrich T. Sommer*

Main category: cs.NE

TL;DR: 论文提出了一种基于稀疏二进制分布式编码的连续吸引子网络（CANs），解决了传统模型中稳定性和分辨率之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 传统连续吸引子网络（CANs）对噪声和异质性敏感，离散化虽能提高稳定性但牺牲分辨率，需要一种新方法兼顾两者。

Method: 采用稀疏二进制分布式编码，基于随机特征嵌入，模拟网格细胞样编码，理论分析和仿真验证其有效性。

Result: 新方法使CANs同时实现高稳定性和高分辨率，并能扩展到非线性流形和自由编程的向量场积分。

Conclusion: 该研究为大脑如何高分辨率、稳定地表示连续变量并执行灵活计算提供了理论支持。

Abstract: Continuous attractor networks (CANs) are widely used to model how the brain
temporarily retains continuous behavioural variables via persistent recurrent
activity, such as an animal's position in an environment. However, this memory
mechanism is very sensitive to even small imperfections, such as noise or
heterogeneity, which are both common in biological systems. Previous work has
shown that discretising the continuum into a finite set of discrete attractor
states provides robustness to these imperfections, but necessarily reduces the
resolution of the represented variable, creating a dilemma between stability
and resolution. We show that this stability-resolution dilemma is most severe
for CANs using unimodal bump-like codes, as in traditional models. To overcome
this, we investigate sparse binary distributed codes based on random feature
embeddings, in which neurons have spatially-periodic receptive fields. We
demonstrate theoretically and with simulations that such grid-cell-like codes
enable CANs to achieve both high stability and high resolution simultaneously.
The model extends to embedding arbitrary nonlinear manifolds into a CAN, such
as spheres or tori, and generalises linear path integration to integration
along freely-programmable on-manifold vector fields. Together, this work
provides a theory of how the brain could robustly represent continuous
variables with high resolution and perform flexible computations over
task-relevant manifolds.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [129] [InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph](https://arxiv.org/abs/2507.00066)
*Xingyu Xiao,Jiejuan Tong,Peng Chen,Jun Sun,Zhe Sui,Jingang Liang,Hongru Zhao,Jun Zhao,Haitao Wang*

Main category: cs.HC

TL;DR: 该研究提出了一种基于AutoGraph的框架（InSight-R），用于自动化识别人为故障事件（HFEs）并评估界面设计对操作员性能的影响，解决了传统HRA方法的主观性和数据整合不足问题。


<details>
  <summary>Details</summary>
Motivation: 传统人为可靠性分析（HRA）方法依赖专家判断，导致结果主观且难以复现，且缺乏对界面设计影响的系统评估。

Method: 通过AutoGraph构建界面嵌入知识图（IE-KG），结合实证行为数据，自动化识别HFEs并评估界面设计风险。

Result: InSight-R提高了HFE识别的客观性和可解释性，为动态实时人为可靠性评估提供了可扩展的途径。

Conclusion: 该框架为界面设计优化提供了实用见解，并推动了机制驱动的HRA方法发展。

Abstract: Human reliability remains a critical concern in safety-critical domains such
as nuclear power, where operational failures are often linked to human error.
While conventional human reliability analysis (HRA) methods have been widely
adopted, they rely heavily on expert judgment for identifying human failure
events (HFEs) and assigning performance influencing factors (PIFs). This
reliance introduces challenges related to reproducibility, subjectivity, and
limited integration of interface-level data. In particular, current approaches
lack the capacity to rigorously assess how human-machine interface design
contributes to operator performance variability and error susceptibility. To
address these limitations, this study proposes a framework for risk-informed
human failure event identification and interface-induced risk assessment driven
by AutoGraph (InSight-R). By linking empirical behavioral data to the
interface-embedded knowledge graph (IE-KG) constructed by the automated
graph-based execution framework (AutoGraph), the InSight-R framework enables
automated HFE identification based on both error-prone and time-deviated
operational paths. Furthermore, we discuss the relationship between
designer-user conflicts and human error. The results demonstrate that InSight-R
not only enhances the objectivity and interpretability of HFE identification
but also provides a scalable pathway toward dynamic, real-time human
reliability assessment in digitalized control environments. This framework
offers actionable insights for interface design optimization and contributes to
the advancement of mechanism-driven HRA methodologies.

</details>


### [130] [Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments](https://arxiv.org/abs/2507.00161)
*Christopher M. Wegemer,Edward Halim,Jeff Burke*

Main category: cs.HC

TL;DR: AI技术通过情感适应性叙事减少政治极化，促进开放思维。


<details>
  <summary>Details</summary>
Motivation: 政治极化阻碍民主公民教育，需新技术干预。

Method: 结合情感计算与叙事理论，开发AI-DCS平台，实时调整内容。

Result: 原型整合情绪识别与GPT-4，提升情感投入与换位思考。

Conclusion: 为AI支持的情感敏感策略奠定基础，推动公民教育与算法素养。

Abstract: Political polarization undermines democratic civic education by exacerbating
identity-based resistance to opposing viewpoints. Emerging AI technologies
offer new opportunities to advance interventions that reduce polarization and
promote political open-mindedness. We examined novel design strategies that
leverage adaptive and emotionally-responsive civic narratives that may sustain
students' emotional engagement in stories, and in turn, promote
perspective-taking toward members of political out-groups. Drawing on theories
from political psychology and narratology, we investigate how affective
computing techniques can support three storytelling mechanisms: transportation
into a story world, identification with characters, and interaction with the
storyteller. Using a design-based research (DBR) approach, we iteratively
developed and refined an AI-mediated Digital Civic Storytelling (AI-DCS)
platform. Our prototype integrates facial emotion recognition and attention
tracking to assess users' affective and attentional states in real time.
Narrative content is organized around pre-structured story outlines, with
beat-by-beat language adaptation implemented via GPT-4, personalizing
linguistic tone to sustain students' emotional engagement in stories that
center political perspectives different from their own. Our work offers a
foundation for AI-supported, emotionally-sensitive strategies that address
affective polarization while preserving learner autonomy. We conclude with
implications for civic education interventions, algorithmic literacy, and HCI
challenges associated with AI dialogue management and affect-adaptive learning
environments.

</details>


### [131] [Exploring AR Label Placements in Visually Cluttered Scenarios](https://arxiv.org/abs/2507.00198)
*Ji Hwan Park,Braden Roper,Amirhossein Arezoumand,Tien Tran*

Main category: cs.HC

TL;DR: 研究在AR环境中视觉杂乱场景下的标签放置方法，提出三种技术并验证其效果。


<details>
  <summary>Details</summary>
Motivation: 随着AR场景中物品数量增加，现有标签放置指南难以有效应对视觉杂乱问题。

Method: 实现三种标签放置技术，针对同一类型物品密集分布的场景。

Result: 研究表明，使用标签对同类物品进行空间分组有助于识别、比较和总结数据。

Conclusion: 在AR环境中，标签的空间分组技术能有效提升用户体验。

Abstract: We investigate methods for placing labels in AR environments that have
visually cluttered scenes. As the number of items increases in a scene within
the user' FOV, it is challenging to effectively place labels based on existing
label placement guidelines. To address this issue, we implemented three label
placement techniques for in-view objects for AR applications. We specifically
target a scenario, where various items of different types are scattered within
the user's field of view, and multiple items of the same type are situated
close together. We evaluate three placement techniques for three target tasks.
Our study shows that using a label to spatially group the same types of items
is beneficial for identifying, comparing, and summarizing data.

</details>


### [132] [Examining the Social Communication and Community Engagement of Autistic Adults through an Asynchronous Focus Group](https://arxiv.org/abs/2507.00202)
*Blade Frisch,Betts Peters,Keith Vertanen*

Main category: cs.HC

TL;DR: 研究探讨自闭症成人的沟通需求及AAC设计，发现情绪、自闭症关闭状态影响沟通，提出未来AAC设计建议。


<details>
  <summary>Details</summary>
Motivation: 探索自闭症成人的独特沟通需求及AAC如何支持这些需求。

Method: 在线异步文本焦点小组，五名自闭症成人参与。

Result: 情绪影响沟通方式，AAC对能说话的自闭症者有益，自闭症关闭状态需动态支持。

Conclusion: 提出AAC设计建议及未来研究方向，如自闭症关闭状态沟通需求、社会环境影响等。

Abstract: Purpose: Little research has explored the communication needs of autistic
adults and how their needs differ from those of other disabled populations.
Augmentative and Alternative Communication (AAC) can support these
communication needs, but more guidance is needed on how to design AAC to
support this population.
  Materials and Methods: We conducted an online, asynchronous, text-based focus
group with five autistic adults to explore their social communication and
community engagement and how AAC can help support them.
  Results and Conclusion: Our analysis of the participant responses found that
1) participants' emotional experiences impacted the communication methods they
used, 2) speaking autistic adults can benefit from AAC use, and 3) autistic
shutdown creates dynamic communication needs. We present implications for
future AAC design: supporting communication in times of shutdown, indicating
communication ability to communication partners, and a need to better
understand the fear of using AAC. These implications can inform the design for
future AAC systems. We also provide themes for future autism research:
exploring the impact of a late diagnosis, gaining a better understanding of the
communication needs during autistic shutdown, and expanding research to include
the social and environmental factors that impact communication. Finally, we
provide guidance on how future online focus groups can be run in an accessible
manner.

</details>


### [133] [User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the "Sunday Blues"](https://arxiv.org/abs/2507.00271)
*Zhuochao Peng,Jiaxin Xu,Jun Hu,Haian Xue,Laurens A. G. Kolks,Pieter M. A. Desmet*

Main category: cs.HC

TL;DR: 研究探讨了社交机器人如何帮助调节日常情绪，以“周日忧郁”为例，通过案例研究揭示了用户对机器人使用的期望和担忧。


<details>
  <summary>Details</summary>
Motivation: 探索用户对社交机器人融入日常生活的看法，特别是在情绪管理方面的潜在应用。

Method: 采用探索性案例研究，使用虚构机器人“Mora”和视频原型，通过故事共建方法与15名参与者讨论。

Result: 研究发现用户对社交机器人的共情能力、干预效果和伦理边界有复杂看法，为未来设计提供了参考。

Conclusion: 研究为社交机器人在情绪管理中的应用提供了设计思路，强调了用户视角的重要性。

Abstract: While recent research highlights the potential of social robots to support
mood regulation, little is known about how prospective users view their
integration into everyday life. To explore this, we conducted an exploratory
case study that used a speculative robot concept "Mora" to provoke reflection
and facilitate meaningful discussion about using social robots to manage
subtle, day-to-day emotional experiences. We focused on the "Sunday Blues," a
common dip in mood that occurs at the end of the weekend, as a relatable
context in which to explore individuals' insights. Using a video prototype and
a co-constructing stories method, we engaged 15 participants in imagining
interactions with Mora and discussing their expectations, doubts, and concerns.
The study surfaced a range of nuanced reflections around the attributes of
social robots like empathy, intervention effectiveness, and ethical boundaries,
which we translated into design considerations for future research and
development in human-robot interaction.

</details>


### [134] [Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels](https://arxiv.org/abs/2507.00333)
*Emin Zerman,Jonas Carlsson,Mårten Sjöström*

Main category: cs.HC

TL;DR: 研究开发了一种射击可视化系统，通过第一人称视角视频和叠加数据帮助射手和教练分析表现，发现复合视图（结合原始视频、极坐标图和图表）最受欢迎且有效。


<details>
  <summary>Details</summary>
Motivation: 当前射击训练主要依赖重复练习和事后分析，缺乏实时视角共享和深入分析工具，因此需要更直观的可视化系统。

Method: 开发了五种复合可视化视图，基于第一人称射击视频和叠加数据，通过混合方法研究（任务测试、偏好比较和访谈）评估效果。

Result: 复合视图（仪表盘风格）在10名参与者中9人偏好，且对不同技能水平的射手均有帮助。

Conclusion: 第一人称视频与可视化分析的结合在射击训练中具有潜力，并可推广至其他精准运动。

Abstract: Marksmanship practices are required in various professions, including police,
military personnel, hunters, as well as sports shooters, such as Olympic
shooting, biathlon, and modern pentathlon. The current form of training and
coaching is mostly based on repetition, where the coach does not see through
the eyes of the shooter, and analysis is limited to stance and accuracy
post-session. In this study, we present a shooting visualization system and
evaluate its perceived effectiveness for both novice and expert shooters. To
achieve this, five composite visualizations were developed using first-person
shooting video recordings enriched with overlaid metrics and graphical
summaries. These views were evaluated with 10 participants (5 expert marksmen,
5 novices) through a mixed-methods study including shot-count and aiming
interpretation tasks, pairwise preference comparisons, and semi-structured
interviews. The results show that a dashboard-style composite view, combining
raw video with a polar plot and selected graphs, was preferred in 9 of 10 cases
and supported understanding across skill levels. The insights gained from this
design study point to the broader value of integrating first-person video with
visual analytics for coaching, and we suggest directions for applying this
approach to other precision-based sports.

</details>


### [135] [Visual Privacy Management with Generative AI for Blind and Low-Vision People](https://arxiv.org/abs/2507.00286)
*Tanusree Sharma,Yu-Yun Tseng,Lotus Zhang,Ayae Ide,Kelly Avery Mack,Leah Findlater,Danna Gurari,Yang Wang*

Main category: cs.HC

TL;DR: BLV人群使用GenAI工具处理视觉内容，但面临视觉隐私挑战。通过21人访谈研究，发现用户权衡隐私、效率和情感代理，并提出设计偏好如本地处理、零保留保证等。


<details>
  <summary>Details</summary>
Motivation: 研究BLV人群如何通过GenAI工具增强视觉内容可访问性，同时应对隐私挑战。

Method: 通过21名BLV参与者的访谈研究，分析其当前实践和未来设计偏好。

Result: 用户在实践中平衡隐私、效率和情感代理，提出本地处理、敏感内容屏蔽等设计偏好。

Conclusion: 提出以用户为中心的GenAI设计建议，扩展隐私概念和他人数据的负责任处理。

Abstract: Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to
interpret and manage visual content in their daily lives. While such tools can
enhance the accessibility of visual content and so enable greater user
independence, they also introduce complex challenges around visual privacy. In
this paper, we investigate the current practices and future design preferences
of blind and low vision individuals through an interview study with 21
participants. Our findings reveal a range of current practices with GenAI that
balance privacy, efficiency, and emotional agency, with users accounting for
privacy risks across six key scenarios, such as self-presentation,
indoor/outdoor spatial privacy, social sharing, and handling professional
content. Our findings reveal design preferences, including on-device
processing, zero-retention guarantees, sensitive content redaction,
privacy-aware appearance indicators, and multimodal tactile mirrored
interaction methods. We conclude with actionable design recommendations to
support user-centered visual privacy through GenAI, expanding the notion of
privacy and responsible handling of others data.

</details>


### [136] [When Kids Mode Isn't For Kids: Investigating TikTok's "Under 13 Experience"](https://arxiv.org/abs/2507.00299)
*Olivia Figueira,Pranathi Chamarthi,Tu Le,Athina Markopoulou*

Main category: cs.HC

TL;DR: 论文研究了TikTok的“儿童模式”，发现其内容审核存在问题，83%的视频并非面向儿童，且缺乏关键功能如家长控制和辅助设置。


<details>
  <summary>Details</summary>
Motivation: 尽管TikTok的常规模式已被广泛研究，但其儿童模式的内容审核和安全隐私保护缺乏透明度和研究。

Method: 提出并应用了一种审计方法，基于COPPA法规，分析儿童模式的内容审核和儿童导向内容的普及情况。

Result: 研究发现83%的视频非儿童导向，甚至存在不适宜内容，且平台缺乏家长控制和辅助设置。

Conclusion: 研究结果对设计和监管有重要意义，儿童可能因此转向常规模式，面临更多安全和隐私风险。

Abstract: TikTok, the social media platform that is popular among children and
adolescents, offers a more restrictive "Under 13 Experience" exclusively for
young users in the US, also known as TikTok's "Kids Mode". While prior research
has studied various aspects of TikTok's regular mode, including privacy and
personalization, TikTok's Kids Mode remains understudied, and there is a lack
of transparency regarding its content curation and its safety and privacy
protections for children. In this paper, (i) we propose an auditing methodology
to comprehensively investigate TikTok's Kids Mode and (ii) we apply it to
characterize the platform's content curation and determine the prevalence of
child-directed content, based on regulations in the Children's Online Privacy
Protection Act (COPPA). We find that 83% of videos observed on the "For You"
page in Kids Mode are actually not child-directed, and even inappropriate
content was found. The platform also lacks critical features, namely parental
controls and accessibility settings. Our findings have important design and
regulatory implications, as children may be incentivized to use TikTok's
regular mode instead of Kids Mode, where they are known to be exposed to
further safety and privacy risks.

</details>


### [137] [EEG-Based Auditory BCI for Communication in a Completely Locked-In Patient Using Volitional Frequency Band Modulation](https://arxiv.org/abs/2507.00305)
*Deland Liu,Frigyes Samuel Racz,Zoe Lalji,Jose del R. Millan*

Main category: cs.HC

TL;DR: 非侵入性脑电图（EEG）脑机接口（BCI）可能为完全闭锁状态（CLIS）患者恢复基本沟通能力提供途径。


<details>
  <summary>Details</summary>
Motivation: 完全闭锁状态（CLIS）患者失去所有可靠运动控制，无法沟通，研究是否可通过非侵入性EEG-BCI实现其意志性沟通。

Method: 通过EEG-BCI系统，患者通过调节alpha和beta波段功率在不同通道上实现“是”/“否”回答，并接受实时听觉反馈。

Result: 患者在辅助需求问题上表现高于随机水平，最后一节达到完美；一般知识问题表现不一，部分节次准确且高于随机水平。

Conclusion: 非侵入性BCI可能为CLIS患者提供基本沟通途径。

Abstract: Patients with amyotrophic lateral sclerosis (ALS) in the completely locked-in
state (CLIS) can lose all reliable motor control and are left without any means
of communication. It remains unknown whether non-invasive electroencephalogram
(EEG) based brain-computer interfaces (BCIs) can support volitional
communication in CLIS. Here, we show that a CLIS patient was able to operate an
EEG-based BCI across multiple online sessions to respond to both general
knowledge and personally relevant assistive questions. The patient delivered
"Yes"/"No" responses by volitionally modulating alpha and beta band power at
different channels, guided by real-time auditory feedback from the BCI. The
patient communicated assistive needs above chance in all sessions, achieving a
perfect score in the final session. Performance on general knowledge questions
varied across sessions, with two sessions showing accurate and above-chance
responses, while the first and last sessions remained at chance level. The
patient also showed consistent modulation patterns over time. These findings
suggest that non-invasive BCIs may offer a potential pathway for restoring
basic communication in CLIS.

</details>


### [138] [Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center](https://arxiv.org/abs/2507.00513)
*Kai Qin,Kexin Du,Yimeng Chen,Yueyan Liu,Jie Cai,Zhiqiang Nie,Nan Gao,Guohui Wei,Shengzhu Wang,Chun Yu*

Main category: cs.HC

TL;DR: 研究探讨了电网客服中心员工对AI辅助的感知，发现AI减轻了传统负担但引入了新负担。


<details>
  <summary>Details</summary>
Motivation: 探索AI工具在客服代表与客户互动中的实际影响。

Method: 通过实地考察和半结构化访谈13名客服代表。

Result: AI减轻了打字和记忆等负担，但带来了学习、合规和心理负担。

Conclusion: 研究深化了对AI在组织中整合的理解，并强调了客服代表的适应努力和负担。

Abstract: The integration of various AI tools creates a complex socio-technical
environment where employee-customer interactions form the core of work
practices. This study investigates how customer service representatives (CSRs)
at the power grid service customer service call center perceive AI assistance
in their interactions with customers. Through a field visit and semi-structured
interviews with 13 CSRs, we found that AI can alleviate some traditional
burdens during the call (e.g., typing and memorizing) but also introduces new
burdens (e.g., earning, compliance, psychological burdens). This research
contributes to a more nuanced understanding of AI integration in organizational
settings and highlights the efforts and burdens undertaken by CSRs to adapt to
the updated system.

</details>


### [139] [Gaze3P: Gaze-Based Prediction of User-Perceived Privacy](https://arxiv.org/abs/2507.00596)
*Mayar Elfares,Pascal Reisert,Ralf Küsters,Andreas Bulling*

Main category: cs.HC

TL;DR: Gaze3P数据集通过眼动数据预测用户隐私感知，并优化隐私保护技术参数。


<details>
  <summary>Details</summary>
Motivation: 解决现有隐私感知量化方法依赖问卷且未充分应用于隐私保护技术优化的局限性。

Method: 收集100名参与者的眼动数据及1000个刺激物，训练机器学习模型预测隐私感知。

Result: 模型预测准确率高，并成功优化差分隐私机制参数。

Conclusion: Gaze3P为隐私感知研究提供了新工具，提升了隐私保护技术的用户对齐性。

Abstract: Privacy is a highly subjective concept and perceived variably by different
individuals. Previous research on quantifying user-perceived privacy has
primarily relied on questionnaires. Furthermore, applying user-perceived
privacy to optimise the parameters of privacy-preserving techniques (PPT)
remains insufficiently explored. To address these limitations, we introduce
Gaze3P -- the first dataset specifically designed to facilitate systematic
investigations into user-perceived privacy. Our dataset comprises gaze data
from 100 participants and 1,000 stimuli, encompassing a range of private and
safe attributes. With Gaze3P, we train a machine learning model to implicitly
and dynamically predict perceived privacy from human eye gaze. Through
comprehensive experiments, we show that the resulting models achieve high
accuracy. Finally, we illustrate how predicted privacy can be used to optimise
the parameters of differentially private mechanisms, thereby enhancing their
alignment with user expectations.

</details>


### [140] [Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity](https://arxiv.org/abs/2507.00657)
*Jacopo Nudo,Mario Edoardo Pandolfo,Edoardo Loru,Mattia Samory,Matteo Cinelli,Walter Quattrociocchi*

Main category: cs.HC

TL;DR: 研究探讨了大型语言模型（LLM）在模拟社交媒体政治讨论时的行为，发现其输出会放大极化、风格化信号和有害语言，且更反映内部优化动态而非真实用户行为。


<details>
  <summary>Details</summary>
Motivation: 探究LLM在模拟政治讨论时的表现及其对社会应用的潜在影响。

Method: 基于2024年美国总统选举期间的2100万次X平台互动，构建了1186个真实用户的LLM代理，在受控条件下回复政治推文，比较零样本和少样本初始化效果。

Result: 发现LLM输出会放大极化、风格化信号和有害语言，且存在“生成夸张”现象，即系统性放大显著特征。

Conclusion: LLM的输出更反映内部优化动态而非真实用户行为，其作为社会代理的可靠性受到挑战。

Abstract: We investigate how Large Language Models (LLMs) behave when simulating
political discourse on social media. Leveraging 21 million interactions on X
during the 2024 U.S. presidential election, we construct LLM agents based on
1,186 real users, prompting them to reply to politically salient tweets under
controlled conditions. Agents are initialized either with minimal ideological
cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one
comparisons with human replies. We evaluate three model families (Gemini,
Mistral, and DeepSeek) across linguistic style, ideological consistency, and
toxicity. We find that richer contextualization improves internal consistency
but also amplifies polarization, stylized signals, and harmful language. We
observe an emergent distortion that we call "generation exaggeration": a
systematic amplification of salient traits beyond empirical baselines. Our
analysis shows that LLMs do not emulate users, they reconstruct them. Their
outputs, indeed, reflect internal optimization dynamics more than observed
behavior, introducing structural biases that compromise their reliability as
social proxies. This challenges their use in content moderation, deliberative
simulations, and policy modeling.

</details>


### [141] [Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review](https://arxiv.org/abs/2507.00775)
*Haonan Yao,Lingyun Yu,Lijie Yao*

Main category: cs.HC

TL;DR: 本文对有形数据探索中的任务、交互和可视化小部件进行了系统综述，旨在填补该领域缺乏结构化理解的空白。


<details>
  <summary>Details</summary>
Motivation: 有形小部件能降低认知负荷并支持复杂任务，但缺乏对任务类型、交互方法和部件设计的协调理解，限制了设计模式的识别和创新机会。

Method: 通过系统综述分析现有工作，总结数据探索任务、交互和可视化小部件的设计特点。

Result: 提出了未来有形数据探索小部件设计工具包的研究议程。

Conclusion: 系统综述为有形数据探索领域的设计和创新提供了基础，并提出了未来研究方向。

Abstract: We present a systematic review on tasks, interactions, and visualization
widgets (refer to tangible entities that are used to accomplish data
exploration tasks through specific interactions) in the context of tangible
data exploration. Tangible widgets have been shown to reduce cognitive load,
enable more natural interactions, and support the completion of complex data
exploration tasks. Yet, the field lacks a structured understanding of how task
types, interaction methods, and widget designs are coordinated, limiting the
ability to identify recurring design patterns and opportunities for innovation.
To address this gap, we conduct a systematic review to analyze existing work
and characterize the current design of data exploration tasks, interactions,
and tangible visualization widgets. We next reflect based on our findings and
propose a research agenda to inform the development of a future widget design
toolkit for tangible data exploration. Our systematic review and supplemental
materials are available at physicalviswidget.github.io and osf.io/vjw5e.

</details>


### [142] [Sensemaking Through Making: Developing Clinical Domain Knowledge by Crafting Synthetic Datasets and Prototyping System Architectures](https://arxiv.org/abs/2507.00821)
*Mihnea Stefan Calota,Wessel Nieuwenhuys,Janet Yi-Ching Huang,Lin-Lin Chen,Mathias Funk*

Main category: cs.HC

TL;DR: 本文提出了一种通过制作合成数据集的方法，帮助设计师在无法直接接触医疗系统的情况下理解复杂的医疗环境。


<details>
  <summary>Details</summary>
Motivation: 设计师在医疗领域有重要影响，但医院系统的封闭性限制了他们对数据和系统的接触。

Method: 通过观察和建模真实的远程患者监测（RPM）环境，制作合成数据集，并迭代开发简化的RPM系统。

Result: 设计师可以通过动手操作数据结构和迭代原型开发，熟悉复杂的医疗系统。

Conclusion: 这种方法为设计师提供了一种间接但有效的途径，以理解不透明的医疗系统。

Abstract: Designers have ample opportunities to impact the healthcare domain. However,
hospitals are often closed ecosystems that pose challenges in engaging clinical
stakeholders, developing domain knowledge, and accessing relevant systems and
data. In this paper, we introduce a making-oriented approach to help designers
understand the intricacies of their target healthcare context. Using Remote
Patient Monitoring (RPM) as a case study, we explore how manually crafting
synthetic datasets based on real-world observations enables designers to learn
about complex data-driven healthcare systems. Our process involves observing
and modeling the real-world RPM context, crafting synthetic datasets, and
iteratively prototyping a simplified RPM system that balances contextual
richness and intentional abstraction. Through this iterative process of
sensemaking through making, designers can still develop context familiarity
when direct access to the actual healthcare system is limited. Our approach
emphasizes the value of hands-on interaction with data structures to support
designers in understanding opaque healthcare systems.

</details>


### [143] [Towards Difficulty-Aware Analysis of Deep Neural Networks](https://arxiv.org/abs/2507.00881)
*Linhao Meng,Stef van den Elzen,Anna Vilanova*

Main category: cs.HC

TL;DR: 论文提出了一种将实例难度纳入深度神经网络评估的方法，结合数据、模型和人类视角，并开发了交互式工具DifficultyEyes用于分析。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅关注误分类实例，忽略了不同实例的难度差异，而模型应能反映这些挑战。

Method: 从数据、模型和人类三个角度衡量实例难度，并开发了交互式工具DifficultyEyes。

Result: 案例研究表明该方法有效。

Conclusion: 该方法能更全面地评估模型性能，并帮助识别潜在问题。

Abstract: Traditional instance-based model analysis focuses mainly on misclassified
instances. However, this approach overlooks the varying difficulty associated
with different instances. Ideally, a robust model should recognize and reflect
the challenges presented by intrinsically difficult instances. It is also
valuable to investigate whether the difficulty perceived by the model aligns
with that perceived by humans. To address this, we propose incorporating
instance difficulty into the deep neural network evaluation process,
specifically for supervised classification tasks on image data. Specifically,
we consider difficulty measures from three perspectives -- data, model, and
human -- to facilitate comprehensive evaluation and comparison. Additionally,
we develop an interactive visual tool, DifficultyEyes, to support the
identification of instances of interest based on various difficulty patterns
and to aid in analyzing potential data or model issues. Case studies
demonstrate the effectiveness of our approach.

</details>


### [144] [Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception](https://arxiv.org/abs/2507.00963)
*Fan Wang,Giulia Perugia,Yuan Feng,Wijnand IJsselsteijn*

Main category: cs.HC

TL;DR: 本文探讨了社交机器人在痴呆症护理中可能引发的误导性感知，分析了设计线索如何影响这些感知，并提出了基于双过程理论的机器人欺骗定义。


<details>
  <summary>Details</summary>
Motivation: 随着社交机器人进入痴呆症护理领域，如何避免误导性感知成为重要问题，但目前研究不足。

Method: 通过综述26项实证研究，识别了四类可能引发误导的设计线索，并分析了用户反应。

Result: 研究发现痴呆症患者常赋予机器人生物、社交和心理能力，感知在意识和幻觉间动态变化。

Conclusion: 提出基于双过程理论的机器人欺骗定义，强调设计需兼顾互动性和认知尊重。

Abstract: As social robots increasingly enter dementia care, concerns about deception,
intentional or not, are gaining attention. Yet, how robotic design cues might
elicit misleading perceptions in people with dementia, and how these
perceptions arise, remains insufficiently understood. In this scoping review,
we examined 26 empirical studies on interactions between people with dementia
and physical social robots. We identify four key design cue categories that may
influence deceptive impressions: cues resembling physiological signs (e.g.,
simulated breathing), social intentions (e.g., playful movement), familiar
beings (e.g., animal-like form and sound), and, to a lesser extent, cues that
reveal artificiality. Thematic analysis of user responses reveals that people
with dementia often attribute biological, social, and mental capacities to
robots, dynamically shifting between awareness and illusion. These findings
underscore the fluctuating nature of ontological perception in dementia
contexts. Existing definitions of robotic deception often rest on philosophical
or behaviorist premises, but rarely engage with the cognitive mechanisms
involved. We propose an empirically grounded definition: robotic deception
occurs when Type 1 (automatic, heuristic) processing dominates over Type 2
(deliberative, analytic) reasoning, leading to misinterpretation of a robot's
artificial nature. This dual-process perspective highlights the ethical
complexity of social robots in dementia care and calls for design approaches
that are not only engaging, but also epistemically respectful.

</details>


### [145] [A Comprehensive Review of Human Error in Risk-Informed Decision Making: Integrating Human Reliability Assessment, Artificial Intelligence, and Human Performance Models](https://arxiv.org/abs/2507.01017)
*Xingyu Xiao,Hongxu Zhu,Jingang Liang,Jiejuan Tong,Haitao Wang*

Main category: cs.HC

TL;DR: 本文综述了人工智能、认知科学与风险决策的交叉领域，探讨如何通过整合认知模型与AI技术提升高风险系统中的人为错误预测与防范能力。


<details>
  <summary>Details</summary>
Motivation: 人为错误在核能、航空等高风险领域仍是主要风险源，现有方法受限于数据稀缺、算法不透明和专家依赖。

Method: 分类人为错误形式，分析风险决策框架，结合认知模型与AI技术优化人为可靠性评估（HRA）。

Result: 整合认知模型与AI分析显著提升预测准确性，但需更丰富数据、透明算法和严格验证。

Conclusion: 未来研究方向包括结合韧性工程理论与跨领域数据合作，建立多学科范式以提升高风险系统的可靠性。

Abstract: Human error remains a dominant risk driver in safety-critical sectors such as
nuclear power, aviation, and healthcare, where seemingly minor mistakes can
cascade into catastrophic outcomes. Although decades of research have produced
a rich repertoire of mitigation techniques, persistent limitations: scarce
high-quality data, algorithmic opacity, and residual reliance on expert
judgment, continue to constrain progress. This review synthesizes recent
advances at the intersection of risk-informed decision making, human
reliability assessment (HRA), artificial intelligence (AI), and cognitive
science to clarify how their convergence can curb human-error risk. We first
categorize the principal forms of human error observed in complex
sociotechnical environments and outline their quantitative impact on system
reliability. Next, we examine risk-informed frameworks that embed HRA within
probabilistic and data-driven methodologies, highlighting successes and gaps.
We then survey cognitive and human-performance models, detailing how
mechanistic accounts of perception, memory, and decision-making enrich error
prediction and complement HRA metrics. Building on these foundations, we
critically assess AI-enabled techniques for real-time error detection,
operator-state estimation, and AI-augmented HRA workflows. Across these
strands, a recurring insight emerges: integrating cognitive models with
AI-based analytics inside risk-informed HRA pipelines markedly enhances
predictive fidelity, yet doing so demands richer datasets, transparent
algorithms, and rigorous validation. Finally, we identify promising research
directions, coupling resilience engineering concepts with grounded theory,
operationalizing the iceberg model of incident causation, and establishing
cross-domain data consortia, to foster a multidisciplinary paradigm that
elevates human reliability in high-stakes systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [146] [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
*Christopher James Augeri*

Main category: cs.LG

TL;DR: 论文提出HDRAM框架，通过信息论方法解决LLMs中的精度损失问题，结合经典纠错码、全息计算和量子搜索，显著提升关联检索能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）中因信息扩散导致的精度损失问题，将其重新定义为信息论通信问题。

Method: 引入HDRAM框架，将Transformer潜在空间视为扩频信道，结合经典纠错码、全息计算和量子搜索，实现高效键值操作和Grover式搜索。

Result: HDRAM显著提升了关联检索能力，且无需改变模型架构。

Conclusion: 通过CHQ（经典-全息-量子启发）原则，HDRAM为Transformer架构提供了新的强化方向。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from
apparent precision loss, reframed here as information spreading. This reframing
shifts the problem from computational precision to an information-theoretic
communication issue. We address the K:V and V:K memory problem in LLMs by
introducing HDRAM (Holographically Defined Random Access Memory), a symbolic
memory framework treating transformer latent space as a spread-spectrum
channel. Built upon hypertokens, structured symbolic codes integrating
classical error-correcting codes (ECC), holographic computing, and
quantum-inspired search, HDRAM recovers distributed information through
principled despreading. These phase-coherent memory addresses enable efficient
key-value operations and Grover-style search in latent space. By combining ECC
grammar with compressed sensing and Krylov subspace alignment, HDRAM
significantly improves associative retrieval without architectural changes,
demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can
fortify transformer architectures.

</details>


### [147] [Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE](https://arxiv.org/abs/2507.00003)
*Eyhab Al-Masri*

Main category: cs.LG

TL;DR: NeutroSENSE是一个基于中性逻辑的集成框架，用于物联网环境中的可解释入侵检测，通过分解预测置信度为真、假和不确定性分量，实现不确定性量化和主动回避。


<details>
  <summary>Details</summary>
Motivation: 提升物联网环境中入侵检测的准确性和可解释性，同时量化不确定性以支持更可信的决策。

Method: 集成随机森林、XGBoost和逻辑回归，结合中性逻辑分解预测置信度，并使用全局和自适应阈值标记高不确定性预测。

Result: 在IoT-CAD数据集上达到97%准确率，误分类样本的不确定性显著高于正确分类样本。

Conclusion: 中性逻辑提升了准确性和可解释性，为边缘和雾计算物联网安全系统提供了信任感知的AI基础。

Abstract: This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework
for interpretable intrusion detection in IoT environments. By integrating
Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the
system decomposes prediction confidence into truth (T), falsity (F), and
indeterminacy (I) components, enabling uncertainty quantification and
abstention. Predictions with high indeterminacy are flagged for review using
both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD
dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that
misclassified samples exhibit significantly higher indeterminacy (I = 0.62)
than correct ones (I = 0.24). The use of indeterminacy as a proxy for
uncertainty enables informed abstention and targeted review-particularly
valuable in edge deployments. Figures and tables validate the correlation
between I-scores and error likelihood, supporting more trustworthy,
human-in-the-loop AI decisions. This work shows that neutrosophic logic
enhances both accuracy and explainability, providing a practical foundation for
trust-aware AI in edge and fog-based IoT security systems.

</details>


### [148] [A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search](https://arxiv.org/abs/2507.00004)
*Austin R. Ellis-Mohr,Anuj K. Nayak,Lav R. Varshney*

Main category: cs.LG

TL;DR: 论文提出了一种名为DS3的框架，用于优化大型语言模型的推理成本，通过技能图随机遍历实现高效推理，并分析了不同推理策略的效率和任务难度关系。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在训练和推理阶段的资源消耗巨大，现有方法未能全面考虑计算最优性，需要更高效的框架来优化推理成本。

Method: 引入DS3框架，将推理建模为技能图的随机遍历，推导出任务成功率和计算成本的闭式表达式，并与训练框架结合。

Result: 理论分析揭示了线性精度与对数计算的关系、任务难度与模型能力对推理策略的影响，以及推理引发的涌现行为。

Conclusion: DS3框架深化了对训练与推理相互依赖的理解，为算法设计和资源分配提供了理论基础。

Abstract: Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.

</details>


### [149] [What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness](https://arxiv.org/abs/2507.00195)
*Kumar Kshitij Patel*

Main category: cs.LG

TL;DR: 该论文深入研究了分布式和联邦优化中局部更新算法（如Local SGD）的理论性能，特别是在数据异构性下的表现。核心贡献是证明了二阶有界异构假设的必要性和充分性，并提出了更精细的收敛分析框架。


<details>
  <summary>Details</summary>
Motivation: 研究局部更新算法在异构数据环境下的性能，明确其优于集中式或小批量方法的条件。

Method: 采用基于共识误差的精细分析框架，结合三阶平滑性和松弛的异构假设，推导收敛边界。

Result: 证明了局部更新算法在凸和非凸问题中的优越性，并扩展到在线联邦学习，提供了遗憾边界。

Conclusion: 论文阐明了局部更新算法的优势条件，为异构环境中的Local SGD分析提供了完整指南。

Abstract: This thesis contributes to the theoretical understanding of local update
algorithms, especially Local SGD, in distributed and federated optimization
under realistic models of data heterogeneity. A central focus is on the bounded
second-order heterogeneity assumption, which is shown to be both necessary and
sufficient for local updates to outperform centralized or mini-batch methods in
convex and non-convex settings. The thesis establishes tight upper and lower
bounds in several regimes for various local update algorithms and characterizes
the min-max complexity of multiple problem classes. At its core is a
fine-grained consensus-error-based analysis framework that yields sharper
finite-time convergence bounds under third-order smoothness and relaxed
heterogeneity assumptions. The thesis also extends to online federated
learning, providing fundamental regret bounds under both first-order and bandit
feedback. Together, these results clarify when and why local updates offer
provable advantages, and the thesis serves as a self-contained guide for
analyzing Local SGD in heterogeneous environments.

</details>


### [150] [Novel RL approach for efficient Elevator Group Control Systems](https://arxiv.org/abs/2507.00011)
*Nathan Vaartjes,Vincent Francois-Lavet*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的电梯群控系统，通过创新的动作空间编码和奖励信号设计，显著提升了电梯调度的效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 大型建筑中电梯交通管理的高效性对减少乘客等待时间和能耗至关重要，传统基于启发式或模式检测的控制器难以应对调度的随机性和组合复杂性。

Method: 将电梯系统建模为马尔可夫决策过程，采用端到端强化学习方法，引入创新的动作空间编码、基础设施步骤和定制奖励信号，并探索基于Dueling Double Deep Q-learning的架构。

Result: 提出的强化学习电梯群控系统能够适应波动的交通模式，在高度随机环境中学习，性能优于传统基于规则的算法。

Conclusion: 研究表明，强化学习方法在电梯调度中具有显著优势，能够有效应对复杂性和随机性，为实际应用提供了新思路。

Abstract: Efficient elevator traffic management in large buildings is critical for
minimizing passenger travel times and energy consumption. Because heuristic- or
pattern-detection-based controllers struggle with the stochastic and
combinatorial nature of dispatching, we model the six-elevator, fifteen-floor
system at Vrije Universiteit Amsterdam as a Markov Decision Process and train
an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).
Key innovations include a novel action space encoding to handle the
combinatorial complexity of elevator dispatching, the introduction of
infra-steps to model continuous passenger arrivals, and a tailored reward
signal to improve learning efficiency. In addition, we explore various ways to
adapt the discounting factor to the infra-step formulation. We investigate RL
architectures based on Dueling Double Deep Q-learning, showing that the
proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a
highly stochastic environment, and thereby outperforms a traditional rule-based
algorithm.

</details>


### [151] [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
*Zehao Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为GLU Attention的新型注意力机制，通过引入非线性提升注意力值，实验表明其能提高模型性能和收敛速度，且无需额外参数和计算成本。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过引入非线性改进注意力机制，以提升模型性能和收敛速度。

Method: 提出GLU Attention机制，将非线性引入注意力值，并与现有技术（如Flash Attention、RoPE等）无缝集成。

Result: 实验证明GLU Attention在文本和视觉任务中均能提升性能和收敛速度，且无额外开销。

Conclusion: GLU Attention是一种轻量级、高效的注意力机制改进方法，具有广泛适用性。

Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.

</details>


### [152] [Towards Undistillable Models by Minimizing Conditional Mutual Information](https://arxiv.org/abs/2507.00012)
*Linfeng Ye,Shayan Mohajer Hamidi,En-hui Yang*

Main category: cs.LG

TL;DR: 提出了一种名为CMIM的新训练方法，通过最小化交叉熵损失和条件互信息（CMI）值，构建不可蒸馏的深度神经网络（DNN），以保护知识产权。


<details>
  <summary>Details</summary>
Motivation: 为了保护DNN的知识产权，需要构建不可蒸馏的DNN，即通过知识蒸馏（KD）无法复现其性能的模型。

Method: 提出CMIM方法，通过联合最小化交叉熵损失和温度缩放集群的CMI值，训练DNN。

Result: 实验表明，CMIM模型对所有测试的KD方法均不可蒸馏，且其预测性能优于仅使用交叉熵损失的模型。

Conclusion: CMIM方法成功构建了不可蒸馏的DNN，同时提升了模型自身的预测性能。

Abstract: A deep neural network (DNN) is said to be undistillable if, when used as a
black-box input-output teacher, it cannot be distilled through knowledge
distillation (KD). In this case, the distilled student (referred to as the
knockoff student) does not outperform a student trained independently with
label smoothing (LS student) in terms of prediction accuracy. To protect
intellectual property of DNNs, it is desirable to build undistillable DNNs. To
this end, it is first observed that an undistillable DNN may have the trait
that each cluster of its output probability distributions in response to all
sample instances with the same label should be highly concentrated to the
extent that each cluster corresponding to each label should ideally collapse
into one probability distribution. Based on this observation and by measuring
the concentration of each cluster in terms of conditional mutual information
(CMI), a new training method called CMI minimized (CMIM) method is proposed,
which trains a DNN by jointly minimizing the conventional cross entropy (CE)
loss and the CMI values of all temperature scaled clusters across the entire
temperature spectrum. The resulting CMIM model is shown, by extensive
experiments, to be undistillable by all tested KD methods existing in the
literature. That is, the knockoff students distilled by these KD methods from
the CMIM model underperform the respective LS students. In addition, the CMIM
model is also shown to performs better than the model trained with the CE loss
alone in terms of their own prediction accuracy.

</details>


### [153] [ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting](https://arxiv.org/abs/2507.00013)
*Hyunwoo Seo,Chiehyeon Lim*

Main category: cs.LG

TL;DR: ST-MTM是一种结合季节性-趋势分解的掩码时间序列建模框架，通过分解方法解决复杂时间序列中的纠缠模式，提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 复杂时间序列预测具有挑战性，现有掩码建模方法忽略了时间序列的固有语义结构，可能导致学习到虚假模式。

Method: 提出ST-MTM框架，采用季节性-趋势分解，设计针对季节性和趋势成分的新型掩码策略，并结合对比学习任务增强上下文一致性。

Result: 实验表明，ST-MTM在预测性能上优于现有掩码建模、对比学习和监督预测方法。

Conclusion: ST-MTM通过分解和新型掩码策略有效捕捉时间序列的复杂模式，显著提升了预测效果。

Abstract: Forecasting complex time series is an important yet challenging problem that
involves various industrial applications. Recently, masked time-series modeling
has been proposed to effectively model temporal dependencies for forecasting by
reconstructing masked segments from unmasked ones. However, since the semantic
information in time series is involved in intricate temporal variations
generated by multiple time series components, simply masking a raw time series
ignores the inherent semantic structure, which may cause MTM to learn spurious
temporal patterns present in the raw data. To capture distinct temporal
semantics, we show that masked modeling techniques should address entangled
patterns through a decomposition approach. Specifically, we propose ST-MTM, a
masked time-series modeling framework with seasonal-trend decomposition, which
includes a novel masking method for the seasonal-trend components that
incorporates different temporal variations from each component. ST-MTM uses a
period masking strategy for seasonal components to produce multiple masked
seasonal series based on inherent multi-periodicity and a sub-series masking
strategy for trend components to mask temporal regions that share similar
variations. The proposed masking method presents an effective pre-training task
for learning intricate temporal variations and dependencies. Additionally,
ST-MTM introduces a contrastive learning task to support masked modeling by
enhancing contextual consistency among multiple masked seasonal
representations. Experimental results show that our proposed ST-MTM achieves
consistently superior forecasting performance compared to existing masked
modeling, contrastive learning, and supervised forecasting methods.

</details>


### [154] [SWE-Bench-CL: Continual Learning for Coding Agents](https://arxiv.org/abs/2507.00014)
*Thomas Joshi,Shayan Chowdhury,Fatih Uysal*

Main category: cs.LG

TL;DR: SWE-Bench-CL是一个新的持续学习基准，基于GitHub问题的时间序列，评估AI代理在软件工程中的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在静态代码生成任务上表现优异，但缺乏对持续演化的软件开发场景的评估能力。

Method: 通过组织GitHub问题为时间序列，结合LangGraph框架和FAISS语义记忆模块，设计了一套持续学习指标和实验协议。

Result: 提出了SWE-Bench-CL数据集和评估框架，并公开了代码和数据。

Conclusion: 该工作为开发适应性更强的AI代理提供了可复现的平台。

Abstract: Large Language Models (LLMs) have achieved impressive results on static
code-generation benchmarks, but real-world software development unfolds as a
continuous stream of evolving issues, fixes, and feature requests. We introduce
SWE-Bench-CL, a novel continual learning benchmark built on the human-verified
SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By
organizing GitHub issues into chronologically ordered sequences that reflect
natural repository evolution, SWE-Bench-CL enables direct evaluation of an
agent's ability to accumulate experience, transfer knowledge across tasks, and
resist catastrophic forgetting. We complement the dataset with (i) a
preliminary analysis of inter-task structural similarity and contextual
sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented
with a FAISS-backed semantic memory module, and (iii) a suite of specialized
continual learning metrics -- including average accuracy, forgetting,
forward/backward transfer, tool-use efficiency, and a generalized Composite
Continual Learning Score and CL-F-beta score -- to capture the
stability-plasticity trade-off. We outline a rigorous experimental protocol
comparing memory-enabled and memory-disabled agents across diverse Python
repositories. All code and data are publicly available at
https://github.com/thomasjoshi/agents-never-forget, providing the community
with a reproducible platform for developing more adaptive and robust AI agents
in software engineering.

</details>


### [155] [Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications](https://arxiv.org/abs/2507.00015)
*Lu Zhang,Sangarapillai Lambotharan,Gan Zheng,Guisheng Liao,Xuekang Liu,Fabio Roli,Carsten Maple*

Main category: cs.LG

TL;DR: 提出了一种新型视觉变换器（ViT）架构，通过引入对抗指示器（AdvI）标记来检测对抗攻击，结合对抗训练和检测机制，提升系统防御能力。


<details>
  <summary>Details</summary>
Motivation: 变换器在自动调制分类中的应用易受对抗攻击，需开发防御策略。

Method: 提出AdvI标记的ViT架构，结合对抗训练和检测机制，统一训练和运行时防御。

Result: 实验表明，该方法在白盒攻击场景中优于多种竞争方法。

Conclusion: AdvI标记在ViT中有效检测对抗攻击，简化系统架构并提升防御能力。

Abstract: The remarkable success of transformers across various fields such as natural
language processing and computer vision has paved the way for their
applications in automatic modulation classification, a critical component in
the communication systems of Internet of Things (IoT) devices. However, it has
been observed that transformer-based classification of radio signals is
susceptible to subtle yet sophisticated adversarial attacks. To address this
issue, we have developed a defensive strategy for transformer-based modulation
classification systems to counter such adversarial attacks. In this paper, we
propose a novel vision transformer (ViT) architecture by introducing a new
concept known as adversarial indicator (AdvI) token to detect adversarial
attacks. To the best of our knowledge, this is the first work to propose an
AdvI token in ViT to defend against adversarial attacks. Integrating an
adversarial training method with a detection mechanism using AdvI token, we
combine a training time defense and running time defense in a unified neural
network model, which reduces architectural complexity of the system compared to
detecting adversarial perturbations using separate models. We investigate into
the operational principles of our method by examining the attention mechanism.
We show the proposed AdvI token acts as a crucial element within the ViT,
influencing attention weights and thereby highlighting regions or features in
the input data that are potentially suspicious or anomalous. Through
experimental results, we demonstrate that our approach surpasses several
competitive methods in handling white-box attack scenarios, including those
utilizing the fast gradient method, projected gradient descent attacks and
basic iterative method.

</details>


### [156] [Graph Neural Networks in Wind Power Forecasting](https://arxiv.org/abs/2507.00105)
*Javier Castellano,Ignacio Villanueva*

Main category: cs.LG

TL;DR: GNNs在风能预测中表现与CNN相当，基于五年历史数据和NWP变量。


<details>
  <summary>Details</summary>
Motivation: 探索图神经网络（GNNs）在风能预测中的适用性。

Method: 使用三种风电场五年历史数据和NWP变量，评估24至36小时预测性能。

Result: 某些GNN架构性能与最佳CNN基准相当。

Conclusion: GNNs在风能预测中具有潜力，性能接近CNN。

Abstract: We study the applicability of GNNs to the problem of wind energy forecasting.
We find that certain architectures achieve performance comparable to our best
CNN-based benchmark. The study is conducted on three wind power facilities
using five years of historical data. Numerical Weather Prediction (NWP)
variables were used as predictors, and models were evaluated on a 24 to 36 hour
ahead test horizon.

</details>


### [157] [Gradient-based Fine-Tuning through Pre-trained Model Regularization](https://arxiv.org/abs/2507.00016)
*Xuanbo Liu,Liu Liu,Fuxiang Wu,Fusheng Hao,Xianglong Liu*

Main category: cs.LG

TL;DR: 提出了一种高效的基于梯度和正则化的微调方法（GRFT），通过更新权重矩阵的行或列来减少存储开销并提高参数选择效率，同时结合正则化提升知识迁移效果。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型在微调时需要大量计算资源和存储，现有方法如GPS虽减少训练参数但增加资源需求。

Method: GRFT方法通过更新权重矩阵中梯度平方和最高的行或列，结合正则化优化微调过程。

Result: GRFT在FGVC和VTAB数据集上仅需更新1.22%和0.30%的参数，性能优于GPS、Adapter Tuning和LoRA。

Conclusion: GRFT在高效性和性能上均优于现有方法，是一种资源友好的微调策略。

Abstract: Large pre-trained models have demonstrated extensive applications across
various fields. However, fine-tuning these models for specific downstream tasks
demands significant computational resources and storage. One fine-tuning
method, gradient-based parameter selection (GPS), focuses on fine-tuning only
the parameters with high gradients in each neuron, thereby reducing the number
of training parameters. Nevertheless, this approach increases computational
resource requirements and storage demands. In this paper, we propose an
efficient gradient-based and regularized fine-tuning method (GRFT) that updates
the rows or columns of the weight matrix. We theoretically demonstrate that the
rows or columns with the highest sum of squared gradients are optimal for
updating. This strategy effectively reduces storage overhead and improves the
efficiency of parameter selection. Additionally, we incorporate regularization
to enhance knowledge transfer from the pre-trained model. GRFT achieves
state-of-the-art performance, surpassing existing methods such as GPS, Adapter
Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the
total parameters on FGVC and VTAB datasets, respectively, demonstrating its
high efficiency and effectiveness. The source code will be released soon.

</details>


### [158] [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
*Bo Wang,Qinyuan Cheng,Runyu Peng,Rong Bao,Peiji Li,Qipeng Guo,Linyang Li,Zhiyuan Zeng,Yunhua Zhou,Xipeng Qiu*

Main category: cs.LG

TL;DR: 本文提出了一个统一的理论框架，将监督微调（SFT）和偏好学习（如DPO）联系起来，揭示了SFT是隐式奖励学习的特例，并提出了一种学习率调整方法以提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过理论框架统一SFT和偏好学习，以优化语言模型的后训练过程。

Method: 通过数学推导证明SFT和偏好学习在同一最优策略-奖励子空间中，并提出学习率调整和基于f-散度的SFT目标。

Result: 提出的方法在指令跟随任务中实现了25%的相对增益和6%的绝对胜率提升。

Conclusion: 理论框架和优化方法显著提升了后训练模型的性能，并扩展了LLM的理论基础。

Abstract: Post-training processes are essential phases in grounding pre-trained
language models to real-world tasks, with learning from demonstrations or
preference signals playing a crucial role in this adaptation. We present a
unified theoretical framework bridging Supervised Fine-Tuning (SFT) and
preference learning in Large Language Model (LLM) post-training. Through
rigorous mathematical derivation, we demonstrate that both SFT and preference
learning methods like Direct Preference Optimization (DPO) operate within the
same optimal policy-reward subspace, with SFT representing a special case of
implicit reward learning. Our analysis reveals a critical limitation in
conventional SFT: the KL divergence term in distribution matching becomes
constant with respect to the policy during optimization, failing to constrain
model updates. To address this, we propose a simple yet effective learning rate
reduction approach that yields significant performance improvements (up to
\textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in
instruction following tasks. Additionally, we derive alternative SFT objectives
from various f-divergence functions that preserve the KL term during
optimization, further enhancing post-DPO model performance. Finally, we extend
the theoretical relationship between LLM logits and Q-functions from preference
learning to the SFT context, providing mathematical derivations and
experimental validation.

</details>


### [159] [Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems](https://arxiv.org/abs/2507.00358)
*Yilie Huang,Xun Yu Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种自适应探索机制，用于连续时间随机线性二次控制问题，通过动态调整熵正则化和策略方差，提高了学习效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统固定探索机制需要大量调参且忽略学习进度的问题，提升模型自由方法的学习效率。

Method: 提出模型自由、数据驱动的探索机制，动态调整熵正则化和策略方差。

Result: 实现了与最佳已知模型自由结果相匹配的次线性遗憾界，并通过实验验证了自适应探索的优越性。

Conclusion: 自适应探索机制显著提升了学习效率和遗憾性能，优于非自适应和基于模型的方法。

Abstract: We study reinforcement learning (RL) for the same class of continuous-time
stochastic linear--quadratic (LQ) control problems as in
\cite{huang2024sublinear}, where volatilities depend on both states and
controls while states are scalar-valued and running control rewards are absent.
We propose a model-free, data-driven exploration mechanism that adaptively
adjusts entropy regularization by the critic and policy variance by the actor.
Unlike the constant or deterministic exploration schedules employed in
\cite{huang2024sublinear}, which require extensive tuning for implementations
and ignore learning progresses during iterations, our adaptive exploratory
approach boosts learning efficiency with minimal tuning. Despite its
flexibility, our method achieves a sublinear regret bound that matches the
best-known model-free results for this class of LQ problems, which were
previously derived only with fixed exploration schedules. Numerical experiments
demonstrate that adaptive explorations accelerate convergence and improve
regret performance compared to the non-adaptive model-free and model-based
counterparts.

</details>


### [160] [Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations](https://arxiv.org/abs/2507.00019)
*Minati Rath,Hema Date*

Main category: cs.LG

TL;DR: 本研究提出并比较了三种量子启发的数据编码策略（ILS、GDS、CCVS），旨在降低编码时间并分析其对分类性能的影响。


<details>
  <summary>Details</summary>
Motivation: 目标是减少高编码时间，同时确保正确的编码值，并分析其对分类性能的影响。

Method: 提出了三种编码策略：实例级策略（ILS）、全局离散策略（GDS）和类条件值策略（CCVS），分别独立处理数据行、全局映射特征值到量子态，以及按类编码特征值。

Result: 通过分类任务评估了编码效率、正确性、模型准确性和计算成本，分析了编码时间、精度和预测性能之间的权衡。

Conclusion: 研究为优化量子启发的数据转换提供了见解，适用于经典机器学习工作流。

Abstract: In this study, we propose, evaluate and compare three quantum inspired data
encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy
(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical
data into quantum data for use in pure classical machine learning models. The
primary objective is to reduce high encoding time while ensuring correct
encoding values and analyzing their impact on classification performance. The
Instance Level Strategy treats each row of dataset independently; mimics local
quantum states. Global Discrete Value Based encoding strategy maps all unique
feature values across the full dataset to quantum states uniformly. In
contrast, the Class conditional Value based encoding strategy encodes unique
values separately for each class, preserving class dependent information.
  We apply these encoding strategies to a classification task and assess their
impact on en-coding efficiency, correctness, model accuracy, and computational
cost. By analyzing the trade offs between encoding time, precision, and
predictive performance, this study provides insights into optimizing quantum
inspired data transformations for classical machine learning workflows.

</details>


### [161] [Neural Augmented Kalman Filters for Road Network assisted GNSS positioning](https://arxiv.org/abs/2507.00654)
*Hans van Gorp,Davide Belli,Amir Jalalirad,Bence Major*

Main category: cs.LG

TL;DR: 提出了一种结合道路网络数据和GNSS测量的深度学习方法，通过时间图神经网络（TGNN）提高城市环境中定位精度。


<details>
  <summary>Details</summary>
Motivation: 解决GNSS在密集城市环境中因多路径和非视距误差导致的定位精度下降问题。

Method: 训练TGNN预测正确道路段及其不确定性，并将其整合到卡尔曼滤波（KF）的测量更新步骤中。

Result: 在真实GNSS数据和开源道路网络上验证，定位误差减少29%。

Conclusion: 首次提出基于深度学习的联合道路网络和GNSS测量的定位方法，显著提升定位精度。

Abstract: The Global Navigation Satellite System (GNSS) provides critical positioning
information globally, but its accuracy in dense urban environments is often
compromised by multipath and non-line-of-sight errors. Road network data can be
used to reduce the impact of these errors and enhance the accuracy of a
positioning system. Previous works employing road network data are either
limited to offline applications, or rely on Kalman Filter (KF) heuristics with
little flexibility and robustness. We instead propose training a Temporal Graph
Neural Network (TGNN) to integrate road network information into a KF. The TGNN
is designed to predict the correct road segment and its associated uncertainty
to be used in the measurement update step of the KF. We validate our approach
with real-world GNSS data and open-source road networks, observing a 29%
decrease in positioning error for challenging scenarios compared to a GNSS-only
KF. To the best of our knowledge, ours is the first deep learning-based
approach jointly employing road network data and GNSS measurements to determine
the user position on Earth.

</details>


### [162] [Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods](https://arxiv.org/abs/2507.00020)
*Marcio Borges,Felipe Pereira,Michel Tosin*

Main category: cs.LG

TL;DR: 使用变分自编码器（VAE）改进马尔可夫链蒙特卡洛（McMC）方法，生成更广泛的先验提案，提升效率和适用性。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如KLE）需要已知协方差函数，而实际应用中往往缺乏此类信息。VAE提供数据驱动的方法，灵活捕捉相关结构。

Method: 采用VAE框架，应用于地下水流动反演问题，通过压力数据估计渗透率场。

Result: VAE在已知相关长度时与KLE精度相当，在假设相关长度偏离真实值时优于KLE，且显著降低随机维度。

Conclusion: 深度生成模型可提升McMC方法在高维问题中的适应性和效率。

Abstract: This study uses a Variational Autoencoder method to enhance the efficiency
and applicability of Markov Chain Monte Carlo (McMC) methods by generating
broader-spectrum prior proposals. Traditional approaches, such as the
Karhunen-Lo\`eve Expansion (KLE), require previous knowledge of the covariance
function, often unavailable in practical applications. The VAE framework
enables a data-driven approach to flexibly capture a broader range of
correlation structures in Bayesian inverse problems, particularly subsurface
flow modeling. The methodology is tested on a synthetic groundwater flow
inversion problem, where pressure data is used to estimate permeability fields.
Numerical experiments demonstrate that the VAE-based parameterization achieves
comparable accuracy to KLE when the correlation length is known and outperforms
KLE when the assumed correlation length deviates from the true value. Moreover,
the VAE approach significantly reduces stochastic dimensionality, improving
computational efficiency. The results suggest that leveraging deep generative
models in McMC methods can lead to more adaptable and efficient Bayesian
inference in high-dimensional problems.

</details>


### [163] [A Test-Function Approach to Incremental Stability](https://arxiv.org/abs/2507.00695)
*Daniel Pfrommer,Max Simchowitz,Ali Jadbabaie*

Main category: cs.LG

TL;DR: 论文提出了一种基于奖励作为“测试函数”的新框架，用于分析增量输入到状态稳定性（δISS），与传统控制理论中的Lyapunov函数方法不同。


<details>
  <summary>Details</summary>
Motivation: 传统控制理论使用Lyapunov函数证明稳定性，而强化学习（RL）中的价值函数基于非平滑且无界的奖励函数构建，无法直接作为Lyapunov证书。本文旨在建立RL价值函数与增量稳定性之间的新联系。

Method: 通过将增量输入到状态稳定性与RL价值函数的正则性联系起来，提出了一种新的等价关系，使用Hölder连续奖励函数进行对抗性选择。

Result: 证明了RL价值函数的正则性与增量稳定性之间存在独特的联系，不同于传统的Lyapunov方法。

Conclusion: 该研究为理解RL价值函数与系统稳定性之间的关系提供了新视角，拓展了控制理论的应用范围。

Abstract: This paper presents a novel framework for analyzing
Incremental-Input-to-State Stability ($\delta$ISS) based on the idea of using
rewards as "test functions." Whereas control theory traditionally deals with
Lyapunov functions that satisfy a time-decrease condition, reinforcement
learning (RL) value functions are constructed by exponentially decaying a
Lipschitz reward function that may be non-smooth and unbounded on both sides.
Thus, these RL-style value functions cannot be directly understood as Lyapunov
certificates. We develop a new equivalence between a variant of incremental
input-to-state stability of a closed-loop system under given a policy, and the
regularity of RL-style value functions under adversarial selection of a
H\"older-continuous reward function. This result highlights that the regularity
of value functions, and their connection to incremental stability, can be
understood in a way that is distinct from the traditional Lyapunov-based
approach to certifying stability in control theory.

</details>


### [164] [AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity](https://arxiv.org/abs/2507.00024)
*Yeyong Yu,Xilei Bian,Jie Xiong,Xing Wu,Quan Qian*

Main category: cs.LG

TL;DR: AIMatDesign是一种强化学习框架，通过结合实验数据和领域专家知识，解决了高维材料设计中的预测偏差和知识整合问题，显著提升了材料发现的效率和成功率。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在高维材料设计中存在预测偏差和无法有效整合专家知识的问题，限制了材料逆向设计的效率和可靠性。

Method: AIMatDesign采用强化学习框架，结合差分算法增强实验数据，利用大语言模型动态修正预测不一致，并通过知识驱动的奖励函数提升训练稳定性。

Result: 实验表明，AIMatDesign在发现效率、收敛速度和成功率上优于传统方法，成功合成了高性能Zr基合金。

Conclusion: AIMatDesign展示了在闭环材料发现中的可靠性和潜力，为高维材料设计提供了新思路。

Abstract: With the growing demand for novel materials, machine learning-driven inverse
design methods face significant challenges in reconciling the high-dimensional
materials composition space with limited experimental data. Existing approaches
suffer from two major limitations: (I) machine learning models often lack
reliability in high-dimensional spaces, leading to prediction biases during the
design process; (II) these models fail to effectively incorporate domain expert
knowledge, limiting their capacity to support knowledge-guided inverse design.
To address these challenges, we introduce AIMatDesign, a reinforcement learning
framework that addresses these limitations by augmenting experimental data
using difference-based algorithms to build a trusted experience pool,
accelerating model convergence. To enhance model reliability, an automated
refinement strategy guided by large language models (LLMs) dynamically corrects
prediction inconsistencies, reinforcing alignment between reward signals and
state value functions. Additionally, a knowledge-based reward function
leverages expert domain rules to improve stability and efficiency during
training. Our experiments demonstrate that AIMatDesign significantly surpasses
traditional machine learning and reinforcement learning methods in discovery
efficiency, convergence speed, and success rates. Among the numerous candidates
proposed by AIMatDesign, experimental synthesis of representative Zr-based
alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\%
elongation, closely matching predictions. Moreover, the framework accurately
captured the trend of yield strength variation with composition, demonstrating
its reliability and potential for closed-loop materials discovery.

</details>


### [165] [Generalizing to New Dynamical Systems via Frequency Domain Adaptation](https://arxiv.org/abs/2507.00025)
*Tiexin Qin,Hong Yan,Haoliang Li*

Main category: cs.LG

TL;DR: FNSDA提出了一种参数高效的方法，通过傅里叶空间适应实现对新动态的泛化。


<details>
  <summary>Details</summary>
Motivation: 当前方法在特定领域内预测可靠性和泛化能力受限，难以适应环境特征不同的系统。

Method: FNSDA基于已知环境通过傅里叶模式自动分区识别共享动态，并通过低维潜在系统参数调整特定模式。

Result: 在四种动态系统上，FNSDA以显著减少的参数成本实现了优于或竞争性的泛化性能。

Conclusion: FNSDA为复杂动态系统的泛化提供了一种高效且参数节约的解决方案。

Abstract: Learning the underlying dynamics from data with deep neural networks has
shown remarkable potential in modeling various complex physical dynamics.
However, current approaches are constrained in their ability to make reliable
predictions in a specific domain and struggle with generalizing to unseen
systems that are governed by the same general dynamics but differ in
environmental characteristics. In this work, we formulate a parameter-efficient
method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can
readily generalize to new dynamics via adaptation in the Fourier space.
Specifically, FNSDA identifies the shareable dynamics based on the known
environments using an automatic partition in Fourier modes and learns to adjust
the modes specific for each new environment by conditioning on low-dimensional
latent systematic parameters for efficient generalization. We evaluate our
approach on four representative families of dynamic systems, and the results
show that FNSDA can achieve superior or competitive generalization performance
compared to existing methods with a significantly reduced parameter cost. Our
code is available at https://github.com/WonderSeven/FNSDA.

</details>


### [166] [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
*Jiale Ding,Xiang Zheng,Cong Wang,Wei-Bin Lee,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.LG

TL;DR: 论文提出了ROSE框架，通过多目标强化学习生成多样化和情境丰富的对抗性提示，以更有效地评估大型语言模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在现实应用中的广泛部署，评估其安全性（尤其是在对抗性提示下的表现）变得至关重要。现有手动基准的静态性和更新成本高，难以适应快速发展的LLMs。

Method: 提出ROSE框架，利用多目标强化学习微调对抗性LLM，生成多样化和情境丰富的对抗性提示。

Result: 实验表明，ROSE在揭示先进LLMs的安全漏洞方面优于现有方法，综合评估指标显著提升。

Conclusion: ROSE为更实用和面向现实的LLM安全评估迈出了一步。

Abstract: As Large Language Models (LLMs) are increasingly deployed as black-box
components in real-world applications, evaluating their safety-especially under
adversarial prompting-has become critical. Arguably, effective safety
evaluations should be adaptive, evolving with LLM capabilities, and also cover
a broad spectrum of harmful topics and real-world scenarios to fully expose
potential vulnerabilities. Existing manual safety benchmarks, built on
handcrafted adversarial prompts, are limited by their static nature and the
intensive labor required to update them, making it difficult to keep pace with
rapidly advancing LLMs. In contrast, automated adversarial prompt generation
offers a promising path toward adaptive evaluation. However, current methods
often suffer from insufficient adversarial topic coverage (topic-level
diversity) and weak alignment with real-world contexts. These shortcomings stem
from the exploration-exploitation dilemma in black-box optimization and a lack
of real-world contextualization, resulting in adversarial prompts that are both
topically narrow and scenario-repetitive. To address these issues, we propose
Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses
multi-objective reinforcement learning to fine-tune an adversarial LLM for
generating topically diverse and contextually rich adversarial prompts.
Experiments show that ROSE outperforms existing methods in uncovering safety
vulnerabilities in state-of-the-art LLMs, with notable improvements in
integrated evaluation metrics. We hope ROSE represents a step toward more
practical and reality-oriented safety evaluation of LLMs. WARNING: This paper
contains examples of potentially harmful text.

</details>


### [167] [HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation](https://arxiv.org/abs/2507.00028)
*Lihuan Li,Hao Xue,Shuang Ao,Yang Song,Flora Salim*

Main category: cs.LG

TL;DR: HiT-JEPA提出了一种分层轨迹表示框架，结合细粒度细节与高层语义，提升多尺度轨迹分析能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时捕捉轨迹的细粒度细节和高层语义，限制了轨迹分析的全面性。

Method: HiT-JEPA采用三层分层结构，逐步捕获点级细节、中间模式和高层抽象，整合局部动态与全局语义。

Result: 实验表明，HiT-JEPA的分层设计在多尺度轨迹相似性计算中表现优越。

Conclusion: HiT-JEPA为多尺度轨迹表示提供了统一框架，显著提升了轨迹分析的全面性和准确性。

Abstract: The representation of urban trajectory data plays a critical role in
effectively analyzing spatial movement patterns. Despite considerable progress,
the challenge of designing trajectory representations that can capture diverse
and complementary information remains an open research problem. Existing
methods struggle in incorporating trajectory fine-grained details and
high-level summary in a single model, limiting their ability to attend to both
long-term dependencies while preserving local nuances. To address this, we
propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint
Embedding Predictive Architecture), a unified framework for learning
multi-scale urban trajectory representations across semantic abstraction
levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures
point-level fine-grained details, intermediate patterns, and high-level
trajectory abstractions, enabling the model to integrate both local dynamics
and global semantics in one coherent structure. Extensive experiments on
multiple real-world datasets for trajectory similarity computation show that
HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code
is available at: https://anonymous.4open.science/r/HiT-JEPA.

</details>


### [168] [Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation](https://arxiv.org/abs/2507.00055)
*Varsha Pendyala,Pedro Morgado,William Sethares*

Main category: cs.LG

TL;DR: 论文提出了一种名为LiSER的轻量级知识蒸馏框架，利用未标记的音频-视觉数据和大型教师模型来减少对标记数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别（SER）在多模态人机交互中很重要，但标记数据收集成本高。

Method: LiSER通过知识蒸馏，利用大型教师模型（基于先进的语音和面部表示模型）向轻量级学生模型传递情感和表情知识。

Result: 在RAVDESS和CREMA-D数据集上的实验表明，LiSER能有效减少对标记数据的依赖。

Conclusion: LiSER为SER任务提供了一种高效且成本较低的解决方案。

Abstract: Voice interfaces integral to the human-computer interaction systems can
benefit from speech emotion recognition (SER) to customize responses based on
user emotions. Since humans convey emotions through multi-modal audio-visual
cues, developing SER systems using both the modalities is beneficial. However,
collecting a vast amount of labeled data for their development is expensive.
This paper proposes a knowledge distillation framework called LightweightSER
(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher
models built on advanced speech and face representation models. LiSER transfers
knowledge regarding speech emotions and facial expressions from the teacher
models to lightweight student models. Experiments conducted on two benchmark
datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence
on extensive labeled datasets for SER tasks.

</details>


### [169] [LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing](https://arxiv.org/abs/2507.00029)
*Wenbing Li,Zikai Song,Hang Zhou,Yunyao Zhang,Junqing Yu,Wei Yang*

Main category: cs.LG

TL;DR: LoRA-Mixer是一个轻量级的MoE框架，通过动态路由任务特定的LoRA专家，提升参数效率和任务保真度，在多个基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在结合LoRA和MoE时存在参数效率低和任务保真度不足的问题，需要一种更高效的解决方案。

Method: 提出LoRA-Mixer框架，替换注意力模块的输入/输出线性层为动态路由的LoRA专家，支持联合优化或直接部署预训练模块，并引入自适应Specialization Balance Loss优化路由。

Result: 在GSM8K、HumanEval和MedQA等数据集上，LoRA-Mixer分别提升了7.61%、4.88%和3.08%，且仅使用48%的参数。

Conclusion: LoRA-Mixer在高效性和性能上均优于现有方法，展示了其在多任务适应中的潜力。

Abstract: Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts
(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit
prevailing limitations: they either swap entire attention/feed-forward layers
for switch experts or bolt on parallel expert branches, diluting parameter
efficiency and task fidelity. We propose the LoRA-Mixer, a modular and
lightweight MoE framework that integrates LoRA experts. Our core innovation
lies in replacing the projection matrices of the attention module's
input/output linear layers with dynamically routed, task-specific LoRA experts.
This design ensures seamless compatibility with diverse foundation models,
including transformers and state space models (SSMs), by leveraging their
inherent linear projection structures. The framework supports two operational
paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a
novel hard-soft routing strategy, or (2) direct deployment of pre-trained,
frozen LoRA modules sourced from external repositories. To enable robust router
training with limited data while ensuring stable routing decisions and
maximizing expert reuse, we introduce an adaptive Specialization Balance Loss
(SBL) that jointly optimizes expert balance and task-specific alignment.
Extensive experiments on seven benchmark datasets, including MedQA, CoLA,
SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of
LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer
achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base
models, respectively. Compared with state-of-the-art methods, LoRA-Mixer
achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,
using only 48% of the parameters, demonstrating its efficiency and strong
performance.

</details>


### [170] [Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.00030)
*Abhishek Verma,Nallarasan V,Balaraman Ravindran*

Main category: cs.LG

TL;DR: 提出了一种结合上下文赌博机与深度强化学习的新范式，通过自适应选择动作持续时间提升策略灵活性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习在复杂序列决策任务中表现优异，但动作执行的时间尺度问题尚未充分探索。

Method: 在深度Q网络中集成上下文赌博机模块，根据状态上下文选择最优动作重复率。

Result: 在Atari 2600游戏中，相比静态持续时间基线，性能显著提升。

Conclusion: 该范式为实时应用（如游戏和机器人）提供了可扩展的动态动作持续时间解决方案。

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in complex
sequential decision-making tasks, such as playing Atari 2600 games and
mastering board games. A critical yet underexplored aspect of DRL is the
temporal scale of action execution. We propose a novel paradigm that integrates
contextual bandits with DRL to adaptively select action durations, enhancing
policy flexibility and computational efficiency. Our approach augments a Deep
Q-Network (DQN) with a contextual bandit module that learns to choose optimal
action repetition rates based on state contexts. Experiments on Atari 2600
games demonstrate significant performance improvements over static duration
baselines, highlighting the efficacy of adaptive temporal abstractions in DRL.
This paradigm offers a scalable solution for real-time applications like gaming
and robotics, where dynamic action durations are critical.

</details>


### [171] [Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru](https://arxiv.org/abs/2507.00031)
*Chuan Li,Jiang You,Hassine Moungla,Vincent Gauthier,Miguel Nunez-del-Prado,Hugo Alatrista-Salas*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级的空间邻域融合（SPN）技术，用于解决稀疏空间数据对预测模型的影响，显著提升了流行病期间城市区域流动性预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 准确建模人类流动性对理解流行病传播和及时干预至关重要，但稀疏的空间数据限制了传统时间序列模型的预测能力。

Method: 提出SPN技术，通过聚合邻近六边形网格单元的信号来增强每个单元的特征，并在三种预测模型（NLinear、PatchTST、K-U-Net）上验证其效果。

Result: SPN显著提升了预测性能，测试MSE最多降低9.85%。

Conclusion: 空间平滑稀疏流动性信号是一种简单有效的时空预测方法，适用于公共卫生危机期间的预测需求。

Abstract: Accurate modeling of human mobility is critical for understanding epidemic
spread and deploying timely interventions. In this work, we leverage a
large-scale spatio-temporal dataset collected from Peru's national Digital
Contact Tracing (DCT) application during the COVID-19 pandemic to forecast
mobility flows across urban regions. A key challenge lies in the spatial
sparsity of hourly mobility counts across hexagonal grid cells, which limits
the predictive power of conventional time series models. To address this, we
propose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)
technique that augments each cell's features with aggregated signals from its
immediate H3 neighbors. We evaluate this strategy on three forecasting
backbones: NLinear, PatchTST, and K-U-Net, under various historical input
lengths. Experimental results show that SPN consistently improves forecasting
performance, achieving up to 9.85 percent reduction in test MSE. Our findings
demonstrate that spatial smoothing of sparse mobility signals provides a simple
yet effective path toward robust spatio-temporal forecasting during public
health crises.

</details>


### [172] [Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark](https://arxiv.org/abs/2507.00034)
*Reece Bourisaw,Reid McCants,Jean-Marie Le Corre,Anna Iskhakova,Arsen S. Iskhakov*

Main category: cs.LG

TL;DR: 本文为OECD/NEA AI/ML CHF基准测试的第二阶段提供了涵盖均匀和非均匀轴向加热条件的CHF数据集，并验证了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 临界热通量（CHF）是轻水反应堆安全运行的关键参数，现有方法在非均匀加热条件下表现不佳，需要改进。

Method: 通过提取技术报告中的加热曲线，进行插值和能量平衡验证，生成机器可读格式的数据集，并评估经典和现代预测方法的性能。

Result: 经典CHF相关性在非均匀加热条件下误差显著，神经网络在均匀数据上表现良好但无法推广到非均匀场景。

Conclusion: 研究为下一阶段的基准测试提供了数据集和基线结果，支持转移学习、不确定性量化和设计优化。

Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water
reactors, defining safe thermal-hydraulic operating limits. To support Phase II
of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power
profiles, this work compiles and digitizes a broad CHF dataset covering both
uniform and non-uniform axial heating conditions. Heating profiles were
extracted from technical reports, interpolated onto a consistent axial mesh,
validated via energy-balance checks, and encoded in machine-readable formats
for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating
and degrade markedly when applied to non-uniform profiles, while modern tabular
methods offer improved but still imperfect predictions. A neural network
trained solely on uniform data performs well in that regime but fails to
generalize to spatially varying scenarios, underscoring the need for models
that explicitly incorporate axial power distributions. By providing these
curated datasets and baseline modeling results, this study lays the groundwork
for advanced transfer-learning strategies, rigorous uncertainty quantification,
and design-optimization efforts in the next phase of the CHF benchmark.

</details>


### [173] [IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting](https://arxiv.org/abs/2507.00036)
*Rohan Putatunda,Sanjay Purushotham,Ratnaksha Lele,Vandana P. Janeja*

Main category: cs.LG

TL;DR: 提出了一种混合模型IDRIFTNET，结合物理驱动和深度学习，用于预测冰山漂移轨迹，在有限数据和动态环境条件下表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 冰山漂移对气候系统和极地导航有重要影响，但现有模型因数据稀缺和非线性动态难以准确预测。

Method: IDRIFTNET结合冰山漂移物理的解析公式与增强残差学习模型，通过旋转增强谱神经网络捕捉全局和局部模式。

Result: IDRIFTNET在冰山A23A和B22A上表现优于其他模型，FDE和ADE更低。

Conclusion: IDRIFTNET能有效预测冰山漂移轨迹，适用于数据有限和动态环境条件。

Abstract: Drifting icebergs in the polar oceans play a key role in the Earth's climate
system, impacting freshwater fluxes into the ocean and regional ecosystems
while also posing a challenge to polar navigation. However, accurately
forecasting iceberg trajectories remains a formidable challenge, primarily due
to the scarcity of spatiotemporal data and the complex, nonlinear nature of
iceberg motion, which is also impacted by environmental variables. The iceberg
motion is influenced by multiple dynamic environmental factors, creating a
highly variable system that makes trajectory identification complex. These
limitations hinder the ability of deep learning models to effectively capture
the underlying dynamics and provide reliable predictive outcomes. To address
these challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep
learning model that combines an analytical formulation of iceberg drift
physics, with an augmented residual learning model. The model learns the
pattern of mismatch between the analytical solution and ground-truth
observations, which is combined with a rotate-augmented spectral neural network
that captures both global and local patterns from the data to forecast future
iceberg drift positions. We compare IDRIFTNET model performance with
state-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings
demonstrate that IDRIFTNET outperforms other models by achieving a lower Final
Displacement Error (FDE) and Average Displacement Error (ADE) across a variety
of time points. These results highlight IDRIFTNET's effectiveness in capturing
the complex, nonlinear drift of icebergs for forecasting iceberg trajectories
under limited data and dynamic environmental conditions.

</details>


### [174] [Model Fusion via Neuron Interpolation](https://arxiv.org/abs/2507.00037)
*Phoomraphee Luenam,Andreas Spanopoulos,Amit Sant,Thomas Hofmann,Sotiris Anagnostidis,Sidak Pal Singh*

Main category: cs.LG

TL;DR: 提出了一种新的基于神经元的模型融合算法，通过整合多个训练好的神经网络，有效解决因内部表示差异导致的融合难题。


<details>
  <summary>Details</summary>
Motivation: 模型融合因内部表示差异（如排列不变性、随机初始化或不同分布的训练数据）而复杂化，需一种更有效的方法。

Method: 通过神经元分组和神经元属性评分，设计了一种适用于任意层类型的融合算法。

Result: 在多个基准数据集上，新算法在零样本和非独立同分布场景中表现优于现有技术。

Conclusion: 该算法为模型融合提供了更通用的解决方案，代码已开源。

Abstract: Model fusion aims to combine the knowledge of multiple models by creating one
representative model that captures the strengths of all of its parents.
However, this process is non-trivial due to differences in internal
representations, which can stem from permutation invariance, random
initialization, or differently distributed training data. We present a novel,
neuron-centric family of model fusion algorithms designed to integrate multiple
trained neural networks into a single network effectively regardless of
training data distribution. Our algorithms group intermediate neurons of parent
models to create target representations that the fused model approximates with
its corresponding sub-network. Unlike prior approaches, our approach
incorporates neuron attribution scores into the fusion process. Furthermore,
our algorithms can generalize to arbitrary layer types. Experimental results on
various benchmark datasets demonstrate that our algorithms consistently
outperform previous fusion techniques, particularly in zero-shot and non-IID
fusion scenarios. The code is available at
https://github.com/AndrewSpano/neuron-interpolation-model-fusion.

</details>


### [175] [Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information](https://arxiv.org/abs/2507.00038)
*Fei Chen,Wenchi Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于点状V信息（PVI）的数据缩减策略，通过筛选高难度实例提升模型训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据集中如何选择最优实例以提升数据质量和训练效率的核心挑战。

Method: 使用PVI量化实例难度，静态筛选低难度实例，并采用渐进式学习训练分类器。

Result: 实验显示移除10%-30%数据仅损失0.0001%-0.76%准确率，渐进学习提升0.8%准确率。

Conclusion: PVI策略能有效提升模型性能和训练效率，并成功扩展到中文NLP任务。

Abstract: Data reduction plays a vital role in data-centric AI by identifying the most
informative instance within large-scale datasets to enhance model training
efficiency. The core challenge lies in how to select the optimal
instances-rather than the entire datasets-to improve data quality and training
efficiency. In this paper, we propose an effective data reduction strategy
based on Pointwise V-information(PVI). First, we quantify instance difficulty
using PVI and filter out low-difficulty instances enabling a static approach.
Experiments demonstrate that removing 10%-30% of the data preserves the
classifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we
use a progressive learning approach to training the classifiers on instances
sorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy
gain over conventional training. Our results suggest that with the effective
data reduction strategy, training a classifier on the selected optimal subset
could enhance the model performance and boost training efficiency. Moreover, we
have transferred the PVI framework, which previously applied only to English
datasets, to diverse Chinese NLP tasks and base models, leading to valuable
insights for cross-lingual data reduction and faster training. The codes are
released at https://github.com/zhouwenchi/DatasetReductionStrategy.

</details>


### [176] [Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing](https://arxiv.org/abs/2507.00039)
*Lucas Potin,Rosa Figueiredo,Vincent Labatut,Christine Largeron*

Main category: cs.LG

TL;DR: 论文对图分类中的38种质量度量进行了理论和实证比较，提出了基于聚类的预处理方法，并发现某些流行度量并非最优。


<details>
  <summary>Details</summary>
Motivation: 解决图分类中质量度量选择困难的问题，避免盲目使用流行度量。

Method: 理论分析四类数学性质，构建基准数据集和黄金标准排名，提出聚类预处理方法。

Result: 实验表明聚类预处理有效减少模式数量且性能相当，某些流行度量表现不佳。

Conclusion: 研究为图分类中的质量度量选择提供了指导，并展示了聚类预处理的优势。

Abstract: Graph classification aims to categorize graphs based on their structural and
attribute features, with applications in diverse fields such as social network
analysis and bioinformatics. Among the methods proposed to solve this task,
those relying on patterns (i.e. subgraphs) provide good explainability, as the
patterns used for classification can be directly interpreted. To identify
meaningful patterns, a standard approach is to use a quality measure, i.e. a
function that evaluates the discriminative power of each pattern. However, the
literature provides tens of such measures, making it difficult to select the
most appropriate for a given application. Only a handful of surveys try to
provide some insight by comparing these measures, and none of them specifically
focuses on graphs. This typically results in the systematic use of the most
widespread measures, without thorough evaluation. To address this issue, we
present a comparative analysis of 38 quality measures from the literature. We
characterize them theoretically, based on four mathematical properties. We
leverage publicly available datasets to constitute a benchmark, and propose a
method to elaborate a gold standard ranking of the patterns. We exploit these
resources to perform an empirical comparison of the measures, both in terms of
pattern ranking and classification performance. Moreover, we propose a
clustering-based preprocessing step, which groups patterns appearing in the
same graphs to enhance classification performance. Our experimental results
demonstrate the effectiveness of this step, reducing the number of patterns to
be processed while achieving comparable performance. Additionally, we show that
some popular measures widely used in the literature are not associated with the
best results.

</details>


### [177] [Residual Reward Models for Preference-based Reinforcement Learning](https://arxiv.org/abs/2507.00611)
*Chenyang Cao,Miguel Rogel-García,Mohamed Nabail,Xueqian Wang,Nicholas Rhinehart*

Main category: cs.LG

TL;DR: 提出了一种基于残差奖励模型（RRM）的方法，通过结合先验奖励和学习奖励，显著提升了偏好强化学习（PbRL）的性能和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 解决偏好强化学习中因奖励模型训练导致的收敛速度慢问题，并探索如何有效利用先验知识。

Method: 提出残差奖励模型（RRM），将环境真实奖励分为先验奖励和学习奖励两部分，分别处理已知和未知的奖励信息。

Result: 实验表明，RRM在Meta-World环境和真实机器人任务中均显著提升了性能，加快了策略学习速度。

Conclusion: RRM方法有效利用了先验知识，提升了PbRL的效率和性能，适用于多种任务和奖励类型。

Abstract: Preference-based Reinforcement Learning (PbRL) provides a way to learn
high-performance policies in environments where the reward signal is hard to
specify, avoiding heuristic and time-consuming reward design. However, PbRL can
suffer from slow convergence speed since it requires training in a reward
model. Prior work has proposed learning a reward model from demonstrations and
fine-tuning it using preferences. However, when the model is a neural network,
using different loss functions for pre-training and fine-tuning can pose
challenges to reliable optimization. In this paper, we propose a method to
effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM
assumes that the true reward of the environment can be split into a sum of two
parts: a prior reward and a learned reward. The prior reward is a term
available before training, for example, a user's ``best guess'' reward
function, or a reward function learned from inverse reinforcement learning
(IRL), and the learned reward is trained with preferences. We introduce
state-based and image-based versions of RRM and evaluate them on several tasks
in the Meta-World environment suite. Experimental results show that our method
substantially improves the performance of a common PbRL method. Our method
achieves performance improvements for a variety of different types of prior
rewards, including proxy rewards, a reward obtained from IRL, and even a
negated version of the proxy reward. We also conduct experiments with a Franka
Panda to show that our method leads to superior performance on a real robot. It
significantly accelerates policy learning for different tasks, achieving
success in fewer steps than the baseline. The videos are presented at
https://sunlighted.github.io/RRM-web/.

</details>


### [178] [Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data](https://arxiv.org/abs/2507.00061)
*Hoang-Dieu Vu,Duc-Nghia Tran,Quang-Tu Pham,Hieu H. Pham,Nicolas Vuillerme,Duc-Tan Tran*

Main category: cs.LG

TL;DR: Smooth-Distill是一种新颖的自蒸馏框架，用于同时进行人类活动识别（HAR）和传感器位置检测，通过统一的CNN架构MTL-net处理加速度计数据，并显著减少训练计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决传统蒸馏方法需要独立师生模型的问题，同时提升多任务学习的效率和性能。

Method: 使用MTL-net处理加速度计数据，通过平滑的历史模型作为教师模型，减少计算开销。

Result: 在多个评估场景中表现优于其他方法，提升了HAR和设备位置检测的准确性，并减少了过拟合。

Conclusion: Smooth-Distill为多任务学习提供了一种高效且准确的解决方案，尤其适用于资源受限的平台。

Abstract: This paper introduces Smooth-Distill, a novel self-distillation framework
designed to simultaneously perform human activity recognition (HAR) and sensor
placement detection using wearable sensor data. The proposed approach utilizes
a unified CNN-based architecture, MTL-net, which processes accelerometer data
and branches into two outputs for each respective task. Unlike conventional
distillation methods that require separate teacher and student models, the
proposed framework utilizes a smoothed, historical version of the model itself
as the teacher, significantly reducing training computational overhead while
maintaining performance benefits. To support this research, we developed a
comprehensive accelerometer-based dataset capturing 12 distinct sleep postures
across three different wearing positions, complementing two existing public
datasets (MHealth and WISDM). Experimental results show that Smooth-Distill
consistently outperforms alternative approaches across different evaluation
scenarios, achieving notable improvements in both human activity recognition
and device placement detection tasks. This method demonstrates enhanced
stability in convergence patterns during training and exhibits reduced
overfitting compared to traditional multitask learning baselines. This
framework contributes to the practical implementation of knowledge distillation
in human activity recognition systems, offering an effective solution for
multitask learning with accelerometer data that balances accuracy and training
efficiency. More broadly, it reduces the computational cost of model training,
which is critical for scenarios requiring frequent model updates or training on
resource-constrained platforms. The code and model are available at
https://github.com/Kuan2vn/smooth\_distill.

</details>


### [179] [Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding](https://arxiv.org/abs/2507.00669)
*Duc Cao-Dinh,Khai Le-Duc,Anh Dao,Bach Phan Tat,Chris Ngo,Duy M. H. Nguyen,Nguyen X. Khanh,Thanh Nguyen-Tang*

Main category: cs.LG

TL;DR: 论文提出Audio-3DVG框架，通过分解音频输入为对象提及检测和音频引导注意力模块，提升3D视觉定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位多基于文本描述，而音频输入的研究较少且具挑战性，结合语音识别和表示学习进展，探索音频与空间信息的融合。

Method: 提出Object Mention Detection多标签分类任务识别音频中提及的对象，并设计Audio-Guided Attention模块捕捉对象与语音线索的交互。

Result: 在ScanRefer、Sr3D和Nr3D数据集上，Audio-3DVG在音频定位中达到新SOTA，且与文本方法竞争。

Conclusion: Audio-3DVG展示了语音与3D视觉任务结合的潜力，为未来研究提供方向。

Abstract: 3D Visual Grounding (3DVG) involves localizing target objects in 3D point
clouds based on natural language. While prior work has made strides using
textual descriptions, leveraging spoken language-known as Audio-based 3D Visual
Grounding-remains underexplored and challenging. Motivated by advances in
automatic speech recognition (ASR) and speech representation learning, we
propose Audio-3DVG, a simple yet effective framework that integrates audio and
spatial information for enhanced grounding. Rather than treating speech as a
monolithic input, we decompose the task into two complementary components.
First, we introduce Object Mention Detection, a multi-label classification task
that explicitly identifies which objects are referred to in the audio, enabling
more structured audio-scene reasoning. Second, we propose an Audio-Guided
Attention module that captures interactions between candidate objects and
relational speech cues, improving target discrimination in cluttered scenes. To
support benchmarking, we synthesize audio descriptions for standard 3DVG
datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate
that Audio-3DVG not only achieves new state-of-the-art performance in
audio-based grounding, but also competes with text-based methods-highlighting
the promise of integrating spoken language into 3D vision tasks.

</details>


### [180] [Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory](https://arxiv.org/abs/2507.00073)
*Urvi Pawar,Kunal Telangi*

Main category: cs.LG

TL;DR: 提出了Fractional Policy Gradients (FPG)，一种结合分数微积分进行长期时间建模的强化学习框架，显著提升了样本效率和方差减少。


<details>
  <summary>Details</summary>
Motivation: 标准策略梯度方法受限于马尔可夫假设，存在高方差和采样效率低的问题。

Method: 通过Caputo分数导数重新定义梯度，建立状态转移之间的幂律时间相关性，并开发了高效的递归计算技术。

Result: 理论分析显示FPG实现了渐进方差减少，实验验证了35-68%的样本效率提升和24-52%的方差减少。

Conclusion: FPG为利用长期依赖关系提供了数学基础，且无额外计算开销。

Abstract: We propose Fractional Policy Gradients (FPG), a reinforcement learning
framework incorporating fractional calculus for long-term temporal modeling in
policy optimization. Standard policy gradient approaches face limitations from
Markovian assumptions, exhibiting high variance and inefficient sampling. By
reformulating gradients using Caputo fractional derivatives, FPG establishes
power-law temporal correlations between state transitions. We develop an
efficient recursive computation technique for fractional temporal-difference
errors with constant time and memory requirements. Theoretical analysis shows
FPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus
standard policy gradients while preserving convergence. Empirical validation
demonstrates 35-68% sample efficiency gains and 24-52% variance reduction
versus state-of-the-art baselines. This framework provides a mathematically
grounded approach for leveraging long-range dependencies without computational
overhead.

</details>


### [181] [Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap](https://arxiv.org/abs/2507.00075)
*Yifan Sun,Yushan Liang,Zhen Zhang,Jiaye Teng*

Main category: cs.LG

TL;DR: 本文通过理论建模和实证验证，探讨了大型语言模型（LLM）自我改进过程中的性能演化，提出了求解器-验证器间隙的概念，并分析了外部数据对动态的影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLM自我改进过程中性能演化的理论机制，填补现有研究的空白。

Method: 通过求解器-验证器间隙理论建模训练动态，并利用早期训练信息预测最终性能。

Result: 理论模型在多种LLM和数据集上得到验证，外部数据在有限情况下不影响最终性能。

Conclusion: 求解器-验证器间隙理论有效解释了LLM自我改进的动态，外部数据在特定条件下可灵活使用。

Abstract: Self-improvement is among the most prominent techniques within the realm of
large language models (LLM), aiming to enhance the LLM performance without
relying on external data. Despite its significance, generally how LLM
performances evolve during the self-improvement process remains underexplored.
In this paper, we theoretically model the training dynamics of self-improvement
via the concept of solver-verifier gap. This is inspired by the conjecture that
the performance enhancement of self-improvement stems from the gap between
LLM's solver capability and verifier capability. Based on the theoretical
framework, we further introduce how to predict the ultimate power of
self-improvement using only information from the first few training epochs. We
empirically validate the effectiveness of the theoretical model on various LLMs
and datasets. Beyond self-improvement, we extend our analysis to investigate
how external data influences these dynamics within the framework. Notably, we
find that under limited external data regimes, such external data can be
utilized at any stage without significantly affecting final performances, which
accords with the empirical observations.

</details>


### [182] [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
*Yi Xie,Yun Xiong,Zejian Shi,Hao Niu,Zhengfu Liu*

Main category: cs.LG

TL;DR: 论文探讨了基于补丁的时间序列基础模型的表示学习机制和泛化能力，解释了其跨领域迁移成功的理论依据。


<details>
  <summary>Details</summary>
Motivation: 研究时间序列基础模型在跨领域迁移中的成功现象，解决其与时间序列数据动态特性矛盾的悖论。

Method: 从理论和实验角度分析补丁时间序列模型的表示学习机制，提出其将确定性向量表示扩展为潜在概率分布形式的框架。

Result: 理论分析表明，连续时间序列补丁可量化为离散词汇，其统计特性与自然语言高度一致，从而继承语言模型的表示和迁移能力。

Conclusion: 为理解、评估和改进大规模时间序列基础模型的安全性和可靠性提供了理论基石。

Abstract: With the rise of large language models, the paradigm of training foundation
models with massive parameter counts on vast datasets has been adopted in
multiple domains to achieve remarkable success. Time series foundation models
represent a significant extension of this paradigm, demonstrating exceptional
expressive power, generalization, and cross-domain transferability. However,
this gives rise to a fundamental paradox: time series data reflect distinct
dynamical systems, making cross-domain transfer intuitively implausible, yet
this is contradicted by the models' empirical success. To resolve this paradox,
this paper investigates, from both theoretical and experimental perspectives,
the representation learning mechanisms and generalization capabilities of
patch-based time series foundation models. We argue that such models are not
merely applying a new architecture but are fundamentally generalizing the
representation paradigm of language models by extending deterministic
vector-based representations to latent probabilistic distributional forms. Our
theoretical analysis supports this framework by demonstrating that continuous
time-series patches can be faithfully quantized into a discrete vocabulary
whose key statistical properties are highly consistent with those of natural
language. This generalization allows time series models to inherit the robust
representation and transfer abilities of large language models, thereby
explaining their superior performance in temporal tasks. Ultimately, our work
provides a rigorous theoretical cornerstone for understanding, evaluating, and
improving the safety and reliability of large-scale time series foundation
models.

</details>


### [183] [Online Meal Detection Based on CGM Data Dynamics](https://arxiv.org/abs/2507.00080)
*Ali Tavasoli,Heman Shakeri*

Main category: cs.LG

TL;DR: 利用动态模式从连续血糖监测数据中检测餐食事件，提高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在检测餐食事件时准确性和可解释性不足，需要更有效的特征提取方法。

Method: 通过动态模式捕捉血糖变化的关键特征，识别与餐食相关的模式和异常。

Result: 显著提高了餐食检测的准确性，增强了血糖动态的可解释性。

Conclusion: 该方法优于传统方法，适用于多样化数据集和实际应用。

Abstract: We utilize dynamical modes as features derived from Continuous Glucose
Monitoring (CGM) data to detect meal events. By leveraging the inherent
properties of underlying dynamics, these modes capture key aspects of glucose
variability, enabling the identification of patterns and anomalies associated
with meal consumption. This approach not only improves the accuracy of meal
detection but also enhances the interpretability of the underlying glucose
dynamics. By focusing on dynamical features, our method provides a robust
framework for feature extraction, facilitating generalization across diverse
datasets and ensuring reliable performance in real-world applications. The
proposed technique offers significant advantages over traditional approaches,
improving detection accuracy,

</details>


### [184] [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
*Faranaksadat Solat,Joohyung Lee,Mohamed Seif,Dusit Niyato,H. Vincent Poor*

Main category: cs.LG

TL;DR: FedHLM是一种通信高效的混合语言模型框架，结合了不确定性感知推理和联邦学习，通过动态优化阈值和P2P重用减少LLM调用，显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决传统混合语言模型中因频繁调用LLM导致的通信开销问题，特别是在带宽受限的环境中。

Method: 提出FedHLM框架，动态学习不确定性阈值，利用联邦学习优化阈值，并引入P2P重用和分层模型聚合。

Result: 在大规模新闻分类任务中，FedHLM减少了95%以上的LLM传输，且精度损失可忽略。

Conclusion: FedHLM适合高效、可扩展的边缘AI应用，显著降低了通信成本。

Abstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small
Language Models (SLMs) on edge devices with the high accuracy of Large Language
Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM
inference, HLMs reduce latency and communication by invoking LLMs only when
local SLM predictions are uncertain, i.e., when token-level confidence is low
or entropy is high. However, ambiguous or low-confidence predictions still
require frequent offloading to the LLM, leading to significant communication
overhead in bandwidth-constrained settings. To address this, we propose FedHLM,
a communication-efficient HLM framework that integrates uncertainty-aware
inference with Federated Learning (FL). FedHLM's key innovation lies in
collaboratively learning token-level uncertainty thresholds that govern when
LLM assistance is needed. Rather than using static or manually tuned
thresholds, FedHLM employs FL to optimize these thresholds in a
privacy-preserving, distributed manner. Additionally, it leverages
embedding-based token representations for Peer-to-Peer (P2P) resolution,
enabling clients to reuse tokens inferred by semantically similar peers without
engaging the LLM. We further introduce hierarchical model aggregation: edge
servers refine local routing policies through client updates, while
cross-cluster coordination aligns global decision boundaries. This layered
design captures recurring uncertainty patterns, reducing redundant LLM queries.
Experiments on large-scale news classification tasks show that FedHLM reduces
LLM transmissions by over 95 percent with negligible accuracy loss, making it
well-suited for scalable and efficient edge-AI applications.

</details>


### [185] [Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks](https://arxiv.org/abs/2507.00083)
*Wei Meng*

Main category: cs.LG

TL;DR: 提出了一种名为IA-STGNN的新框架，用于解决战略级模拟中战术打击行为与战略延迟之间的因果关系建模问题。


<details>
  <summary>Details</summary>
Motivation: 当前战略级模拟中缺乏对战术打击行为与战略延迟之间结构化因果建模的能力，尤其是在捕捉‘韧性-节点压制-谈判窗口’链中的中间变量时存在瓶颈。

Method: 采用干预感知的时空图神经网络（IA-STGNN），整合图注意力机制、反事实模拟单元和空间干预节点重建，实现动态模拟。

Result: 实验显示IA-STGNN显著优于基线模型，MAE降低12.8%，Top-5准确率提升18.4%，同时提高了因果路径一致性和干预稳定性。

Conclusion: IA-STGNN为高层政策建模提供了结构化且透明的AI决策支持机制，适用于核威慑模拟、外交窗口评估等应用。

Abstract: This study addresses the lack of structured causal modeling between tactical
strike behavior and strategic delay in current strategic-level simulations,
particularly the structural bottlenecks in capturing intermediate variables
within the "resilience - nodal suppression - negotiation window" chain. We
propose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),
a novel framework that closes the causal loop from tactical input to strategic
delay output. The model integrates graph attention mechanisms, counterfactual
simulation units, and spatial intervention node reconstruction to enable
dynamic simulations of strike configurations and synchronization strategies.
Training data are generated from a multi-physics simulation platform (GEANT4 +
COMSOL) under NIST SP 800-160 standards, ensuring structural traceability and
policy-level validation. Experimental results demonstrate that IA-STGNN
significantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),
achieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5
percent accuracy, while improving causal path consistency and intervention
stability. IA-STGNN enables interpretable prediction of strategic delay and
supports applications such as nuclear deterrence simulation, diplomatic window
assessment, and multi-strategy optimization, providing a structured and
transparent AI decision-support mechanism for high-level policy modeling.

</details>


### [186] [A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism](https://arxiv.org/abs/2507.00085)
*Ruiyuan Jiang,Dongyao Jia,Eng Gee Lim,Pengfei Fan,Yuli Zhang,Shangbo Wang*

Main category: cs.LG

TL;DR: 提出了一种名为GFEN的新框架，用于交通速度预测，通过融合时空特征和动态数据平滑技术，显著提升了预测精度和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测方法难以处理复杂的时空动态和非平稳数据，限制了预测的准确性和适应性。

Method: GFEN结合了拓扑时空图融合技术和基于注意力的深度学习结构，动态提取多尺度时空特征并平滑数据。

Result: GFEN在预测精度上比现有方法提高了6.3%，收敛速度是最近混合模型的两倍。

Conclusion: GFEN在交通预测中表现出卓越性能，有望显著提升智能交通系统的效率。

Abstract: Accurate traffic prediction is essential for Intelligent Transportation
Systems (ITS), yet current methods struggle with the inherent complexity and
non-linearity of traffic dynamics, making it difficult to integrate spatial and
temporal characteristics. Furthermore, existing approaches use static
techniques to address non-stationary and anomalous historical data, which
limits adaptability and undermines data smoothing. To overcome these
challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative
framework for network-level traffic speed prediction. GFEN introduces a novel
topological spatiotemporal graph fusion technique that meticulously extracts
and merges spatial and temporal correlations from both data distribution and
network topology using trainable methods, enabling the modeling of multi-scale
spatiotemporal features. Additionally, GFEN employs a hybrid methodology
combining a k-th order difference-based mathematical framework with an
attention-based deep learning structure to adaptively smooth historical
observations and dynamically mitigate data anomalies and non-stationarity.
Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods
by approximately 6.3% in prediction accuracy and exhibits convergence rates
nearly twice as fast as recent hybrid models, confirming its superior
performance and potential to significantly enhance traffic prediction system
efficiency.

</details>


### [187] [pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation](https://arxiv.org/abs/2507.00087)
*Jiale Zhao,Pengzhi Mao,Kaifei Wang,Yiming Li,Yaping Peng,Ranfei Chen,Shuqi Lu,Xiaohong Ji,Jiaxiang Ding,Xin Zhang,Yucheng Liao,Weinan E,Weijie Zhang,Han Wen,Hao Chi*

Main category: cs.LG

TL;DR: pUniFind是一种大规模多模态预训练模型，用于蛋白质组学分析，通过端到端肽谱评分和零样本从头测序，显著提高了肽段识别数量和修饰覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型多为特征提取器，缺乏统一的评分框架。pUniFind旨在填补这一空白，提供更灵敏、全面的蛋白质组学分析工具。

Method: pUniFind通过跨模态预测整合光谱和肽段信息，训练于超过1亿个开放搜索光谱，支持1,300多种修饰，并包含深度学习质量控制模块。

Result: pUniFind在免疫肽组学中肽段识别数量增加42.6%，比现有从头测序方法多识别60%的PSM，且搜索空间更大。质量控制模块额外恢复38.5%的肽段。

Conclusion: pUniFind为蛋白质组学提供了一个统一、可扩展的深度学习框架，显著提升了灵敏度、修饰覆盖范围和可解释性。

Abstract: Deep learning has advanced mass spectrometry data interpretation, yet most
models remain feature extractors rather than unified scoring frameworks. We
present pUniFind, the first large-scale multimodal pre-trained model in
proteomics that integrates end-to-end peptide-spectrum scoring with open,
zero-shot de novo sequencing. Trained on over 100 million open search-derived
spectra, pUniFind aligns spectral and peptide modalities via cross modality
prediction and outperforms traditional engines across diverse datasets,
particularly achieving a 42.6 percent increase in the number of identified
peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind
identifies 60 percent more PSMs than existing de novo methods despite a
300-fold larger search space. A deep learning based quality control module
further recovers 38.5 percent additional peptides including 1,891 mapped to the
genome but absent from reference proteomes while preserving full fragment ion
coverage. These results establish a unified, scalable deep learning framework
for proteomic analysis, offering improved sensitivity, modification coverage,
and interpretability.

</details>


### [188] [A new machine learning framework for occupational accidents forecasting with safety inspections integration](https://arxiv.org/abs/2507.00089)
*Aho Yapi,Pierre Latouche,Arnaud Guillin,Yan Bailly*

Main category: cs.LG

TL;DR: 提出了一种基于安全检查的短期职业事故预测框架，将事故建模为二元时间序列，并通过机器学习算法（如LSTM）实现高精度预测，为决策者提供每周风险评估。


<details>
  <summary>Details</summary>
Motivation: 通过安全检查数据预测职业事故，帮助决策者提前干预高风险时段，优化安全资源配置。

Method: 采用滑动窗口交叉验证和多种机器学习算法（逻辑回归、树模型、神经网络），LSTM表现最佳。

Result: LSTM的平衡准确率为0.86，能有效识别高风险时段。

Conclusion: 该方法将安全检查数据转化为每周风险评分，为决策者提供实用工具，预防事故发生。

Abstract: We propose a generic framework for short-term occupational accident
forecasting that leverages safety inspections and models accident occurrences
as binary time series. The approach generates daily predictions, which are then
aggregated into weekly safety assessments to better inform decision making. To
ensure the reliability and operational applicability of the forecasts, we apply
a sliding-window cross-validation procedure specifically designed for time
series data, combined with an evaluation based on aggregated period-level
metrics. Several machine learning algorithms, including logistic regression,
tree-based models, and neural networks, are trained and systematically compared
within this framework. Unlike the other approaches, the long short-term memory
(LSTM) network outperforms the other approaches and detects the upcoming
high-risk periods with a balanced accuracy of 0.86, confirming the robustness
of our methodology and demonstrating that a binary time series model can
anticipate these critical periods based on safety inspections. The proposed
methodology converts routine safety inspection data into clear weekly risk
scores, detecting the periods when accidents are most likely. Decision-makers
can integrate these scores into their planning tools to classify inspection
priorities, schedule targeted interventions, and funnel resources to the sites
or shifts classified as highest risk, stepping in before incidents occur and
getting the greatest return on safety investments.

</details>


### [189] [Generating Heterogeneous Multi-dimensional Data : A Comparative Study](https://arxiv.org/abs/2507.00090)
*Corbeau Michael,Claeys Emmanuelle,Serrurier Mathieu,Zaraté Pascale*

Main category: cs.LG

TL;DR: 比较不同数据生成方法在消防员干预场景中的效果，提出领域特定指标评估合成数据质量。


<details>
  <summary>Details</summary>
Motivation: 优化消防员资源分配需依赖模拟场景，而数据生成方法对模拟效果至关重要。

Method: 比较随机采样、变分自编码器、生成对抗网络、条件表格生成对抗网络和扩散概率模型，并使用领域特定和标准指标评估。

Result: 传统指标不足以评估合成数据质量，领域特定指标（如响应时间分布、时空分布）更有效。

Conclusion: 领域特定指标能更准确评估合成数据质量，为消防资源分配提供更可靠支持。

Abstract: Allocation of personnel and material resources is highly sensible in the case
of firefighter interventions. This allocation relies on simulations to
experiment with various scenarios. The main objective of this allocation is the
global optimization of the firefighters response. Data generation is then
mandatory to study various scenarios In this study, we propose to compare
different data generation methods. Methods such as Random Sampling, Tabular
Variational Autoencoders, standard Generative Adversarial Networks, Conditional
Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are
examined to ascertain their efficacy in capturing the intricacies of
firefighter interventions. Traditional evaluation metrics often fall short in
capturing the nuanced requirements of synthetic datasets for real-world
scenarios. To address this gap, an evaluation of synthetic data quality is
conducted using a combination of domain-specific metrics tailored to the
firefighting domain and standard measures such as the Wasserstein distance.
Domain-specific metrics include response time distribution, spatial-temporal
distribution of interventions, and accidents representation. These metrics are
designed to assess data variability, the preservation of fine and complex
correlations and anomalies such as event with a very low occurrence, the
conformity with the initial statistical distribution and the operational
relevance of the synthetic data. The distribution has the particularity of
being highly unbalanced, none of the variables following a Gaussian
distribution, adding complexity to the data generation process.

</details>


### [190] [DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks](https://arxiv.org/abs/2507.00101)
*Giovanni Ruggieri*

Main category: cs.LG

TL;DR: DFReg是一种基于物理启发的正则化方法，通过全局权重分布优化神经网络。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法（如Dropout或L2衰减）存在局限性，DFReg旨在提供一种全局结构正则化方法，无需改变架构或随机扰动。

Method: 借鉴密度泛函理论（DFT），DFReg通过功能惩罚鼓励权重配置的平滑性、多样性和均匀分布。

Result: DFReg在不改变网络结构或引入随机扰动的情况下，实现了全局结构正则化。

Conclusion: DFReg为神经网络正则化提供了一种新的物理启发方法，具有潜在的应用价值。

Abstract: We introduce DFReg, a physics-inspired regularization method for deep neural
networks that operates on the global distribution of weights. Drawing from
Density Functional Theory (DFT), DFReg applies a functional penalty to
encourage smooth, diverse, and well-distributed weight configurations. Unlike
traditional techniques such as Dropout or L2 decay, DFReg imposes global
structural regularity without architectural changes or stochastic
perturbations.

</details>


### [191] [Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series](https://arxiv.org/abs/2507.00102)
*Bernd Hofmann,Patrick Bruendl,Huong Giang Nguyen,Joerg Franke*

Main category: cs.LG

TL;DR: 提出了一种结合数据驱动和透明性的工业故障检测方法，通过机器学习、可解释性分析和可视化技术，实现了高准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统质量控制方法缺乏适应性且依赖专家经验，而数据驱动方法虽高效但缺乏可解释性，难以在工业环境中推广。

Method: 结合监督学习模型、Shapley可解释性分析和领域特定可视化技术，提出定量扰动分析和定性专家评估的评价方法。

Result: 在压接工艺中实现了95.9%的故障检测准确率，定量和定性评估均证实了方法的有效性和可解释性。

Conclusion: 该方法提升了数据驱动故障检测的可信度和可解释性，为工业质量控制提供了实用解决方案。

Abstract: Ensuring consistent product quality in modern manufacturing is crucial,
particularly in safety-critical applications. Conventional quality control
approaches, reliant on manually defined thresholds and features, lack
adaptability to the complexity and variability inherent in production data and
necessitate extensive domain expertise. Conversely, data-driven methods, such
as machine learning, demonstrate high detection performance but typically
function as black-box models, thereby limiting their acceptance in industrial
environments where interpretability is paramount. This paper introduces a
methodology for industrial fault detection, which is both data-driven and
transparent. The approach integrates a supervised machine learning model for
multi-class fault classification, Shapley Additive Explanations for post-hoc
interpretability, and a do-main-specific visualisation technique that maps
model explanations to operator-interpretable features. Furthermore, the study
proposes an evaluation methodology that assesses model explanations through
quantitative perturbation analysis and evaluates visualisations by qualitative
expert assessment. The approach was applied to the crimping process, a
safety-critical joining technique, using a dataset of univariate, discrete time
series. The system achieves a fault detection accuracy of 95.9 %, and both
quantitative selectivity analysis and qualitative expert evaluations confirmed
the relevance and inter-pretability of the generated explanations. This
human-centric approach is designed to enhance trust and interpretability in
data-driven fault detection, thereby contributing to applied system design in
industrial quality control.

</details>


### [192] [Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros](https://arxiv.org/abs/2507.00184)
*Jacob Schrum,Olivia Kilday,Emilio Salas,Bess Hagan,Reid Williams*

Main category: cs.LG

TL;DR: 扩散模型用于文本到游戏关卡生成的研究，提出自动标注和训练策略，比较不同模型效果，并开发GUI工具。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在文本到游戏关卡生成中的应用，解决现有方法在数据标注、文本嵌入和整体关卡生成上的不足。

Method: 自动为关卡数据集分配描述性标题，使用预训练文本编码器和简单Transformer训练扩散模型，评估多样性和可玩性。

Result: 最佳扩散模型采用简单Transformer文本嵌入，训练时间短于复杂编码器模型，效果优于无条件扩散模型和GAN。

Conclusion: 扩散模型在文本到关卡生成中表现优异，无需依赖大型语言模型，并提供了实用的GUI工具。

Abstract: Recent research shows how diffusion models can unconditionally generate
tile-based game levels, but use of diffusion models for text-to-level
generation is underexplored. There are practical considerations for creating a
usable model: caption/level pairs are needed, as is a text embedding model, and
a way of generating entire playable levels, rather than individual scenes. We
present strategies to automatically assign descriptive captions to an existing
level dataset, and train diffusion models using both pretrained text encoders
and simple transformer models trained from scratch. Captions are automatically
assigned to generated levels so that the degree of overlap between input and
output captions can be compared. We also assess the diversity and playability
of the resulting levels. Results are compared with an unconditional diffusion
model and a generative adversarial network, as well as the text-to-level
approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model
uses a simple transformer model for text embedding, and takes less time to
train than diffusion models employing more complex text encoders, indicating
that reliance on larger language models is not necessary. We also present a GUI
allowing designers to construct long levels from model-generated scenes.

</details>


### [193] [Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions](https://arxiv.org/abs/2507.00191)
*Eray Erturk,Fahad Kamran,Salar Abbaspourazad,Sean Jewell,Harsh Sharma,Yujie Li,Sinead Williamson,Nicholas J Foti,Joseph Futoma*

Main category: cs.LG

TL;DR: 该论文开发了一种基于行为信号的基础模型，利用162K个体的25亿小时可穿戴设备数据，优化架构和标记策略，在57项健康任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备记录的行为信号比低级别传感器数据更具信息量，但现有基础模型主要应用于后者。本文旨在填补这一空白。

Method: 使用162K个体的25亿小时可穿戴数据，系统优化模型架构和标记策略，开发行为信号的基础模型。

Result: 模型在57项健康任务中表现优异，尤其在行为驱动任务（如睡眠预测）中，结合原始传感器数据后性能进一步提升。

Conclusion: 研究表明，为基础模型设计适配可穿戴数据的策略至关重要，并展示了其在健康应用中的潜力。

Abstract: Wearable devices record physiological and behavioral signals that can improve
health predictions. While foundation models are increasingly used for such
predictions, they have been primarily applied to low-level sensor data, despite
behavioral data often being more informative due to their alignment with
physiologically relevant timescales and quantities. We develop foundation
models of such behavioral signals using over 2.5B hours of wearable data from
162K individuals, systematically optimizing architectures and tokenization
strategies for this unique dataset. Evaluated on 57 health-related tasks, our
model shows strong performance across diverse real-world applications including
individual-level classification and time-varying health state prediction. The
model excels in behavior-driven tasks like sleep prediction, and improves
further when combined with representations of raw sensor data. These results
underscore the importance of tailoring foundation model design to wearables and
demonstrate the potential to enable new health applications.

</details>


### [194] [PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction](https://arxiv.org/abs/2507.00230)
*Peilin He,James Joshi*

Main category: cs.LG

TL;DR: 提出了一种基于联邦学习的隐私保护RDSN框架（PPFL-RDSN），用于图像重建，兼顾性能与隐私安全。


<details>
  <summary>Details</summary>
Motivation: 解决集中式训练中的隐私风险（如数据泄露和推理攻击）和高计算成本问题。

Method: 结合联邦学习、本地差分隐私和模型水印技术，确保数据安全和模型真实性。

Result: 性能与集中式方法相当，同时降低计算负担并有效缓解安全隐私漏洞。

Conclusion: PPFL-RDSN是安全、隐私保护的协作计算机视觉应用的实用解决方案。

Abstract: Reconstructing high-quality images from low-resolution inputs using Residual
Dense Spatial Networks (RDSNs) is crucial yet challenging, particularly in
collaborative scenarios where centralized training poses significant privacy
risks, including data leakage and inference attacks, as well as high
computational costs. We propose a novel Privacy-Preserving Federated
Learning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image
reconstruction. PPFL-RDSN integrates Federated Learning (FL), local
differential privacy, and robust model watermarking techniques, ensuring data
remains secure on local devices, safeguarding sensitive information, and
maintaining model authenticity without revealing underlying data. Empirical
evaluations show that PPFL-RDSN achieves comparable performance to the
state-of-the-art centralized methods while reducing computational burdens, and
effectively mitigates security and privacy vulnerabilities, making it a
practical solution for secure and privacy-preserving collaborative computer
vision applications.

</details>


### [195] [Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations](https://arxiv.org/abs/2507.00234)
*Jiztom Kavalakkatt Francis,Matthew J Darr*

Main category: cs.LG

TL;DR: 提出了一种结合ResNet和Transformer热图的新框架，解决了时空不对齐问题，提升了模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有可解释性方法中卷积网络无法捕捉全局上下文和Transformer缺乏局部精度的问题，尤其在医疗和工业监测等关键领域。

Method: 整合ResNet的梯度加权激活图和Transformer注意力展开，形成统一的可视化方法，同时保持实时性能。

Result: 在临床和工业数据集上表现优异，PhysioNet数据集准确率达94.1%，UCI能源数据集回归误差降至RMSE=0.28kWh。

Conclusion: 通过因果保真度和时空对齐，该方法为透明、时间感知的决策提供了可扩展的解决方案。

Abstract: In this paper, we present a novel framework for enhancing model
interpretability by integrating heatmaps produced separately by ResNet and a
restructured 2D Transformer with globally weighted input saliency. We address
the critical problem of spatial-temporal misalignment in existing
interpretability methods, where convolutional networks fail to capture global
context and Transformers lack localized precision - a limitation that impedes
actionable insights in safety-critical domains like healthcare and industrial
monitoring. Our method merges gradient-weighted activation maps (ResNet) and
Transformer attention rollout into a unified visualization, achieving full
spatial-temporal alignment while preserving real-time performance. Empirical
evaluations on clinical (ECG arrhythmia detection) and industrial (energy
consumption prediction) datasets demonstrate significant improvements: the
hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and
reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy
Appliance dataset-outperforming standalone ResNet, Transformer, and
InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps
into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds
suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L
(0.650) scores. By formalizing interpretability as causal fidelity and
spatial-temporal alignment, our approach bridges the gap between technical
outputs and stakeholder understanding, offering a scalable solution for
transparent, time-aware decision-making.

</details>


### [196] [Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning](https://arxiv.org/abs/2507.00257)
*Davide Salaorni,Vincenzo De Paola,Samuele Delpero,Giovanni Dispoto,Paolo Bonetti,Alessio Russo,Giuseppe Calcagno,Francesco Trovò,Matteo Papini,Alberto Maria Metelli,Marco Mussi,Marcello Restelli*

Main category: cs.LG

TL;DR: 论文介绍了Gym4ReaL，一套针对现实世界复杂性的强化学习环境，旨在推动RL算法在真实场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前RL研究多集中于理想化环境，忽视了现实世界的复杂性（如大状态-动作空间、非平稳性和部分可观测性），因此需要更真实的评估环境。

Method: 开发了Gym4ReaL，包含多样化的任务，以模拟现实世界的挑战。

Result: 实验表明，标准RL算法在这些环境中仍具竞争力，但需新方法以充分发挥RL潜力。

Conclusion: Gym4ReaL为RL在现实场景中的应用提供了重要工具，并呼吁开发更适应复杂性的算法。

Abstract: In recent years, \emph{Reinforcement Learning} (RL) has made remarkable
progress, achieving superhuman performance in a wide range of simulated
environments. As research moves toward deploying RL in real-world applications,
the field faces a new set of challenges inherent to real-world settings, such
as large state-action spaces, non-stationarity, and partial observability.
Despite their importance, these challenges are often underexplored in current
benchmarks, which tend to focus on idealized, fully observable, and stationary
environments, often neglecting to incorporate real-world complexities
explicitly. In this paper, we introduce \texttt{Gym4ReaL}, a comprehensive
suite of realistic environments designed to support the development and
evaluation of RL algorithms that can operate in real-world scenarios. The suite
includes a diverse set of tasks that expose algorithms to a variety of
practical challenges. Our experimental results show that, in these settings,
standard RL algorithms confirm their competitiveness against rule-based
benchmarks, motivating the development of new methods to fully exploit the
potential of RL to tackle the complexities of real-world tasks.

</details>


### [197] [Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning](https://arxiv.org/abs/2507.00259)
*Amr Abourayya,Jens Kleesiek,Bharat Rao,Michael Kamp*

Main category: cs.LG

TL;DR: FEDMOSAIC提出了一种基于自适应协作的个性化联邦学习方法，通过客户端在共享未标记数据集上交换预测，实现细粒度信任决策，优于现有PFL方法。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据异构性挑战，现有PFL方法未能超越本地或集中式基线，表明协作方式与数据结构不匹配。

Method: 提出FEDMOSAIC方法，客户端自适应决定依赖他人程度及信任对象，通过共享未标记数据集交换预测，调整损失权重和贡献全局伪标签。

Result: FEDMOSAIC在多样化非IID设置下优于现有PFL方法，并提供收敛保证。

Conclusion: 数据感知协作在实现鲁棒和有效个性化方面具有潜力。

Abstract: Data heterogeneity is a central challenge in federated learning, and
personalized federated learning (PFL) aims to address it by tailoring models to
each client's distribution. Yet many PFL methods fail to outperform local or
centralized baselines, suggesting a mismatch between the collaboration they
enforce and the structure of the data. We propose an approach based on adaptive
collaboration, where clients decide adaptively not only how much to rely on
others, but also whom to trust at the level of individual examples. We
instantiate this principle in FEDMOSAIC, a federated co-training method in
which clients exchange predictions over a shared unlabeled dataset. This
enables fine-grained trust decisions that are difficult to achieve with
parameter sharing alone. Each client adjusts its loss weighting based on the
agreement between private and public data, and contributes to global
pseudo-labels in proportion to its estimated per-example confidence.
Empirically, FEDMOSAIC improves upon state-of-the-art PFL methods across
diverse non-IID settings, and we provide convergence guarantees under standard
assumptions. Our results demonstrate the potential of data-aware collaboration
for robust and effective personalization.

</details>


### [198] [Examining Reject Relations in Stimulus Equivalence Simulations](https://arxiv.org/abs/2507.00265)
*Alexis Carrillo,Asieh Abolpour Mofrad,Anis Yazidi,Moises Betancort*

Main category: cs.LG

TL;DR: 研究通过计算模型探讨拒绝关系在刺激等价性（SE）中的作用，发现人工神经网络可能依赖关联学习而非SE。


<details>
  <summary>Details</summary>
Motivation: 探索拒绝关系是否会影响刺激等价类的形成，并验证人工神经网络是否能真正实现SE。

Method: 使用前馈神经网络（FFN）、BERT和GPT模型，在18种匹配到样本（MTS）模拟条件下测试，比较不同训练结构和关系类型。

Result: 拒绝关系影响模型表现，部分模型在高准确率下仍与概率代理相似，表明其依赖关联学习。

Conclusion: 人工神经网络可能未真正实现SE，需更严格标准评估计算模型。

Abstract: Simulations offer a valuable tool for exploring stimulus equivalence (SE),
yet the potential of reject relations to disrupt the assessment of equivalence
class formation is contentious. This study investigates the role of reject
relations in the acquisition of stimulus equivalence using computational
models. We examined feedforward neural networks (FFNs), bidirectional encoder
representations from transformers (BERT), and generative pre-trained
transformers (GPT) across 18 conditions in matching-to-sample (MTS)
simulations. Conditions varied in training structure (linear series,
one-to-many, and many-to-one), relation type (select-only, reject-only, and
select-reject), and negative comparison selection (standard and biased). A
probabilistic agent served as a benchmark, embodying purely associative
learning. The primary goal was to determine whether artificial neural networks
could demonstrate equivalence class formation or whether their performance
reflected associative learning. Results showed that reject relations influenced
agent performance. While some agents achieved high accuracy on equivalence
tests, particularly with reject relations and biased negative comparisons, this
performance was comparable to the probabilistic agent. These findings suggest
that artificial neural networks, including transformer models, may rely on
associative strategies rather than SE. This underscores the need for careful
consideration of reject relations and more stringent criteria in computational
models of equivalence.

</details>


### [199] [Double Q-learning for Value-based Deep Reinforcement Learning, Revisited](https://arxiv.org/abs/2507.00275)
*Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

TL;DR: 论文研究了深度强化学习中Double Q-learning的核心思想，提出了Deep Double Q-learning（DDQL），旨在减少高估问题并验证其性能优于Double DQN。


<details>
  <summary>Details</summary>
Motivation: 解决Q-learning在深度强化学习中的普遍高估问题，尤其是Double DQN未能完全实现Double Q-learning核心思想的问题。

Method: 提出Deep Double Q-learning（DDQL），通过训练两个Q函数并相互引导，减少高估。

Result: DDQL在57款Atari 2600游戏中减少了高估，性能优于Double DQN，且无需额外超参数。

Conclusion: DDQL有效减少高估，性能优于Double DQN，并探讨了其网络架构、回放比例和小批量采样策略。

Abstract: Overestimation is pervasive in reinforcement learning (RL), including in
Q-learning, which forms the algorithmic basis for many value-based deep RL
algorithms. Double Q-learning is an algorithm introduced to address
Q-learning's overestimation by training two Q-functions and using both to
de-correlate action-selection and action-evaluation in bootstrap targets.
Shortly after Q-learning was adapted to deep RL in the form of deep Q-networks
(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.
However, Double DQN only loosely adapts Double Q-learning, forgoing the
training of two different Q-functions that bootstrap off one another. In this
paper, we study algorithms that adapt this core idea of Double Q-learning for
value-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our
aim is to understand whether DDQL exhibits less overestimation than Double DQN
and whether performant instantiations of DDQL exist. We answer both questions
affirmatively, demonstrating that DDQL reduces overestimation and outperforms
Double DQN in aggregate across 57 Atari 2600 games, without requiring
additional hyperparameters. We also study several aspects of DDQL, including
its network architecture, replay ratio, and minibatch sampling strategy.

</details>


### [200] [Structure-preserving Lift & Learn: Scientific machine learning for nonlinear conservative partial differential equations](https://arxiv.org/abs/2507.00301)
*Harsh Sharma,Juan Diego Draxl Giannoni,Boris Kramer*

Main category: cs.LG

TL;DR: 提出了一种结构保持的Lift & Learn方法，通过学习非线性PDE的降阶模型，结合能量二次化策略，保留物理特性。


<details>
  <summary>Details</summary>
Motivation: 解决非线性PDE降阶模型中结构保持和计算效率的问题。

Method: 采用能量二次化策略，推导等效二次系统，并通过约束优化学习线性降阶算子。

Result: 在三个数值实验中验证了方法的准确性和计算效率，优于现有方法。

Conclusion: 结构保持的Lift & Learn方法在非线性PDE降阶中具有高效性和通用性。

Abstract: This work presents structure-preserving Lift & Learn, a scientific machine
learning method that employs lifting variable transformations to learn
structure-preserving reduced-order models for nonlinear partial differential
equations (PDEs) with conservation laws. We propose a hybrid learning approach
based on a recently developed energy-quadratization strategy that uses
knowledge of the nonlinearity at the PDE level to derive an equivalent
quadratic lifted system with quadratic system energy. The lifted dynamics
obtained via energy quadratization are linear in the old variables, making
model learning very effective in the lifted setting. Based on the lifted
quadratic PDE model form, the proposed method derives quadratic reduced terms
analytically and then uses those derived terms to formulate a constrained
optimization problem to learn the remaining linear reduced operators in a
structure-preserving way. The proposed hybrid learning approach yields
computationally efficient quadratic reduced-order models that respect the
underlying physics of the high-dimensional problem. We demonstrate the
generalizability of quadratic models learned via the proposed
structure-preserving Lift & Learn method through three numerical examples: the
one-dimensional wave equation with exponential nonlinearity, the
two-dimensional sine-Gordon equation, and the two-dimensional
Klein-Gordon-Zakharov equations. The numerical results show that the proposed
learning approach is competitive with the state-of-the-art structure-preserving
data-driven model reduction method in terms of both accuracy and computational
efficiency.

</details>


### [201] [MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic](https://arxiv.org/abs/2507.00304)
*Yujun Zhang,Runlong Li,Xiaoxiang Liang,Xinhao Yang,Tian Su,Bo Liu,Yan Zhou*

Main category: cs.LG

TL;DR: MamNet模型结合时域建模和频域特征提取，显著提升了网络流量预测和异常检测的性能。


<details>
  <summary>Details</summary>
Motivation: 网络流量异常波动可能预示安全威胁或系统故障，需高效预测与检测方法。

Method: MamNet通过Mamba模块（时域建模）和傅里叶变换（频域特征提取）捕捉流量长期依赖与周期性波动，并在特征融合层整合多尺度信息。

Result: 在UNSW-NB15和CAIDA数据集上，MamNet在准确率、召回率和F1-Score上优于主流模型，性能提升2%至4%。

Conclusion: MamNet能有效检测多时间尺度的流量异常，未来可通过融入外部事件信息进一步优化。

Abstract: The abnormal fluctuations in network traffic may indicate potential security
threats or system failures. Therefore, efficient network traffic prediction and
anomaly detection methods are crucial for network security and traffic
management. This paper proposes a novel network traffic prediction and anomaly
detection model, MamNet, which integrates time-domain modeling and
frequency-domain feature extraction. The model first captures the long-term
dependencies of network traffic through the Mamba module (time-domain
modeling), and then identifies periodic fluctuations in the traffic using
Fourier Transform (frequency-domain feature extraction). In the feature fusion
layer, multi-scale information is integrated to enhance the model's ability to
detect network traffic anomalies. Experiments conducted on the UNSW-NB15 and
CAIDA datasets demonstrate that MamNet outperforms several recent mainstream
models in terms of accuracy, recall, and F1-Score. Specifically, it achieves an
improvement of approximately 2% to 4% in detection performance for complex
traffic patterns and long-term trend detection. The results indicate that
MamNet effectively captures anomalies in network traffic across different time
scales and is suitable for anomaly detection tasks in network security and
traffic management. Future work could further optimize the model structure by
incorporating external network event information, thereby improving the model's
adaptability and stability in complex network environments.

</details>


### [202] [Open-ended Scientific Discovery via Bayesian Surprise](https://arxiv.org/abs/2507.00310)
*Dhruv Agarwal,Bodhisattwa Prasad Majumder,Reece Adamson,Megha Chakravorty,Satvika Reddy Gavireddy,Aditya Parashar,Harshit Surana,Bhavana Dalvi Mishra,Andrew McCallum,Ashish Sabharwal,Peter Clark*

Main category: cs.LG

TL;DR: AutoDS是一种基于贝叶斯惊喜的开放自主科学发现方法，通过蒙特卡洛树搜索策略优化假设探索，显著提升了发现效率。


<details>
  <summary>Details</summary>
Motivation: 现有开放自主科学发现方法依赖多样性启发式或主观兴趣度，难以有效导航假设空间或定义模糊。AutoDS旨在通过贝叶斯惊喜驱动探索，提升发现效率。

Method: AutoDS利用贝叶斯惊喜量化假设的认知变化，采用蒙特卡洛树搜索策略（MCTS）和渐进扩展方法，以惊喜为奖励函数。

Result: 在21个真实数据集上，AutoDS在固定预算下比竞争对手多产生5-29%的LLM认为惊喜的发现，且三分之二的发现被领域专家认为惊喜。

Conclusion: AutoDS是开放自主科学发现的重要进展，展示了贝叶斯惊喜驱动的探索潜力。

Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on
answering questions, but also on knowing which questions to ask. Most recent
works in ASD explore the use of large language models (LLMs) in goal-driven
settings, relying on human-specified research questions to guide hypothesis
generation. However, scientific discovery may be accelerated further by
allowing the AI system to drive exploration by its own criteria. The few
existing approaches in open-ended ASD select hypotheses based on diversity
heuristics or subjective proxies for human interestingness, but the former
struggles to meaningfully navigate the typically vast hypothesis space, and the
latter suffers from imprecise definitions. This paper presents AutoDS -- a
method for open-ended ASD that instead drives scientific exploration using
Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior
beliefs about a hypothesis to its posterior beliefs after gathering
experimental results. To efficiently explore the space of nested hypotheses,
our method employs a Monte Carlo tree search (MCTS) strategy with progressive
widening using surprisal as the reward function. We evaluate AutoDS in the
setting of data-driven discovery across 21 real-world datasets spanning domains
such as biology, economics, finance, and behavioral science. Our results
demonstrate that under a fixed budget, AutoDS substantially outperforms
competitors by producing 5--29\% more discoveries deemed surprising by the LLM.
Our human evaluation further finds that two-thirds of AutoDS discoveries are
surprising to the domain experts, suggesting this is an important step forward
towards building open-ended ASD systems.

</details>


### [203] [$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/abs/2507.00316)
*Siyou Li,Pengyao Qin,Huanan Wu,Dong Nie,Arun J. Thirunavukarasu,Juntao Yu,Le Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为μ²LLM的多尺度多模态大语言模型，用于自动化放射学报告生成（RRG），通过新型μ²Tokenizer整合多模态特征，并利用DPO优化生成质量，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动化放射学报告生成（RRG）面临两大挑战：从影像数据中提取信息的复杂性及模型生成报告与专家报告差异的客观评估困难。

Method: 提出μ²LLM模型，结合多尺度视觉分词器和文本分词器，通过μ²Tokenizer整合多模态特征，并使用DPO优化生成质量。

Result: 在四个大型CT影像数据集上的实验表明，μ²LLM优于现有方法。

Conclusion: μ²LLM在有限数据下表现出色，展示了其在RRG任务中的潜力。

Abstract: Automated radiology report generation (RRG) aims to produce detailed textual
reports from clinical imaging, such as computed tomography (CT) scans, to
improve the accuracy and efficiency of diagnosis and provision of management
advice. RRG is complicated by two key challenges: (1) inherent complexity in
extracting relevant information from imaging data under resource constraints,
and (2) difficulty in objectively evaluating discrepancies between
model-generated and expert-written reports. To address these challenges, we
propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale
$\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The
novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal
features from the multiscale visual tokenizer and the text tokenizer, then
enhances report generation quality through direct preference optimization
(DPO), guided by GREEN-RedLlama. Experimental results on four large CT
image-report medical datasetdemonstrate that our method outperforms existing
approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited
data for RRG tasks.

</details>


### [204] [Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience](https://arxiv.org/abs/2507.00320)
*Christiana Westlin,Ashutosh Singh,Deniz Erdogmus,Georgios Stratis,Lisa Feldman Barrett*

Main category: cs.LG

TL;DR: 重新分析情绪类别研究数据，发现情绪类别内存在显著个体差异，挑战传统情绪分类假设。


<details>
  <summary>Details</summary>
Motivation: 传统情绪科学假设情绪类别是生物和心理类型学，但本研究提出情绪类别是多样化的实例群体。

Method: 重新分析一项基于类型学的研究数据，采用最小假设的方法检验情绪类别内的脑模式变异。

Result: 未观察到原始研究中的情绪类别映射，而是发现情绪类别内脑模式的显著个体差异。

Conclusion: 研究假设对科学结论有重大影响，需多种分析方法验证假设。

Abstract: In the science of emotion, it is widely assumed that folk emotion categories
form a biological and psychological typology, and studies are routinely
designed and analyzed to identify emotion-specific patterns. This approach
shapes the observations that studies report, ultimately reinforcing the
assumption that guided the investigation. Here, we reanalyzed data from one
such typologically-guided study that reported mappings between individual brain
patterns and group-averaged ratings of 34 emotion categories. Our reanalysis
was guided by an alternative view of emotion categories as populations of
variable, situated instances, and which predicts a priori that there will be
significant variation in brain patterns within a category across instances.
Correspondingly, our analysis made minimal assumptions about the structure of
the variance present in the data. As predicted, we did not observe the original
mappings and instead observed significant variation across individuals. These
findings demonstrate how starting assumptions can ultimately impact scientific
conclusions and suggest that a hypothesis must be supported using multiple
analytic methods before it is taken seriously.

</details>


### [205] [MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE](https://arxiv.org/abs/2507.00390)
*Geng Zhang,Yuxuan Han,Yuxuan Lou,Wangbo Zhao,Yiqi Zhang,Yang You*

Main category: cs.LG

TL;DR: MoNE是一种新型专家剪枝方法，通过用轻量级新手替换冗余专家，实现高效且稳健的模型压缩。


<details>
  <summary>Details</summary>
Motivation: 解决MoE模型因保留所有专家而带来的内存开销问题，同时避免现有剪枝方法在性能和不稳定性上的不足。

Method: 基于访问频率和输出方差评估专家冗余性，替换低使用率且输出稳定的专家为轻量级新手。

Result: 在多种架构、数据源和样本量下，MoNE优于基线方法，零样本任务准确率提升显著。

Conclusion: MoNE是一种有效且稳健的专家剪枝方法，显著降低了内存开销且性能损失最小。

Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models
by activating only a subset of experts per input token. However, deploying
MoE-based models incurs significant memory overhead due to the need to retain
all experts in memory. While structured pruning is promising to reduce memory
costs, existing methods often show suboptimal performance and unstable
degradation in three dimensions: model architectures, calibration data sources,
and calibration sample sizes. This paper proposes
Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that
replaces redundant experts with lightweight novices to achieve effective and
robust model compression. MoNE evaluates expert redundancy based on two
metrics: access frequency and output variance. Experts exhibiting low usage and
stable outputs are pruned and replaced with lightweight novices-unbiased
estimations of their original outputs-minimizing performance degradation.
Extensive experiments demonstrate that MoNE consistently outperforms baseline
methods with minimal accuracy degradation across the three dimensions,
confirming its effectiveness and robustness. Notably, it improves the average
zero shot accuracy across nine downstream tasks by up to 2.71 under 25\%
pruning ratio and 3.61 under 50\% pruning. The code is available at
https://github.com/zxgx/mode-pd.

</details>


### [206] [HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism](https://arxiv.org/abs/2507.00394)
*Geng Zhang,Shenggan Cheng,Xuanlei Zhao,Ziming Liu,Yang You*

Main category: cs.LG

TL;DR: HelixPipe是一种新型的流水线并行方法，用于长序列Transformer训练，通过优化注意力计算和内存管理，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有流水线并行方法在长序列Transformer训练中因二次注意力计算和内存开销导致的性能问题。

Method: 引入注意力并行分区、两阶段先进后出微批次调度、无注意力重计算和分块MLP技术。

Result: 实验表明HelixPipe在长序列训练中表现优越，吞吐量和可扩展性优于现有方法，64 GPU上7B模型训练速度提升26%。

Conclusion: HelixPipe通过创新设计有效解决了长序列训练的挑战，为大规模Transformer模型提供了高效解决方案。

Abstract: As transformer sequence lengths grow, existing pipeline parallelisms incur
suboptimal performance due to the quadratic attention computation and the
substantial memory overhead. To relieve these challenges, we propose HelixPipe,
a novel pipeline parallelism for long sequence transformer training. First,
HelixPipe introduces attention parallel partition, which schedules attention
computations of different micro batches across different pipeline stages in
parallel, reducing pipeline bubbles. Second, it employs a two-fold
first-in-last-out micro batch schedule to balance memory usage and overlap
communication with computation. Additionally, HelixPipe utilizes recomputation
without attention and chunked MLP to mitigate fragmentation and enable longer
sequences. Experiments demonstrate that HelixPipe gains increasing advantages
with longer sequence lengths, and outperforms existing methods in throughput
and scalability across varying pipeline sizes, model sizes, and cluster
configurations. Notably, it achieves a 26\% speedup over baseline methods when
training a 7B model with 128k sequence length on 64 H20 GPUs. Code is available
at https://github.com/code-tunnel/Megatron-LM/tree/dev.

</details>


### [207] [Diffusion Disambiguation Models for Partial Label Learning](https://arxiv.org/abs/2507.00411)
*Jinfu Fan,Xiaohui Zhong,Kangrui Ren,Jiangnan Li,Linqing Huang*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的标签去歧方法（DDMP），用于解决部分标签学习中的标签模糊问题，通过生成模型逐步优化标签。


<details>
  <summary>Details</summary>
Motivation: 部分标签学习中，标签模糊导致实例与标签不匹配，影响分类性能。扩散模型在生成任务中表现优异，因此探索其用于标签去歧的潜力。

Method: 提出DDMP方法，利用实例与标签的互补信息构建伪干净标签进行初始扩散训练，并引入动态更新的转移感知矩阵估计真实标签。

Result: 实验表明DDMP在部分标签学习中具有优势，能有效提升分类性能。

Conclusion: DDMP通过扩散模型逐步优化标签，解决了标签模糊问题，适用于部分标签学习。

Abstract: Learning from ambiguous labels is a long-standing problem in practical
machine learning applications. The purpose of \emph{partial label learning}
(PLL) is to identify the ground-truth label from a set of candidate labels
associated with a given instance. Inspired by the remarkable performance of
diffusion models in various generation tasks, this paper explores their
potential to denoise ambiguous labels through the reverse denoising process.
Therefore, this paper reformulates the label disambiguation problem from the
perspective of generative models, where labels are generated by iteratively
refining initial random guesses. This perspective enables the diffusion model
to learn how label information is generated stochastically. By modeling the
generation uncertainty, we can use the maximum likelihood estimate of the label
for classification inference. However, such ambiguous labels lead to a mismatch
between instance and label, which reduces the quality of generated data. To
address this issue, this paper proposes a \emph{diffusion disambiguation model
for PLL} (DDMP), which first uses the potential complementary information
between instances and labels to construct pseudo-clean labels for initial
diffusion training. Furthermore, a transition-aware matrix is introduced to
estimate the potential ground-truth labels, which are dynamically updated
during the diffusion generation. During training, the ground-truth label is
progressively refined, improving the classifier. Experiments show the advantage
of the DDMP and its suitability for PLL.

</details>


### [208] [Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows](https://arxiv.org/abs/2507.00425)
*Ruixiang Zhang,Shuangfei Zhai,Jiatao Gu,Yizhe Zhang,Huangjie Zheng,Tianrong Chen,Miguel Angel Bautista,Josh Susskind,Navdeep Jaitly*

Main category: cs.LG

TL;DR: 论文提出了一种新的语言建模框架TarFlowLM，将离散标记空间转移到连续潜在空间，利用自回归归一化流模型，支持双向上下文和多层次生成。


<details>
  <summary>Details</summary>
Motivation: 探索自回归模型的设计空间，通过连续潜在空间提供更大的建模灵活性。

Method: 提出TarFlowLM框架，基于自回归归一化流模型，支持双向上下文、块生成和多层次生成。

Result: 在语言建模基准测试中表现出色，展示了框架的灵活建模能力。

Conclusion: TarFlowLM为语言建模提供了新的建模范式，具有灵活性和性能优势。

Abstract: Autoregressive models have driven remarkable progress in language modeling.
Their foundational reliance on discrete tokens, unidirectional context, and
single-pass decoding, while central to their success, also inspires the
exploration of a design space that could offer new axes of modeling
flexibility. In this work, we explore an alternative paradigm, shifting
language modeling from a discrete token space to a continuous latent space. We
propose a novel framework TarFlowLM, that employs transformer-based
autoregressive normalizing flows to model these continuous representations.
This approach unlocks substantial flexibility, enabling the construction of
models that can capture global bi-directional context through stacked,
alternating-direction autoregressive transformations, support block-wise
generation with flexible token patch sizes, and facilitate a hierarchical
multi-pass generation process. We further propose new mixture-based coupling
transformations designed to capture complex dependencies within the latent
space shaped by discrete data, and demonstrate theoretical connections to
conventional discrete autoregressive models. Extensive experiments on language
modeling benchmarks demonstrate strong likelihood performance and highlight the
flexible modeling capabilities inherent in our framework.

</details>


### [209] [A Recipe for Causal Graph Regression: Confounding Effects Revisited](https://arxiv.org/abs/2507.00440)
*Yujia Yin,Tianyi Qu,Zihao Wang,Yifan Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种针对图回归任务的因果图学习方法（CGR），通过改进现有因果图学习技术中的混杂效应处理，提升了图神经网络在分布外（OOD）场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有因果图学习（CGL）技术主要集中在分类任务上，而更具挑战性的回归任务被忽视。本文旨在填补这一空白，解决因果图回归（CGR）问题。

Method: 通过重新设计混杂效应的处理方式，将分类任务中的因果干预技术推广到回归任务，并结合对比学习视角。

Result: 在图OOD基准测试上的大量实验验证了所提CGR方法的有效性。

Conclusion: 本文成功将因果图学习技术扩展到回归任务，为图神经网络的泛化能力提供了新思路。

Abstract: Through recognizing causal subgraphs, causal graph learning (CGL) has risen
to be a promising approach for improving the generalizability of graph neural
networks under out-of-distribution (OOD) scenarios. However, the empirical
successes of CGL techniques are mostly exemplified in classification settings,
while regression tasks, a more challenging setting in graph learning, are
overlooked. We thus devote this work to tackling causal graph regression (CGR);
to this end we reshape the processing of confounding effects in existing CGL
studies, which mainly deal with classification. Specifically, we reflect on the
predictive power of confounders in graph-level regression, and generalize
classification-specific causal intervention techniques to regression through a
lens of contrastive learning. Extensive experiments on graph OOD benchmarks
validate the efficacy of our proposals for CGR. The model implementation and
the code are provided on https://github.com/causal-graph/CGR.

</details>


### [210] [Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design](https://arxiv.org/abs/2507.00445)
*Xingyu Su,Xiner Li,Masatoshi Uehara,Sunwoo Kim,Yulai Zhao,Gabriele Scalia,Ehsan Hajiramezanali,Tommaso Biancalani,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

TL;DR: 提出了一种基于迭代蒸馏的微调框架，用于优化扩散模型以适应任意奖励函数，解决了RL方法的不稳定性和低效问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，扩散模型需要优化不可微的奖励函数（如基于物理模拟或科学知识的奖励），但现有RL方法存在不稳定性和低效问题。

Method: 采用迭代蒸馏框架，通过收集离线数据、模拟奖励驱动的软优化策略，并最小化KL散度来更新模型。

Result: 在蛋白质、小分子和调控DNA设计任务中表现出高效和优越的奖励优化能力。

Conclusion: 该方法显著提升了扩散模型在奖励引导生成中的稳定性和效率。

Abstract: We address the problem of fine-tuning diffusion models for reward-guided
generation in biomolecular design. While diffusion models have proven highly
effective in modeling complex, high-dimensional data distributions, real-world
applications often demand more than high-fidelity generation, requiring
optimization with respect to potentially non-differentiable reward functions
such as physics-based simulation or rewards based on scientific knowledge.
Although RL methods have been explored to fine-tune diffusion models for such
objectives, they often suffer from instability, low sample efficiency, and mode
collapse due to their on-policy nature. In this work, we propose an iterative
distillation-based fine-tuning framework that enables diffusion models to
optimize for arbitrary reward functions. Our method casts the problem as policy
distillation: it collects off-policy data during the roll-in phase, simulates
reward-based soft-optimal policies during roll-out, and updates the model by
minimizing the KL divergence between the simulated soft-optimal policy and the
current model policy. Our off-policy formulation, combined with KL divergence
minimization, enhances training stability and sample efficiency compared to
existing RL-based methods. Empirical results demonstrate the effectiveness and
superior reward optimization of our approach across diverse tasks in protein,
small molecule, and regulatory DNA design.

</details>


### [211] [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
*Zhihao Zhan,Jianan Zhao,Zhaocheng Zhu,Jian Tang*

Main category: cs.LG

TL;DR: 论文分析了状态空间模型（SSMs）在长上下文建模中的不足，提出了一种新的合成任务“联合召回”，并证明了SSMs无法在次二次时间复杂度内解决多查询联合召回问题。通过结合上下文依赖稀疏注意力（CDSA），提出了解决方案HAX，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言处理中长上下文建模的效率问题，尤其是状态空间模型（SSMs）在捕捉长距离依赖方面的不足。

Method: 提出新的合成任务“联合召回”，理论分析SSMs的局限性，结合CDSA提出解决方案HAX，并通过实验验证其有效性。

Result: HAX在合成和真实世界长上下文基准测试中表现优于SSMs及其与上下文无关稀疏注意力（CISA）的结合。

Conclusion: HAX通过结合SSMs和CDSA，有效解决了长上下文建模的挑战，为实际应用提供了高效解决方案。

Abstract: Efficient long-context modeling remains a critical challenge for natural
language processing (NLP), as the time complexity of the predominant
Transformer architecture scales quadratically with the sequence length. While
state-space models (SSMs) offer alternative sub-quadratic solutions, they
struggle to capture long-range dependencies effectively. In this work, we focus
on analyzing and improving the long-context modeling capabilities of SSMs. We
show that the widely used synthetic task, associative recall, which requires a
model to recall a value associated with a single key without context,
insufficiently represents the complexities of real-world long-context modeling.
To address this limitation, we extend the associative recall to a novel
synthetic task, \emph{joint recall}, which requires a model to recall the value
associated with a key given in a specified context. Theoretically, we prove
that SSMs do not have the expressiveness to solve multi-query joint recall in
sub-quadratic time complexity. To resolve this issue, we propose a solution
based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which
has the expressiveness to solve multi-query joint recall with sub-quadratic
computation. To bridge the gap between theoretical analysis and real-world
applications, we propose locality-sensitive Hashing Attention with sparse Key
Selection (HAX), which instantiates the theoretical solution and is further
tailored to natural language domains. Extensive experiments on both synthetic
and real-world long-context benchmarks show that HAX consistently outperforms
SSM baselines and SSMs integrated with context-independent sparse attention
(CISA).

</details>


### [212] [Best Agent Identification for General Game Playing](https://arxiv.org/abs/2507.00451)
*Matthew Stephenson,Alex Newcombe,Eric Piette,Dennis Soemers*

Main category: cs.LG

TL;DR: 提出了一种基于Wilson得分区间的乐观选择方法（Optimistic-WS），用于在多任务领域中高效识别每个子任务的最佳算法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决多任务领域中如何高效准确地识别每个子任务的最佳算法的问题。

Method: 将问题建模为多臂老虎机中的最佳臂识别问题，提出基于Wilson得分区间的乐观选择方法（Optimistic-WS）。

Result: 在GVGAI和Ludii游戏框架中测试，Optimistic-WS在平均简单遗憾上显著优于现有方法。

Conclusion: Optimistic-WS可显著提升多任务领域中算法评估的质量和准确性，尤其适用于高计算成本的场景。

Abstract: We present an efficient and generalised procedure to accurately identify the
best performing algorithm for each sub-task in a multi-problem domain. Our
approach treats this as a set of best arm identification problems for
multi-armed bandits, where each bandit corresponds to a specific task and each
arm corresponds to a specific algorithm or agent. We propose an optimistic
selection process based on the Wilson score interval (Optimistic-WS) that ranks
each arm across all bandits in terms of their potential regret reduction. We
evaluate the performance of Optimistic-WS on two of the most popular general
game domains, the General Video Game AI (GVGAI) framework and the Ludii general
game playing system, with the goal of identifying the highest performing agent
for each game within a limited number of trials. Compared to previous best arm
identification algorithms for multi-armed bandits, our results demonstrate a
substantial performance improvement in terms of average simple regret. This
novel approach can be used to significantly improve the quality and accuracy of
agent evaluation procedures for general game frameworks, as well as other
multi-task domains with high algorithm runtimes.

</details>


### [213] [Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling](https://arxiv.org/abs/2507.00453)
*Ankit Kashyap*

Main category: cs.LG

TL;DR: 提出一种结合全局注意力与局部注意力及门控FIFO内存的Transformer架构，用于高效处理长上下文语言建模。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer在长上下文建模中注意力成本高的问题，同时兼顾短程和长程依赖。

Method: 结合全局注意力、分块局部注意力和门控FIFO内存机制，使用旋转位置编码。

Result: 模型能高效处理长上下文，注意力成本不随长度平方增长。

Conclusion: 该架构轻量且可扩展，适用于对话建模、代码补全等任务。

Abstract: We present a Transformer architecture for long-context language modeling that
combines global attention with two biologically inspired components: chunked
local attention and a gated FIFO memory mechanism. This unified attention block
allows the model to efficiently handle both short-range and long-range
dependencies without increasing attention cost quadratically. The memory module
persistently stores past token representations using a gated update mechanism
inspired by recurrent networks. Rotary positional encoding is applied per
attention head to enable directionally disentangled, scale-invariant positional
signals. The architecture is implemented entirely from scratch in PyTorch, with
no reliance on high-level libraries, enabling transparent and modular
experimentation. Our model offers a lightweight and extensible design for tasks
such as dialogue modeling, code completion, and document understanding.

</details>


### [214] [Diversity Conscious Refined Random Forest](https://arxiv.org/abs/2507.00467)
*Sijan Bhattarai,Saurav Bhandari,Girija Bhusal,Saroj Shakya,Tapendra Pandey*

Main category: cs.LG

TL;DR: 提出了一种改进的随机森林分类器，通过动态选择信息特征和聚类去冗余树，提高了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 标准随机森林依赖大量树和所有特征，导致推理成本高和模型冗余。

Method: 动态选择信息特征，分析确定新树数量，并通过聚类去除冗余树。

Result: 在多个基准数据集上，改进模型比标准随机森林准确性更高。

Conclusion: 改进的随机森林分类器在减少冗余的同时提升了性能。

Abstract: Random Forest (RF) is a widely used ensemble learning technique known for its
robust classification performance across diverse domains. However, it often
relies on hundreds of trees and all input features, leading to high inference
cost and model redundancy. In this work, our goal is to grow trees dynamically
only on informative features and then enforce maximal diversity by clustering
and retaining uncorrelated trees. Therefore, we propose a Refined Random Forest
Classifier that iteratively refines itself by first removing the least
informative features and then analytically determines how many new trees should
be grown, followed by correlation-based clustering to remove redundant trees.
The classification accuracy of our model was compared against the standard RF
on the same number of trees. Experiments on 8 multiple benchmark datasets,
including binary and multiclass datasets, demonstrate that the proposed model
achieves improved accuracy compared to standard RF.

</details>


### [215] [Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization](https://arxiv.org/abs/2507.00480)
*Kiyoung Om,Kyuil Sim,Taeyoung Yun,Hyeongyu Kang,Jinkyoo Park*

Main category: cs.LG

TL;DR: 提出了一种基于流模型和代理模型的新框架，用于解决高维黑盒约束优化问题，通过后验推断和潜在空间采样克服多模态和约束带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 高维黑盒约束优化问题在科学和工程中普遍存在，但现有方法（如贝叶斯优化和生成模型）面临维度灾难、可扩展性差和模态崩溃等问题。

Method: 方法分为两阶段：1) 训练流模型捕捉数据分布和代理模型预测目标值与约束违反；2) 将候选选择问题转化为后验推断问题，并在流模型的潜在空间中进行采样。

Result: 实验表明，该方法在合成和真实世界的约束黑盒优化任务中表现优异。

Conclusion: 提出的框架有效解决了高维约束优化中的多模态和约束挑战，代码已开源。

Abstract: Optimizing high-dimensional black-box functions under black-box constraints
is a pervasive task in a wide range of scientific and engineering problems.
These problems are typically harder than unconstrained problems due to
hard-to-find feasible regions. While Bayesian optimization (BO) methods have
been developed to solve such problems, they often struggle with the curse of
dimensionality. Recently, generative model-based approaches have emerged as a
promising alternative for constrained optimization. However, they suffer from
poor scalability and are vulnerable to mode collapse, particularly when the
target distribution is highly multi-modal. In this paper, we propose a new
framework to overcome these challenges. Our method iterates through two stages.
First, we train flow-based models to capture the data distribution and
surrogate models that predict both function values and constraint violations
with uncertainty quantification. Second, we cast the candidate selection
problem as a posterior inference problem to effectively search for promising
candidates that have high objective values while not violating the constraints.
During posterior inference, we find that the posterior distribution is highly
multi-modal and has a large plateau due to constraints, especially when
constraint feedback is given as binary indicators of feasibility. To mitigate
this issue, we amortize the sampling from the posterior distribution in the
latent space of flow-based models, which is much smoother than that in the data
space. We empirically demonstrate that our method achieves superior performance
on various synthetic and real-world constrained black-box optimization tasks.
Our code is publicly available \href{https://github.com/umkiyoung/CiBO}{here}.

</details>


### [216] [PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning](https://arxiv.org/abs/2507.00485)
*Weiran Guo,Guanjun Liu,Ziyuan Zhou,Ling Wang*

Main category: cs.LG

TL;DR: 论文研究了安全强化学习（Safe RL）中的后门攻击问题，提出了一种结合正负动作样本（PNAct）的攻击框架，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 安全强化学习虽然通过成本指标确保安全性，但其对后门攻击的脆弱性尚未被充分研究。本文旨在揭示这种潜在风险。

Method: 提出了PNAct攻击框架，结合正负动作样本植入后门，并设计了攻击算法。

Result: 实验验证了PNAct攻击框架的有效性，展示了Safe RL可能被操纵执行不安全动作的可行性。

Conclusion: 本文强调了Safe RL的后门攻击风险，为未来研究提供了新的安全挑战。

Abstract: Reinforcement Learning (RL) is widely used in tasks where agents interact
with an environment to maximize rewards. Building on this foundation, Safe
Reinforcement Learning (Safe RL) incorporates a cost metric alongside the
reward metric, ensuring that agents adhere to safety constraints during
decision-making. In this paper, we identify that Safe RL is vulnerable to
backdoor attacks, which can manipulate agents into performing unsafe actions.
First, we introduce the relevant concepts and evaluation metrics for backdoor
attacks in Safe RL. It is the first attack framework in the Safe RL field that
involves both Positive and Negative Action sample (PNAct) is to implant
backdoors, where positive action samples provide reference actions and negative
action samples indicate actions to be avoided. We theoretically point out the
properties of PNAct and design an attack algorithm. Finally, we conduct
experiments to evaluate the effectiveness of our proposed backdoor attack
framework, evaluating it with the established metrics. This paper highlights
the potential risks associated with Safe RL and underscores the feasibility of
such attacks. Our code and supplementary material are available at
https://github.com/azure-123/PNAct.

</details>


### [217] [Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling](https://arxiv.org/abs/2507.00518)
*Walid Bendada,Guillaume Salha-Galvan,Romain Hennequin,Théo Bontempelli,Thomas Bouabça,Tristan Cazenave*

Main category: cs.LG

TL;DR: vMF-exp是一种可扩展的强化学习方法，用于探索大规模动作集，通过von Mises-Fisher分布采样状态嵌入表示，并探索其最近邻动作。


<details>
  <summary>Details</summary>
Motivation: 解决Boltzmann Exploration（B-exp）在大规模动作集中的可扩展性问题，因其需要为每个动作计算softmax值。

Method: 使用von Mises-Fisher分布采样状态嵌入表示，并探索其最近邻动作。

Result: vMF-exp在理论上与B-exp具有相同的探索概率，且在实际应用中验证了其可扩展性和有效性。

Conclusion: vMF-exp是B-exp的可扩展替代方案，适用于具有超球面嵌入的大规模动作集。

Abstract: This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable
method for exploring large action sets in reinforcement learning problems where
hyperspherical embedding vectors represent these actions. vMF-exp involves
initially sampling a state embedding representation using a von Mises-Fisher
distribution, then exploring this representation's nearest neighbors, which
scales to virtually unlimited numbers of candidate actions. We show that, under
theoretical assumptions, vMF-exp asymptotically maintains the same probability
of exploring each action as Boltzmann Exploration (B-exp), a popular
alternative that, nonetheless, suffers from scalability issues as it requires
computing softmax values for each action. Consequently, vMF-exp serves as a
scalable alternative to B-exp for exploring large action sets with
hyperspherical embeddings. Experiments on simulated data, real-world public
data, and the successful large-scale deployment of vMF-exp on the recommender
system of a global music streaming service empirically validate the key
properties of the proposed method.

</details>


### [218] [Foundation Models for Clinical Records at Health System Scale](https://arxiv.org/abs/2507.00574)
*Haresh Rengaraj Rajamohan,Xiang Gao,Weicheng Zhu,Shih-Lun Huang,Long Chen,Kyunghyun Cho,Cem M. Deniz,Narges Razavian*

Main category: cs.LG

TL;DR: 该论文提出了一种针对电子健康记录（EHR）的生成式预训练策略，通过预测下一次就诊事件来学习临床事件的生成，并解决了重复事件对评估指标的干扰问题。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练在语言和其他数据类型中表现优异，但在结构化电子健康记录（EHR）中的应用尚未充分探索。

Method: 采用基于下一次就诊事件预测的自回归生成模型，处理异构数据类型的联合预测，并引入重复事件的预测正则化。

Result: 模型在零样本预测任务中表现优异，与完全微调的Transformer基线相当，能够捕捉复杂的临床依赖关系。

Conclusion: 该方法无需昂贵的任务特定微调，即可有效建模EHR数据，为医疗领域的预训练提供了新思路。

Abstract: Large-scale pretraining has transformed modeling of language and other data
types, but its potential remains underexplored in healthcare with structured
electronic health records (EHRs). We present a novel generative pretraining
strategy for sequential EHR data using next-visit event prediction. Our model
learns to autoregressively generate various tokenized clinical events for the
next visit based on patient history and inherently handles the joint prediction
of heterogeneous data types. Additionally, we introduce regularization on
predicting repeated events and highlight a key pitfall in EHR-based foundation
model evaluations: repeated event tokens can inflate performance metrics when
new onsets are not distinguished from subsequent occurrences. Our model is
evaluated via zero-shot prediction for forecasting dementia and knee
osteoarthritis incidence within 2 and 5 years, and the model performance rivals
a fully fine-tuned masked pretrained Transformer baseline, demonstrating that
our approach captures complex clinical dependencies without requiring costly
task-specific fine-tuning.

</details>


### [219] [Quantum Circuit Structure Optimization for Quantum Reinforcement Learning](https://arxiv.org/abs/2507.00589)
*Seok Bin Son,Joongheon Kim*

Main category: cs.LG

TL;DR: 提出了一种结合量子神经架构搜索（QNAS）的QRL-NAS算法，优化量子强化学习中的参数化量子电路（PQC）结构，实验证明其优于固定电路结构的QRL。


<details>
  <summary>Details</summary>
Motivation: 传统量子强化学习（QRL）使用固定的PQC结构，缺乏验证其最优性，导致性能受限。

Method: 提出QRL-NAS算法，通过QNAS优化PQC结构，结合量子神经网络的线性与非线性变换能力。

Result: 实验显示QRL-NAS比固定电路结构的QRL获得更高的奖励。

Conclusion: QRL-NAS通过优化PQC结构，提升了量子强化学习的效率和实用性。

Abstract: Reinforcement learning (RL) enables agents to learn optimal policies through
environmental interaction. However, RL suffers from reduced learning efficiency
due to the curse of dimensionality in high-dimensional spaces. Quantum
reinforcement learning (QRL) addresses this issue by leveraging superposition
and entanglement in quantum computing, allowing efficient handling of
high-dimensional problems with fewer resources. QRL combines quantum neural
networks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as
the core computational module. The PQC performs linear and nonlinear
transformations through gate operations, similar to hidden layers in classical
neural networks. Previous QRL studies, however, have used fixed PQC structures
based on empirical intuition without verifying their optimality. This paper
proposes a QRL-NAS algorithm that integrates quantum neural architecture search
(QNAS) to optimize PQC structures within QRL. Experiments demonstrate that
QRL-NAS achieves higher rewards than QRL with fixed circuits, validating its
effectiveness and practical utility.

</details>


### [220] [Cooperative Sheaf Neural Networks](https://arxiv.org/abs/2507.00647)
*André Ribeiro,Ana Luiza Tenório,Juan Belieni,Amauri H. Souza,Diego Mesquita*

Main category: cs.LG

TL;DR: 论文探讨了Sheaf扩散是否能够实现合作行为，发现现有方法因缺乏消息方向性而失败，并提出基于有向图的细胞Sheaf和CSNN模型。


<details>
  <summary>Details</summary>
Motivation: 研究Sheaf扩散是否能够实现合作行为，以增强信息扩散的灵活性。

Method: 引入有向图的细胞Sheaf概念，提出CSNN模型，并分析其接收场特性。

Result: CSNN在实验中表现优于现有Sheaf扩散和合作图神经网络方法。

Conclusion: CSNN通过有向Sheaf扩散实现了合作行为，解决了现有方法的局限性。

Abstract: Sheaf diffusion has recently emerged as a promising design pattern for graph
representation learning due to its inherent ability to handle heterophilic data
and avoid oversmoothing. Meanwhile, cooperative message passing has also been
proposed as a way to enhance the flexibility of information diffusion by
allowing nodes to independently choose whether to propagate/gather information
from/to neighbors. A natural question ensues: is sheaf diffusion capable of
exhibiting this cooperative behavior? Here, we provide a negative answer to
this question. In particular, we show that existing sheaf diffusion methods
fail to achieve cooperative behavior due to the lack of message directionality.
To circumvent this limitation, we introduce the notion of cellular sheaves over
directed graphs and characterize their in- and out-degree Laplacians. We
leverage our construction to propose Cooperative Sheaf Neural Networks (CSNNs).
Theoretically, we characterize the receptive field of CSNN and show it allows
nodes to selectively attend (listen) to arbitrarily far nodes while ignoring
all others in their path, potentially mitigating oversquashing. Our experiments
show that CSNN presents overall better performance compared to prior art on
sheaf diffusion as well as cooperative graph neural networks.

</details>


### [221] [GANs Secretly Perform Approximate Bayesian Model Selection](https://arxiv.org/abs/2507.00651)
*Maurizio Filippone,Marius P. Linhard*

Main category: cs.LG

TL;DR: 该论文将GANs解释为概率生成模型，提出其优化和正则化策略，通过边际似然优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管GANs成功，但其优化困难且易过拟合，需深入理解其正则化策略。

Method: 将GANs视为贝叶斯神经网络，通过边际似然优化和最小描述长度原则定义正则化策略。

Result: 实验表明这些策略能平滑损失景观并提升性能。

Conclusion: 研究为GANs的正则化策略提供了新视角，并验证了其有效性。

Abstract: Generative Adversarial Networks (GANs) are popular and successful generative
models. Despite their success, optimization is notoriously challenging and they
require regularization against overfitting. In this work, we explain the
success and limitations of GANs by interpreting them as probabilistic
generative models. This interpretation enables us to view GANs as Bayesian
neural networks with partial stochasticity, allowing us to establish conditions
of universal approximation. We can then cast the adversarial-style optimization
of several variants of GANs as the optimization of a proxy for the marginal
likelihood. Taking advantage of the connection between marginal likelihood
optimization and Occam's razor, we can define regularization and optimization
strategies to smooth the loss landscape and search for solutions with minimum
description length, which are associated with flat minima and good
generalization. The results on a wide range of experiments indicate that these
strategies lead to performance improvements and pave the way to a deeper
understanding of regularization strategies for GANs.

</details>


### [222] [Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models](https://arxiv.org/abs/2507.00653)
*Yilun Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种基于认知负荷理论（CLT）的框架CLAI，通过量化认知负荷指标（ICL_LLM、ECL_LLM、GCL_LLM）优化LLM推理过程，减少计算浪费，并在多个任务中显著降低token消耗（最高45%）。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理的高计算成本阻碍了其广泛应用，现有优化方法缺乏认知理论指导，因此提出CLAI框架以填补这一空白。

Method: CLAI框架将认知负荷理论量化，提出两种实现路径：CLAI-Prompt（零样本方法）和CLAI-Tune（微调模型），通过优化认知经济性减少计算浪费。

Result: 在复杂推理、长文本问答和代码生成任务中，CLAI方法显著减少token消耗（最高45%），且不影响准确性。CLAI-Tune还展现出自主分解难题的能力。

Conclusion: 通过模拟人脑资源管理策略，CLAI框架为构建更高效、鲁棒和智能的AI系统提供了新思路。

Abstract: The escalating computational costs of Large Language Model (LLM) inference
have become a critical barrier to their widespread and sustainable deployment.
While existing optimization strategies are effective, they are predominantly
based on statistical heuristics or architectural modifications, lacking a
guiding cognitive theory to manage the inference process itself. This paper
aims to bridge this gap by introducing a novel paradigm: the Cognitive
Load-Aware Inference (CLAI) framework, which operationalizes principles from
Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize
the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and
Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,
and $GCL_{LLM}$), thereby reframing the inference process as a cognitive
economics optimization problem: based on the intrinsic complexity of a problem
($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically
allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two
implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM
through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a
fine-tuned model that internalizes these principles for spontaneous cognitive
economy. Across a range of benchmarks in complex reasoning, long-context
question answering, and code generation, our methods achieve significant
reductions in token consumption (up to 45\%) without sacrificing accuracy.
Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose
difficult problems, a key characteristic of human expert cognition. This work
demonstrates that by emulating the brain's resource management strategies, we
can build more efficient, robust, and capable artificial intelligence systems.

</details>


### [223] [Diffusion Classifier Guidance for Non-robust Classifiers](https://arxiv.org/abs/2507.00687)
*Philipp Vaeth,Dibyanshu Kumar,Benjamin Paassen,Magda Gregorová*

Main category: cs.LG

TL;DR: 论文提出了一种扩展分类器引导的方法，使其适用于非鲁棒分类器，解决了噪声条件下分类器性能下降的问题，并通过稳定技术提升了引导效果。


<details>
  <summary>Details</summary>
Motivation: 传统分类器引导方法仅限于鲁棒分类器，限制了其应用范围。本文旨在扩展该方法，使其适用于更广泛的非鲁棒分类器。

Method: 通过分析非鲁棒分类器在噪声条件下的性能，提出了一种利用单步去噪图像预测和稳定技术（如指数移动平均）的方法。

Result: 实验表明，该方法提升了分类器引导的稳定性，同时保持了样本多样性和视觉质量。

Conclusion: 本文扩展了生成模型中条件采样技术的应用范围，使更多分类器可用作引导分类器。

Abstract: Classifier guidance is intended to steer a diffusion process such that a
given classifier reliably recognizes the generated data point as a certain
class. However, most classifier guidance approaches are restricted to robust
classifiers, which were specifically trained on the noise of the diffusion
forward process. We extend classifier guidance to work with general,
non-robust, classifiers that were trained without noise. We analyze the
sensitivity of both non-robust and robust classifiers to noise of the diffusion
process on the standard CelebA data set, the specialized SportBalls data set
and the high-dimensional real-world CelebA-HQ data set. Our findings reveal
that non-robust classifiers exhibit significant accuracy degradation under
noisy conditions, leading to unstable guidance gradients. To mitigate these
issues, we propose a method that utilizes one-step denoised image predictions
and implements stabilization techniques inspired by stochastic optimization
methods, such as exponential moving averages. Experimental results demonstrate
that our approach improves the stability of classifier guidance while
maintaining sample diversity and visual quality. This work contributes to
advancing conditional sampling techniques in generative models, enabling a
broader range of classifiers to be used as guidance classifiers.

</details>


### [224] [SCAWaveNet: A Spatial-Channel Attention-based Network for Global Significant Wave Height Retrieval](https://arxiv.org/abs/2507.00701)
*Chong Zhang,Xichao Liu,Yibing Zhan,Dapeng Tao,Jun Ni*

Main category: cs.LG

TL;DR: 提出了一种基于空间-通道注意力的网络SCAWaveNet，用于从GNSS数据中反演有效波高，通过多通道信息交互提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在利用CYGNSS数据时，通常采用单通道输入或简单通道拼接，未能充分利用跨通道信息交互的优势。

Method: 设计了SCAWaveNet网络，将DDMs的每个通道特征建模为独立的注意力头，融合空间和通道信息，并采用轻量级注意力机制处理辅助参数。

Result: 在ERA5和NDBC浮标数据上，SCAWaveNet的平均RMSE分别为0.438 m和0.432 m，比现有最优模型分别降低了3.52%和5.47%。

Conclusion: SCAWaveNet通过空间-通道注意力机制显著提升了有效波高反演的精度，代码已开源。

Abstract: Recent advancements in spaceborne GNSS missions have produced extensive
global datasets, providing a robust basis for deep learning-based significant
wave height (SWH) retrieval. While existing deep learning models predominantly
utilize CYGNSS data with four-channel information, they often adopt
single-channel inputs or simple channel concatenation without leveraging the
benefits of cross-channel information interaction during training. To address
this limitation, a novel spatial-channel attention-based network, namely
SCAWaveNet, is proposed for SWH retrieval. Specifically, features from each
channel of the DDMs are modeled as independent attention heads, enabling the
fusion of spatial and channel-wise information. For auxiliary parameters, a
lightweight attention mechanism is designed to assign weights along the spatial
and channel dimensions. The final feature integrates both spatial and
channel-level characteristics. Model performance is evaluated using
four-channel CYGNSS data. When ERA5 is used as a reference, SCAWaveNet achieves
an average RMSE of 0.438 m. When using buoy data from NDBC, the average RMSE
reaches 0.432 m. Compared to state-of-the-art models, SCAWaveNet reduces the
average RMSE by at least 3.52% on the ERA5 dataset and by 5.47% on the NDBC
buoy observations. The code is available at
https://github.com/Clifx9908/SCAWaveNet.

</details>


### [225] [Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories](https://arxiv.org/abs/2507.00711)
*Jhouben Cuesta-Ramirez,Samuel Beaussant,Mehdi Mounsif*

Main category: cs.LG

TL;DR: 研究发现，大型语言模型（LLMs）在推理任务中会生成冗长但无效的思维链（CoTs），甚至忽视正确解，导致错误结论。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在推理任务中是否真正提升能力，还是仅因生成冗长CoTs而获得基准测试优势。

Method: 在AIME2024数学基准上测试三种最先进模型，观察其对纠正信息的整合能力。

Result: 模型存在过度思考现象，忽视正确解，生成不必要的推理步骤，导致错误结论。

Conclusion: 研究揭示了LLMs在稳健和可解释推理方面的新挑战。

Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have
recently achieved impressive results on reasoning benchmarks. Yet, growing
evidence shows that these models often generate longer but ineffective chains
of thought (CoTs), calling into question whether benchmark gains reflect real
reasoning improvements. We present new evidence of overthinking, where models
disregard correct solutions even when explicitly provided, instead continuing
to generate unnecessary reasoning steps that often lead to incorrect
conclusions. Experiments on three state-of-the-art models using the AIME2024
math benchmark reveal critical limitations in these models ability to integrate
corrective information, posing new challenges for achieving robust and
interpretable reasoning.

</details>


### [226] [Aleatoric and Epistemic Uncertainty Measures for Ordinal Classification through Binary Reduction](https://arxiv.org/abs/2507.00733)
*Stefan Haas,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 该论文提出了一种新的序数分类不确定性度量方法，分解为固有变异性和知识不足两部分，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 序数分类问题在高风险领域（如医学和金融）中普遍存在，但现有研究主要关注名义分类和回归，缺乏对不确定性的量化。

Method: 通过将序数分类问题简化为二元分类，基于熵和方差度量，结合梯度提升树和多层感知机进行贝叶斯推断。

Result: 在多个基准数据集上，新方法在错误检测和分布外检测中表现优异，显著优于传统熵和方差度量。

Conclusion: 序数分类问题的特性在不确定性评估中至关重要，新方法为可靠决策提供了有效工具。

Abstract: Ordinal classification problems, where labels exhibit a natural order, are
prevalent in high-stakes fields such as medicine and finance. Accurate
uncertainty quantification, including the decomposition into aleatoric
(inherent variability) and epistemic (lack of knowledge) components, is crucial
for reliable decision-making. However, existing research has primarily focused
on nominal classification and regression. In this paper, we introduce a novel
class of measures of aleatoric and epistemic uncertainty in ordinal
classification, which is based on a suitable reduction to (entropy- and
variance-based) measures for the binary case. These measures effectively
capture the trade-off in ordinal classification between exact hit-rate and
minimial error distances. We demonstrate the effectiveness of our approach on
various tabular ordinal benchmark datasets using ensembles of gradient-boosted
trees and multi-layer perceptrons for approximate Bayesian inference. Our
method significantly outperforms standard and label-wise entropy and
variance-based measures in error detection, as indicated by misclassification
rates and mean absolute error. Additionally, the ordinal measures show
competitive performance in out-of-distribution (OOD) detection. Our findings
highlight the importance of considering the ordinal nature of classification
problems when assessing uncertainty.

</details>


### [227] [Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN](https://arxiv.org/abs/2507.00736)
*Arthur Thuy,Ekaterina Loginova,Dries F. Benoit*

Main category: cs.LG

TL;DR: 本文研究了问题难度估计（QDE）任务，提出了新的评估指标和模型，解决了现有方法忽略顺序性和类别不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 现有QDE方法忽视了难度级别的顺序性，且评估指标未能解决类别不平衡问题，导致性能评估偏差。

Method: 提出了平衡离散排名概率得分（DRPS）作为评估指标，并开发了OrderedLogitNN模型，结合了有序逻辑回归和神经网络。

Result: OrderedLogitNN在复杂任务上表现显著优于其他方法，平衡DRPS提供了鲁棒且公平的评估。

Conclusion: 本研究为QDE任务提供了新的模型和评估指标，为未来研究奠定了基础。

Abstract: Recent years have seen growing interest in Question Difficulty Estimation
(QDE) using natural language processing techniques. Question difficulty is
often represented using discrete levels, framing the task as ordinal regression
due to the inherent ordering from easiest to hardest. However, the literature
has neglected the ordinal nature of the task, relying on classification or
discretized regression models, with specialized ordinal regression methods
remaining unexplored. Furthermore, evaluation metrics are tightly coupled to
the modeling paradigm, hindering cross-study comparability. While some metrics
fail to account for the ordinal structure of difficulty levels, none adequately
address class imbalance, resulting in biased performance assessments. This
study addresses these limitations by benchmarking three types of model outputs
-- discretized regression, classification, and ordinal regression -- using the
balanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly
captures ordinality and class imbalance. In addition to using popular ordinal
regression methods, we propose OrderedLogitNN, extending the ordered logit
model from econometrics to neural networks. We fine-tune BERT on the RACE++ and
ARC datasets and find that OrderedLogitNN performs considerably better on
complex tasks. The balanced DRPS offers a robust and fair evaluation metric for
discrete-level QDE, providing a principled foundation for future research.

</details>


### [228] [Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports](https://arxiv.org/abs/2507.00742)
*Carlos Caminha,Maria de Lourdes M. Silva,Iago C. Chaves,Felipe T. Brito,Victor A. E. Farias,Javam C. Machado*

Main category: cs.LG

TL;DR: 论文评估了27个开源和2个专有LLM模型，使用四种提示策略，从用户文本报告中识别设备故障组件，最高f1-score达0.76。


<details>
  <summary>Details</summary>
Motivation: 用户设备故障报告通常模糊且缺乏细节，识别故障组件对自动化测试和用户体验至关重要。

Method: 研究评估了27个开源模型和2个专有模型，采用Zero-Shot、Few-Shot、Chain-of-Thought和CoT+Few-Shot四种提示策略。

Result: 最高f1-score为0.76，mistral-small-24b-instruct、llama-3.2-1b-instruct和gemma-2-2b-it在性能和效率上表现最佳。

Conclusion: 研究展示了LLM在设备故障识别中的潜力，并推荐了适合终端设备的高效模型。

Abstract: Computer manufacturers offer platforms for users to describe device faults
using textual reports such as "My screen is flickering". Identifying the faulty
component from the report is essential for automating tests and improving user
experience. However, such reports are often ambiguous and lack detail, making
this task challenging. Large Language Models (LLMs) have shown promise in
addressing such issues. This study evaluates 27 open-source models (1B-72B
parameters) and 2 proprietary LLMs using four prompting strategies: Zero-Shot,
Few-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). We conducted
98,948 inferences, processing over 51 million input tokens and generating 13
million output tokens. We achieve f1-score up to 0.76. Results show that three
models offer the best balance between size and performance:
mistral-small-24b-instruct and two smaller models, llama-3.2-1b-instruct and
gemma-2-2b-it, that offer competitive performance with lower VRAM usage,
enabling efficient inference on end-user devices as modern laptops or
smartphones with NPUs.

</details>


### [229] [A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model](https://arxiv.org/abs/2507.00761)
*Wenbo Yu,Anirbit Ghosh,Tobias Sebastian Finn,Rossella Arcucci,Marc Bocquet,Sibo Cheng*

Main category: cs.LG

TL;DR: 该研究提出了一种基于去噪扩散模型的AI框架，用于预测野火蔓延，能够生成多种可能的场景，反映火灾动态的不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统的野火蔓延预测模型通常只能提供单一确定性结果，无法反映火灾动态的固有不确定性，而生成式AI的进步为解决这一问题提供了新思路。

Method: 研究采用了一种新型的去噪扩散模型，通过学习模拟火灾的多种可能场景，而非单一固定结果，从而捕捉火灾蔓延的不确定性。

Result: 该模型能够生成反映物理意义的火灾蔓延预测集合，优于传统的确定性方法。

Conclusion: 这项技术有望为野火风险评估和响应规划提供更智能、快速和可靠的预测工具。

Abstract: Thanks to recent advances in generative AI, computers can now simulate
realistic and complex natural processes. We apply this capability to predict
how wildfires spread, a task made difficult by the unpredictable nature of fire
and the variety of environmental conditions it depends on. In this study, We
present the first denoising diffusion model for predicting wildfire spread, a
new kind of AI framework that learns to simulate fires not just as one fixed
outcome, but as a range of possible scenarios. By doing so, it accounts for the
inherent uncertainty of wildfire dynamics, a feature that traditional models
typically fail to represent. Unlike deterministic approaches that generate a
single prediction, our model produces ensembles of forecasts that reflect
physically meaningful distributions of where fire might go next. This
technology could help us develop smarter, faster, and more reliable tools for
anticipating wildfire behavior, aiding decision-makers in fire risk assessment
and response planning.

</details>


### [230] [Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments](https://arxiv.org/abs/2507.00762)
*Tom Maus,Asma Atamna,Tobias Glasmachers*

Main category: cs.LG

TL;DR: 该研究提出了一种结合遗传算法（GA）和强化学习（RL）的新方法，通过GA生成的专家演示提升RL性能，实验证明该方法显著优于传统RL训练。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在工业应用中潜力巨大，但其广泛部署受限于样本效率低和学习动态不稳定等问题。本研究旨在探索如何利用遗传算法改进RL性能。

Method: 研究提出了一种新方法，将GA生成的专家演示用于增强策略学习，包括将其纳入DQN的回放缓冲区和作为PPO代理的热启动轨迹。

Result: 实验表明，GA生成的演示显著提升了RL性能，尤其是PPO代理在初始化时使用GA数据后获得了更高的累积奖励。

Conclusion: 该研究展示了启发式搜索方法与数据驱动RL结合的潜力，为实际应用中的自适应RL策略提供了新思路。

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in certain
real-world industrial applications, yet its broader deployment remains limited
by inherent challenges such as sample inefficiency and unstable learning
dynamics. This study investigates the utilization of Genetic Algorithms (GAs)
as a mechanism for improving RL performance in an industrially inspired sorting
environment. We propose a novel approach in which GA-generated expert
demonstrations are used to enhance policy learning. These demonstrations are
incorporated into a Deep Q-Network (DQN) replay buffer for experience-based
learning and utilized as warm-start trajectories for Proximal Policy
Optimization (PPO) agents to accelerate training convergence. Our experiments
compare standard RL training with rule-based heuristics, brute-force
optimization, and demonstration data, revealing that GA-derived demonstrations
significantly improve RL performance. Notably, PPO agents initialized with
GA-generated data achieved superior cumulative rewards, highlighting the
potential of hybrid learning paradigms, where heuristic search methods
complement data-driven RL. The utilized framework is publicly available and
enables further research into adaptive RL strategies for real-world
applications.

</details>


### [231] [BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation](https://arxiv.org/abs/2507.00846)
*Rishal Aggrwal,Jacky Chen,Nicholas M. Boffi,David Ryan Koes*

Main category: cs.LG

TL;DR: 论文提出了一种通过能量模型和噪声对比估计学习生成分布似然的方法，避免了计算昂贵的雅可比矩阵，显著提高了采样效率。


<details>
  <summary>Details</summary>
Motivation: 从玻尔兹曼分布中高效采样是建模物理系统（如分子）的关键挑战，传统方法因计算雅可比矩阵成本高而不适用于大分子系统。

Method: 利用能量模型和噪声对比估计学习生成分布的似然，结合随机插值在先验和生成分布之间退火，高效学习密度函数。

Result: 在丙氨酸二肽系统中，该方法生成的自由能分布与精确似然方法相当，且能以数量级的速度准确估计亚稳态间的自由能差。

Conclusion: 该方法显著提高了采样效率，适用于大分子系统，为物理系统建模提供了实用工具。

Abstract: Efficient sampling from the Boltzmann distribution defined by an energy
function is a key challenge in modeling physical systems such as molecules.
Boltzmann Generators tackle this by leveraging Continuous Normalizing Flows
that transform a simple prior into a distribution that can be reweighted to
match the Boltzmann distribution using sample likelihoods. However, obtaining
likelihoods requires computing costly Jacobians during integration, making it
impractical for large molecular systems. To overcome this, we propose learning
the likelihood of the generated distribution via an energy-based model trained
with noise contrastive estimation and score matching. By using stochastic
interpolants to anneal between the prior and generated distributions, we
combine both the objective functions to efficiently learn the density function.
On the alanine dipeptide system, we demonstrate that our method yields free
energy profiles and energy distributions comparable to those obtained with
exact likelihoods. Additionally, we show that free energy differences between
metastable states can be estimated accurately with orders-of-magnitude speedup.

</details>


### [232] [Quantum Approximate Optimization Algorithm for Spatiotemporal Forecasting of HIV Clusters](https://arxiv.org/abs/2507.00848)
*Don Roosan,Saif Nirzhor,Rubayat Khan,Fahmida Hai,Mohammad Rifat Haidar*

Main category: cs.LG

TL;DR: 论文利用量子加速机器学习分析HIV流行数据，在聚类检测和预测中表现优于传统方法，并揭示了社会因素与HIV传播的因果关系。


<details>
  <summary>Details</summary>
Motivation: HIV流行病数据日益复杂，需要先进计算方法以提高聚类检测和预测的准确性。

Method: 结合量子近似优化算法（QAOA）与经典聚类方法（DBSCAN、HDBSCAN），开发混合量子-经典神经网络预测HIV流行率，并使用量子贝叶斯网络分析社会因素与HIV发病率的因果关系。

Result: QAOA方法在1.6秒内实现92%的聚类检测准确率；混合神经网络预测准确率达94%；量子贝叶斯分析发现住房不稳定是HIV传播的关键驱动因素。

Conclusion: 量子增强方法提升了HIV监测的精度和效率，为针对性干预和资源优化提供了依据。

Abstract: HIV epidemiological data is increasingly complex, requiring advanced
computation for accurate cluster detection and forecasting. We employed
quantum-accelerated machine learning to analyze HIV prevalence at the ZIP-code
level using AIDSVu and synthetic SDoH data for 2022. Our approach compared
classical clustering (DBSCAN, HDBSCAN) with a quantum approximate optimization
algorithm (QAOA), developed a hybrid quantum-classical neural network for HIV
prevalence forecasting, and used quantum Bayesian networks to explore causal
links between SDoH factors and HIV incidence. The QAOA-based method achieved
92% accuracy in cluster detection within 1.6 seconds, outperforming classical
algorithms. Meanwhile, the hybrid quantum-classical neural network predicted
HIV prevalence with 94% accuracy, surpassing a purely classical counterpart.
Quantum Bayesian analysis identified housing instability as a key driver of HIV
cluster emergence and expansion, with stigma exerting a geographically variable
influence. These quantum-enhanced methods deliver greater precision and
efficiency in HIV surveillance while illuminating critical causal pathways.
This work can guide targeted interventions, optimize resource allocation for
PrEP, and address structural inequities fueling HIV transmission.

</details>


### [233] [Aligning Learning and Endogenous Decision-Making](https://arxiv.org/abs/2507.00851)
*Rares Cristian,Pavithra Harsha,Georgia Perakis,Brian Quanz*

Main category: cs.LG

TL;DR: 论文提出了一种端到端方法，用于训练机器学习模型以应对内生不确定性，并引入鲁棒优化变体以处理模型不确定性。通过实验验证了方法在定价和库存推荐问题上的优越性。


<details>
  <summary>Details</summary>
Motivation: 许多观察结果受到决策的偏见影响，缺乏反事实信息，需要学习这些信息以改进决策。

Method: 提出端到端方法训练模型，引入鲁棒优化变体处理不确定性，并扩展了两阶段随机优化问题框架。

Result: 实验表明，该方法在定价和库存推荐问题上优于现有方法。

Conclusion: 该方法能有效应对内生不确定性，并在决策阶段提供更优表现。

Abstract: Many of the observations we make are biased by our decisions. For instance,
the demand of items is impacted by the prices set, and online checkout choices
are influenced by the assortments presented. The challenge in decision-making
under this setting is the lack of counterfactual information, and the need to
learn it instead. We introduce an end-to-end method under endogenous
uncertainty to train ML models to be aware of their downstream, enabling their
effective use in the decision-making stage. We further introduce a robust
optimization variant that accounts for uncertainty in ML models -- specifically
by constructing uncertainty sets over the space of ML models and optimizing
actions to protect against worst-case predictions. We prove guarantees that
this robust approach can capture near-optimal decisions with high probability
as a function of data. Besides this, we also introduce a new class of two-stage
stochastic optimization problems to the end-to-end learning framework that can
now be addressed through our framework. Here, the first stage is an
information-gathering problem to decide which random variable to poll and gain
information about before making a second-stage decision based off of it. We
present several computational experiments for pricing and inventory
assortment/recommendation problems. We compare against existing methods in
online learning/bandits/offline reinforcement learning and show our approach
has consistent improved performance over these. Just as in the endogenous
setting, the model's prediction also depends on the first-stage decision made.
While this decision does not affect the random variable in this setting, it
does affect the correct point forecast that should be made.

</details>


### [234] [Machine Learning-based Early Detection of Potato Sprouting Using Electrophysiological Signals](https://arxiv.org/abs/2507.00862)
*Davide Andreoletti,Aris Marcolongo,Natasa Sarafijanovic Djukic,Julien Roulet,Stefano Billeter,Andrzej Kurenda,Margot Visse-Mansiaux,Brice Dupuis,Carrol Annette Plummer,Beatrice Paoli,Omran Ayoub*

Main category: cs.LG

TL;DR: 提出了一种基于机器学习的早期马铃薯发芽预测方法，利用电生理信号进行检测，优于传统视觉方法。


<details>
  <summary>Details</summary>
Motivation: 马铃薯发芽会降低其商业和营养价值，而传统视觉检测方法滞后，无法提前干预。禁用CIPC后，更昂贵的替代化学品需要更精准的预测以减少浪费和成本。

Method: 通过专有传感器记录马铃薯的电生理信号，预处理后提取小波域特征，训练监督学习模型，并加入不确定性量化技术。

Result: 实验显示该方法能准确预测部分马铃薯的发芽日期，整体平均误差可接受，但最大偏差仍需优化。

Conclusion: 该方法为早期发芽预测提供了有效工具，但需进一步减少预测误差以提高实用性。

Abstract: Accurately predicting potato sprouting before the emergence of any visual
signs is critical for effective storage management, as sprouting degrades both
the commercial and nutritional value of tubers. Effective forecasting allows
for the precise application of anti-sprouting chemicals (ASCs), minimizing
waste and reducing costs. This need has become even more pressing following the
ban on Isopropyl N-(3-chlorophenyl) carbamate (CIPC) or Chlorpropham due to
health and environmental concerns, which has led to the adoption of
significantly more expensive alternative ASCs. Existing approaches primarily
rely on visual identification, which only detects sprouting after morphological
changes have occurred, limiting their effectiveness for proactive management. A
reliable early prediction method is therefore essential to enable timely
intervention and improve the efficiency of post-harvest storage strategies,
where early refers to detecting sprouting before any visible signs appear. In
this work, we address the problem of early prediction of potato sprouting. To
this end, we propose a novel machine learning (ML)-based approach that enables
early prediction of potato sprouting using electrophysiological signals
recorded from tubers using proprietary sensors. Our approach preprocesses the
recorded signals, extracts relevant features from the wavelet domain, and
trains supervised ML models for early sprouting detection. Additionally, we
incorporate uncertainty quantification techniques to enhance predictions.
Experimental results demonstrate promising performance in the early detection
of potato sprouting by accurately predicting the exact day of sprouting for a
subset of potatoes and while showing acceptable average error across all
potatoes. Despite promising results, further refinements are necessary to
minimize prediction errors, particularly in reducing the maximum observed
deviations.

</details>


### [235] [NN-Former: Rethinking Graph Structure in Neural Architecture Representation](https://arxiv.org/abs/2507.00880)
*Ruihan Xu,Haokui Zhang,Yaowei Wang,Wei Zeng,Shiliang Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种结合GNN和transformer优势的新型预测器，通过改进拓扑表示（如考虑兄弟节点）和引入新的混合器，显著提升了神经网络架构的准确性和延迟预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前GNN和transformer在表示神经网络架构时各有不足，GNN难以捕捉复杂特征，transformer在架构深度增加时泛化能力差。论文旨在解决这些问题。

Method: 提出了一种新型预测器，结合GNN和transformer的优势，引入考虑兄弟节点的token mixer和双向图同构前馈网络（channel mixer）。

Result: 该方法在准确性和延迟预测方面表现优异，为学习DAG拓扑提供了新思路。

Conclusion: 通过改进拓扑表示和混合器设计，论文成功提升了神经网络架构预测的性能，代码已开源。

Abstract: The growing use of deep learning necessitates efficient network design and
deployment, making neural predictors vital for estimating attributes such as
accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers
have shown promising performance in representing neural architectures. However,
each of both methods has its disadvantages. GNNs lack the capabilities to
represent complicated features, while transformers face poor generalization
when the depth of architecture grows. To mitigate the above issues, we rethink
neural architecture topology and show that sibling nodes are pivotal while
overlooked in previous research. We thus propose a novel predictor leveraging
the strengths of GNNs and transformers to learn the enhanced topology. We
introduce a novel token mixer that considers siblings, and a new channel mixer
named bidirectional graph isomorphism feed-forward network. Our approach
consistently achieves promising performance in both accuracy and latency
prediction, providing valuable insights for learning Directed Acyclic Graph
(DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.

</details>


### [236] [TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality](https://arxiv.org/abs/2507.00899)
*Carlos Vonessen,Charles Harris,Miruna Cretu,Pietro Liò*

Main category: cs.LG

TL;DR: TABASCO是一种非等变变换器模型，通过简化架构和提升数据吞吐量，在3D分子生成任务中实现了高效且物理可信的结果。


<details>
  <summary>Details</summary>
Motivation: 现有3D分子生成模型依赖强归纳偏置（如SE(3)等变性），但仍难以保证物理可信性，因此需要更简化的方法。

Method: TABASCO采用标准非等变变换器架构，将分子原子视为序列生成，并在生成后确定性重建化学键。

Result: 在GEOM-Drugs基准测试中，TABASCO在PoseBusters有效性上达到最优，推理速度比基线快10倍，且展现出未硬编码的旋转等变性。

Conclusion: TABASCO为高效、简化的生成模型提供了范例，适用于药物设计等专业任务。

Abstract: State-of-the-art models for 3D molecular generation are based on significant
inductive biases, SE(3), permutation equivariance to respect symmetry and graph
message-passing networks to capture local chemistry, yet the generated
molecules still struggle with physical plausibility. We introduce TABASCO which
relaxes these assumptions: The model has a standard non-equivariant transformer
architecture, treats atoms in a molecule as sequences and reconstructs bonds
deterministically after generation. The absence of equivariant layers and
message passing allows us to significantly simplify the model architecture and
scale data throughput. On the GEOM-Drugs benchmark TABASCO achieves
state-of-the-art PoseBusters validity and delivers inference roughly 10x faster
than the strongest baseline, while exhibiting emergent rotational equivariance
despite symmetry not being hard-coded. Our work offers a blueprint for training
minimalist, high-throughput generative models suited to specialised tasks such
as structure- and pharmacophore-based drug design. We provide a link to our
implementation at github.com/carlosinator/tabasco.

</details>


### [237] [Privacy-Preserving Quantized Federated Learning with Diverse Precision](https://arxiv.org/abs/2507.00920)
*Dang Qua Nguyen,Morteza Hashemi,Erik Perrins,Sergiy A. Vorobyov,David J. Love,Taejoon Kim*

Main category: cs.LG

TL;DR: 本文提出了一种新型随机量化器（SQ），旨在同时实现差分隐私（DP）和最小量化误差，解决了联邦学习中隐私风险和量化异构性的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）在分布式机器学习中具有潜力，但面临隐私风险和量化异构性导致的效用下降问题。本文旨在提升隐私保护FL的学习效用。

Method: 引入新型随机量化器（SQ）保证差分隐私和最小量化误差，并提出聚类大小优化技术和线性融合方法以解决量化异构性。

Result: 数值模拟表明，相比传统LaplaceSQ-FL算法，该方法在隐私保护和学习效用方面表现更优。

Conclusion: 本文方法有效解决了FL中的隐私和量化异构性问题，提升了学习效用。

Abstract: Federated learning (FL) has emerged as a promising paradigm for distributed
machine learning, enabling collaborative training of a global model across
multiple local devices without requiring them to share raw data. Despite its
advancements, FL is limited by factors such as: (i) privacy risks arising from
the unprotected transmission of local model updates to the fusion center (FC)
and (ii) decreased learning utility caused by heterogeneity in model
quantization resolution across participating devices. Prior work typically
addresses only one of these challenges because maintaining learning utility
under both privacy risks and quantization heterogeneity is a non-trivial task.
In this paper, our aim is therefore to improve the learning utility of a
privacy-preserving FL that allows clusters of devices with different
quantization resolutions to participate in each FL round. Specifically, we
introduce a novel stochastic quantizer (SQ) that is designed to simultaneously
achieve differential privacy (DP) and minimum quantization error. Notably, the
proposed SQ guarantees bounded distortion, unlike other DP approaches. To
address quantization heterogeneity, we introduce a cluster size optimization
technique combined with a linear fusion approach to enhance model aggregation
accuracy. Numerical simulations validate the benefits of our approach in terms
of privacy protection and learning utility compared to the conventional
LaplaceSQ-FL algorithm.

</details>


### [238] [Understanding Generalization in Node and Link Prediction](https://arxiv.org/abs/2507.00927)
*Antonis Vasileiou,Timo Stoll,Christopher Morris*

Main category: cs.LG

TL;DR: 本文提出了一个统一框架，用于分析消息传递图神经网络（MPNNs）在节点和链接预测任务中的泛化能力，考虑了多种架构参数和损失函数，并量化了图结构的影响。


<details>
  <summary>Details</summary>
Motivation: MPNNs在节点和链接预测任务中广泛应用，但其泛化能力尚未被充分理解，尤其是在非独立同分布（non-i.i.d.）假设下。

Method: 引入一个统一框架，分析MPNNs在归纳和传导性节点及链接预测任务中的泛化性能，结合多种架构参数和损失函数，并量化图结构的影响。

Result: 实证研究支持理论分析，深化了对MPNNs在这些任务中泛化能力的理解。

Conclusion: 该框架不仅适用于图数据，还可扩展到其他分类任务，为MPNNs的泛化性能提供了新的理论支持。

Abstract: Using message-passing graph neural networks (MPNNs) for node and link
prediction is crucial in various scientific and industrial domains, which has
led to the development of diverse MPNN architectures. Besides working well in
practical settings, their ability to generalize beyond the training set remains
poorly understood. While some studies have explored MPNNs' generalization in
graph-level prediction tasks, much less attention has been given to node- and
link-level predictions. Existing works often rely on unrealistic i.i.d.\@
assumptions, overlooking possible correlations between nodes or links, and
assuming fixed aggregation and impractical loss functions while neglecting the
influence of graph structure. In this work, we introduce a unified framework to
analyze the generalization properties of MPNNs in inductive and transductive
node and link prediction settings, incorporating diverse architectural
parameters and loss functions and quantifying the influence of graph structure.
Additionally, our proposed generalization framework can be applied beyond
graphs to any classification task under the inductive or transductive setting.
Our empirical study supports our theoretical insights, deepening our
understanding of MPNNs' generalization capabilities in these tasks.

</details>


### [239] [Time Series Foundation Models are Flow Predictors](https://arxiv.org/abs/2507.00945)
*Massimiliano Luca,Ciro Beneduce,Bruno Lepri*

Main category: cs.LG

TL;DR: 研究了时间序列基础模型（TSFMs）在人群流量预测中的有效性，重点评估了Moirai和TimesFM模型。在零样本设置下，这些模型在三个真实数据集上表现优于统计和深度学习基线。


<details>
  <summary>Details</summary>
Motivation: 探索TSFMs在缺乏空间信息或标注数据有限的情况下，能否实现准确且可扩展的人群流量预测。

Method: 在零样本设置下，仅利用时间序列数据（无空间信息），评估Moirai和TimesFM在三个真实数据集（Bike NYC、Taxi Beijing、西班牙OD流量）上的表现。

Result: Moirai和TimesFM在RMSE、MAE和CPC指标上显著优于基线模型，最高提升33% RMSE、39% MAE和49% CPC。

Conclusion: TSFMs在人群流量预测中具有实际价值，尤其在数据有限或缺乏空间信息的场景下。

Abstract: We investigate the effectiveness of time series foundation models (TSFMs) for
crowd flow prediction, focusing on Moirai and TimesFM. Evaluated on three
real-world mobility datasets-Bike NYC, Taxi Beijing, and Spanish national OD
flows-these models are deployed in a strict zero-shot setting, using only the
temporal evolution of each OD flow and no explicit spatial information. Moirai
and TimesFM outperform both statistical and deep learning baselines, achieving
up to 33% lower RMSE, 39% lower MAE and up to 49% higher CPC compared to
state-of-the-art competitors. Our results highlight the practical value of
TSFMs for accurate, scalable flow prediction, even in scenarios with limited
annotated data or missing spatial context.

</details>


### [240] [Benchmarking the Discovery Engine](https://arxiv.org/abs/2507.00964)
*Jack Foxabbott,Arush Tagade,Andrew Cusick,Robbie McCorkell,Leo McKee-Reid,Jugal Patel,Jamie Rumbelow,Jessica Rumbelow,Zohreh Shams*

Main category: cs.LG

TL;DR: Discovery Engine是一个结合机器学习和可解释性的自动化科学发现系统，在多个领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 推动科学发现的自动化，提供更快速、更可解释的模型。

Method: 结合机器学习和可解释性技术，在多个领域的已有研究上进行基准测试。

Result: 在预测性能和可解释性上均优于或匹配现有研究。

Conclusion: 该系统有望成为自动化、可解释科学建模的新标准。

Abstract: The Discovery Engine is a general purpose automated system for scientific
discovery, which combines machine learning with state-of-the-art ML
interpretability to enable rapid and robust scientific insight across diverse
datasets. In this paper, we benchmark the Discovery Engine against five recent
peer-reviewed scientific publications applying machine learning across
medicine, materials science, social science, and environmental science. In each
case, the Discovery Engine matches or exceeds prior predictive performance
while also generating deeper, more actionable insights through rich
interpretability artefacts. These results demonstrate its potential as a new
standard for automated, interpretable scientific modelling that enables complex
knowledge discovery from data.

</details>


### [241] [Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning](https://arxiv.org/abs/2507.00965)
*Félix Lefebvre,Gaël Varoquaux*

Main category: cs.LG

TL;DR: SEPAL是一种可扩展的嵌入传播算法，用于大规模知识图谱，通过优化核心实体嵌入并通过消息传递传播，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法主要针对链接预测优化，且难以扩展到大型图谱。SEPAL旨在解决这些问题，为下游任务提供高质量嵌入。

Method: SEPAL通过优化核心实体嵌入并通过消息传递传播到整个图谱，实现全局嵌入对齐。

Result: 在7个大规模知识图谱和46个下游任务上，SEPAL显著优于现有方法，并能扩展到大型图谱。

Conclusion: SEPAL是一种高效、可扩展的知识图谱嵌入方法，适用于下游任务。

Abstract: Many machine learning tasks can benefit from external knowledge. Large
knowledge graphs store such knowledge, and embedding methods can be used to
distill it into ready-to-use vector representations for downstream
applications. For this purpose, current models have however two limitations:
they are primarily optimized for link prediction, via local contrastive
learning, and they struggle to scale to the largest graphs due to GPU memory
limits. To address these, we introduce SEPAL: a Scalable Embedding Propagation
ALgorithm for large knowledge graphs designed to produce high-quality
embeddings for downstream tasks at scale. The key idea of SEPAL is to enforce
global embedding alignment by optimizing embeddings only on a small core of
entities, and then propagating them to the rest of the graph via message
passing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream
machine learning tasks. Our results show that SEPAL significantly outperforms
previous methods on downstream tasks. In addition, SEPAL scales up its base
embedding model, enabling fitting huge knowledge graphs on commodity hardware.

</details>


### [242] [Reasoning as an Adaptive Defense for Safety](https://arxiv.org/abs/2507.00971)
*Taeyoun Kim,Fahim Tajwar,Aditi Raghunathan,Aviral Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种名为TARS的方法，通过强化学习训练模型在安全性和任务完成之间平衡，提升LLM对安全漏洞的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用自适应分配测试计算的方法，训练模型以应对安全漏洞，并展示其优势。

Method: 采用强化学习（RL）方法，结合链式思维追踪和奖励信号，设计了三个关键选择：轻量级SFT阶段、混合提示和防止推理能力退化的奖励函数。

Result: TARS训练的模型在模糊查询上分配更多计算，实现了更好的安全性与拒绝的平衡，并提升了对抗攻击的鲁棒性。

Conclusion: TARS为训练LLM抵御越狱和有害请求提供了一种有效的开放式方法。

Abstract: Reasoning methods that adaptively allocate test-time compute have advanced
LLM performance on easy to verify domains such as math and code. In this work,
we study how to utilize this approach to train models that exhibit a degree of
robustness to safety vulnerabilities, and show that doing so can provide
benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners
for Safety), a reinforcement learning (RL) approach that trains models to
reason about safety using chain-of-thought traces and a reward signal that
balances safety with task completion. To build TARS, we identify three critical
design choices: (1) a "lightweight" warmstart SFT stage, (2) a mix of harmful,
harmless, and ambiguous prompts to prevent shortcut behaviors such as too many
refusals, and (3) a reward function to prevent degeneration of reasoning
capabilities during training. Models trained with TARS exhibit adaptive
behaviors by spending more compute on ambiguous queries, leading to better
safety-refusal trade-offs. They also internally learn to better distinguish
between safe and unsafe prompts and attain greater robustness to both white-box
(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an
effective, open recipe for training LLMs against jailbreaks and harmful
requests by reasoning per prompt.

</details>


### [243] [Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes](https://arxiv.org/abs/2507.01003)
*Eun-Ji Park,Sangwon Yun*

Main category: cs.LG

TL;DR: 论文提出了一种通过随机梯度下降理解和加速深度神经网络训练的统一框架，引入Lyapunov指数诊断，并提出幽灵类别扩展以优化训练过程。


<details>
  <summary>Details</summary>
Motivation: 从遍历性角度解释训练过程，解决神经网络训练中收敛与统计稳定性的区分问题，以及早期训练阶段的优化难题。

Method: 分析目标函数的几何景观，引入Lyapunov指数诊断；提出幽灵类别扩展，增加辅助输出节点以绕过窄损失屏障。

Result: 幽灵扩展严格减少近似误差，且在收敛后幽灵维度消失，模型行为与原模型一致。

Conclusion: 提供了一种架构层面的干预方法，加速早期训练同时保持渐近行为。

Abstract: Recent studies have proposed interpreting the training process from an
ergodic perspective. Building on this foundation we present a unified framework
for understanding and accelerating the training of deep neural networks via
stochastic gradient descent. By analyzing the geometric landscape of the
objective function we introduce a practical diagnostic, the running estimate of
the largest Lyapunov exponent, which provably distinguishes genuine convergence
toward stable minimizers from mere statistical stabilization near saddle
points. We then propose a ghost category extension for standard classifiers
that adds auxiliary ghost output nodes so the model gains extra descent
directions that open a lateral corridor around narrow loss barriers and enable
the optimizer to bypass poor basins during the early training phase. We show
that this extension strictly reduces approximation error and that after
sufficient convergence the ghost dimensions collapse and the extended model's
invariant law coincides with that of the original and there exists a path in
the enlarged parameter space along which the total loss does not increase while
the original loss decreases by an arbitrary margin. Taken together these
results provide a principled architecture level intervention that accelerates
early stage trainability while preserving asymptotic behavior.

</details>


### [244] [ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention](https://arxiv.org/abs/2507.01004)
*Yuhong Chou,Zehao Liu,Ruijie Zhu,Xinyi Wan,Tianjian Li,Congying Chu,Qian Liu,Jibin Wu,Zejun Ma*

Main category: cs.LG

TL;DR: ZeCO是一种新型序列并行方法，通过All-Scan通信原语消除通信开销，显著提升线性注意力模型的长序列训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有序列并行方法因高通信开销成为瓶颈，限制了线性注意力模型处理超长序列的能力。

Method: 提出ZeCO序列并行方法，利用All-Scan通信原语减少通信开销，实现近线性扩展。

Result: 在256 GPU上处理8M序列时，ZeCO比现有方法快60%，理论证明其最优性。

Conclusion: ZeCO为高效训练下一代LLM提供了可行路径，解决了长序列训练的难题。

Abstract: Linear attention mechanisms deliver significant advantages for Large Language
Models (LLMs) by providing linear computational complexity, enabling efficient
processing of ultra-long sequences (e.g., 1M context). However, existing
Sequence Parallelism (SP) methods, essential for distributing these workloads
across devices, become the primary bottleneck due to substantial communication
overhead. In this paper, we introduce ZeCO (Zero Communication Overhead)
sequence parallelism for linear attention models, a new SP method designed to
overcome these limitations and achieve end-to-end near-linear scalability for
long sequence training. For example, training a model with a 1M sequence length
across 64 devices using ZeCO takes roughly the same time as training with an
16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new
collective communication primitive. All-Scan provides each SP rank with
precisely the initial operator state it requires while maintaining a minimal
communication footprint, effectively eliminating communication overhead.
Theoretically, we prove the optimaity of ZeCO, showing that it introduces only
negligible time and space overhead. Empirically, we compare the communication
costs of different sequence parallelism strategies and demonstrate that
All-Scan achieves the fastest communication in SP scenarios. Specifically, on
256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to
the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a
clear path toward efficiently training next-generation LLMs on previously
intractable sequence lengths.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [245] [Novel Design of 3D Printed Tumbling Microrobots for in vivo Targeted Drug Delivery](https://arxiv.org/abs/2507.00166)
*Aaron C. Davis,Siting Zhang,Adalyn Meeks,Diya Sakhrani,Luis Carlos Sanjuan Acosta,D. Ethan Kelley,Emma Caldwell,Luis Solorio,Craig J. Goergen,David J. Cappelleri*

Main category: cs.RO

TL;DR: 本文介绍了用于体内靶向药物递送的3D打印翻滚微型机器人的创新设计，通过旋转磁场驱动，并评估了其性能、药物加载方法和生物相容性。


<details>
  <summary>Details</summary>
Motivation: 解决现有翻滚微型机器人在靶向药物递送中的局限性，推动大肠道内药物递送技术的发展。

Method: 采用立体光刻3D打印技术设计微型机器人，结合微磁铁驱动，通过实验评估不同几何形状、驱动频率和环境条件下的性能。

Result: 结果表明设计的微型机器人具有鲁棒性和适应性，适用于高效的靶向药物递送。

Conclusion: 该研究为体内靶向药物递送提供了新方法，尤其适用于大肠道内的应用。

Abstract: This paper presents innovative designs for 3D-printed tumbling microrobots,
specifically engineered for targeted in vivo drug delivery applications. The
microrobot designs, created using stereolithography 3D printing technologies,
incorporate permanent micro-magnets to enable actuation via a rotating magnetic
field actuator system. The experimental framework encompasses a series of
locomotion characterization tests to evaluate microrobot performance under
various conditions. Testing variables include variations in microrobot
geometries, actuation frequencies, and environmental conditions, such as dry
and wet environments, and temperature changes. The paper outlines designs for
three drug loading methods, along with comprehensive assessments thermal drug
release using a focused ultrasound system, as well as biocompatibility tests.
Animal model testing involves tissue phantoms and in vivo rat models, ensuring
a thorough evaluation of the microrobots' performance and compatibility. The
results highlight the robustness and adaptability of the proposed microrobot
designs, showcasing the potential for efficient and targeted in vivo drug
delivery. This novel approach addresses current limitations in existing
tumbling microrobot designs and paves the way for advancements in targeted drug
delivery within the large intestine.

</details>


### [246] [Rethink 3D Object Detection from Physical World](https://arxiv.org/abs/2507.00190)
*Satoshi Tanaka,Koji Minoda,Fumiya Watanabe,Takamasa Horibe*

Main category: cs.RO

TL;DR: 论文提出两种新指标L-AP和P-AP，用于更全面地评估实时3D物体检测，并展示了其在自动驾驶系统中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分解决速度与精度之间的权衡，且忽略了硬件差异和碰撞避免的影响。

Method: 引入L-AP和P-AP指标，结合nuPlan数据集评估模型，并通过L-HPO优化性能。

Result: 证明了新指标的有效性，并优化了硬件和模型选择。

Conclusion: 新指标为实时3D物体检测提供了更全面的评估方法。

Abstract: High-accuracy and low-latency 3D object detection is essential for autonomous
driving systems. While previous studies on 3D object detection often evaluate
performance based on mean average precision (mAP) and latency, they typically
fail to address the trade-off between speed and accuracy, such as 60.0 mAP at
100 ms vs 61.0 mAP at 500 ms. A quantitative assessment of the trade-offs
between different hardware devices and accelerators remains unexplored, despite
being critical for real-time applications. Furthermore, they overlook the
impact on collision avoidance in motion planning, for example, 60.0 mAP leading
to safer motion planning or 61.0 mAP leading to high-risk motion planning. In
this paper, we introduce latency-aware AP (L-AP) and planning-aware AP (P-AP)
as new metrics, which consider the physical world such as the concept of time
and physical constraints, offering a more comprehensive evaluation for
real-time 3D object detection. We demonstrate the effectiveness of our metrics
for the entire autonomous driving system using nuPlan dataset, and evaluate 3D
object detection models accounting for hardware differences and accelerators.
We also develop a state-of-the-art performance model for real-time 3D object
detection through latency-aware hyperparameter optimization (L-HPO) using our
metrics. Additionally, we quantitatively demonstrate that the assumption "the
more point clouds, the better the recognition performance" is incorrect for
real-time applications and optimize both hardware and model selection using our
metrics.

</details>


### [247] [Sim2Real Diffusion: Learning Cross-Domain Adaptive Representations for Transferable Autonomous Driving](https://arxiv.org/abs/2507.00236)
*Chinmay Vilas Samak,Tanmay Vilas Samak,Bing Li,Venkat Krovi*

Main category: cs.RO

TL;DR: 提出了一种基于条件潜在扩散模型的统一框架，用于解决自动驾驶算法从仿真到现实的转移问题，显著缩小了感知差距。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以全面满足自动驾驶领域在条件域适应、鲁棒性、模块化和实时性等方面的需求。

Method: 采用条件潜在扩散模型，结合基础模型、少样本微调管道以及文本和图像提示，实现跨域自适应表示学习。

Result: 实验表明，该框架能将感知差距缩小40%以上，并通过行为克隆案例验证了其有效性。

Conclusion: 生成扩散模型在仿真到现实转移中具有潜力，为更鲁棒和自适应的自动驾驶提供了途径。

Abstract: Simulation-based design, optimization, and validation of autonomous driving
algorithms have proven to be crucial for their iterative improvement over the
years. Nevertheless, the ultimate measure of effectiveness is their successful
transition from simulation to reality (sim2real). However, existing sim2real
transfer methods struggle to comprehensively address the autonomy-oriented
requirements of balancing: (i) conditioned domain adaptation, (ii) robust
performance with limited examples, (iii) modularity in handling multiple domain
representations, and (iv) real-time performance. To alleviate these pain
points, we present a unified framework for learning cross-domain adaptive
representations for sim2real transferable autonomous driving algorithms using
conditional latent diffusion models. Our framework offers options to leverage:
(i) alternate foundation models, (ii) a few-shot fine-tuning pipeline, and
(iii) textual as well as image prompts for mapping across given source and
target domains. It is also capable of generating diverse high-quality samples
when diffusing across parameter spaces such as times of day, weather
conditions, seasons, and operational design domains. We systematically analyze
the presented framework and report our findings in the form of critical
quantitative metrics and ablation studies, as well as insightful qualitative
examples and remarks. Additionally, we demonstrate the serviceability of the
proposed approach in bridging the sim2real gap for end-to-end autonomous
driving using a behavioral cloning case study. Our experiments indicate that
the proposed framework is capable of bridging the perceptual sim2real gap by
over 40%. We hope that our approach underscores the potential of generative
diffusion models in sim2real transfer, offering a pathway toward more robust
and adaptive autonomous driving.

</details>


### [248] [Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems](https://arxiv.org/abs/2507.00268)
*Oren Fivel,Matan Rudman,Kobi Cohen*

Main category: cs.RO

TL;DR: 提出了一种新型控制优化的深度强化学习框架，显式建模并补偿动作执行不匹配问题，提升实际应用中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法假设动作完美执行，忽略了实际系统中动作执行的不确定性，导致性能下降。

Method: 采用两阶段过程：确定期望动作并选择控制信号以确保正确执行，训练中考虑动作不匹配和控制器修正。

Result: 在五个开源机械仿真环境中验证，框架表现出对不确定性的鲁棒性，适用于控制导向应用。

Conclusion: 该框架填补了理想化学习与实际实现之间的差距，为工程实践提供了实用高效的解决方案。

Abstract: Deep reinforcement learning (DRL) has become a powerful tool for complex
decision-making in machine learning and AI. However, traditional methods often
assume perfect action execution, overlooking the uncertainties and deviations
between an agent's selected actions and the actual system response. In
real-world applications, such as robotics, mechatronics, and communication
networks, execution mismatches arising from system dynamics, hardware
constraints, and latency can significantly degrade performance. This work
advances AI by developing a novel control-optimized DRL framework that
explicitly models and compensates for action execution mismatches, a challenge
largely overlooked in existing methods. Our approach establishes a structured
two-stage process: determining the desired action and selecting the appropriate
control signal to ensure proper execution. It trains the agent while accounting
for action mismatches and controller corrections. By incorporating these
factors into the training process, the AI agent optimizes the desired action
with respect to both the actual control signal and the intended outcome,
explicitly considering execution errors. This approach enhances robustness,
ensuring that decision-making remains effective under real-world uncertainties.
Our approach offers a substantial advancement for engineering practice by
bridging the gap between idealized learning and real-world implementation. It
equips intelligent agents operating in engineering environments with the
ability to anticipate and adjust for actuation errors and system disturbances
during training. We evaluate the framework in five widely used open-source
mechanical simulation environments we restructured and developed to reflect
real-world operating conditions, showcasing its robustness against
uncertainties and offering a highly practical and efficient solution for
control-oriented applications.

</details>


### [249] [Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems](https://arxiv.org/abs/2507.00443)
*Reza Ahmadvand,Sarah Safura Sharif,Yaser Mike Banad*

Main category: cs.RO

TL;DR: 提出了一种受鱼类和鸽子群体行为启发的多无人机系统碰撞避免编队控制方法，结合集中式和分布式控制策略，并在2D和3D环境中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 城市环境中多无人机系统的需求增加，但面临静态和动态障碍物的挑战，需要一种有效的碰撞避免编队控制方法。

Method: 采用半分布式控制框架，集中式算法优化无人机位置，分布式控制实现碰撞和障碍物避免，并扩展到3D空间。

Result: 在2D和3D动态环境中，方法成功实现了无人机编队控制和障碍物避免。

Conclusion: 提出的方法在多无人机系统中有效，适用于复杂动态环境。

Abstract: Recent advances in multi-agent systems manipulation have demonstrated a
rising demand for the implementation of multi-UAV systems in urban areas, which
are always subjected to the presence of static and dynamic obstacles. Inspired
by the collective behavior of tilapia fish and pigeons, the focus of the
presented research is on the introduction of a nature-inspired collision-free
formation control for a multi-UAV system, considering the obstacle avoidance
maneuvers. The developed framework in this study utilizes a semi-distributed
control approach, in which, based on a probabilistic Lloyd's algorithm, a
centralized guidance algorithm works for optimal positioning of the UAVs, while
a distributed control approach has been used for the intervehicle collision and
obstacle avoidance. Further, the presented framework has been extended to the
3D space with a novel definition of 3D maneuvers. Finally, the presented
framework has been applied to multi-UAV systems in 2D and 3D scenarios, and the
obtained results demonstrated the validity of the presented method in dynamic
environments with stationary and moving obstacles.

</details>


### [250] [Mechanical Intelligence-Aware Curriculum Reinforcement Learning for Humanoids with Parallel Actuation](https://arxiv.org/abs/2507.00273)
*Yusuke Tanaka,Alvin Zhu,Quanyou Wang,Dennis Hong*

Main category: cs.RO

TL;DR: 提出了一种端到端课程强化学习框架，用于模拟人形机器人BRUCE的并行驱动机制，相比传统方法在真实场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习框架未考虑并行驱动机制的机械智能，导致运动建模不准确和策略次优。

Method: 使用GPU加速的MJX模拟所有闭链约束，保留硬件物理特性，并与MPC进行对比。

Result: 在零样本部署中表现出更好的泛化能力和性能。

Conclusion: 完全模拟并行机制在端到端学习流程中具有计算优势和性能提升。

Abstract: Reinforcement learning (RL) has enabled significant advances in humanoid
robot locomotion, yet most learning frameworks do not account for mechanical
intelligence embedded in parallel actuation mechanisms due to limitations in
simulator support for closed kinematic chains. This omission can lead to
inaccurate motion modeling and suboptimal policies, particularly for robots
with high actuation complexity. This paper presents an end-to-end curriculum RL
framework for BRUCE, a kid-sized humanoid robot featuring three distinct
parallel mechanisms in its legs: a differential pulley, a 5-bar linkage, and a
4-bar linkage. Unlike prior approaches that rely on simplified serial
approximations, we simulate all closed-chain constraints natively using
GPU-accelerated MJX (MuJoCo), preserving the hardware's physical properties
during training. We benchmark our RL approach against a Model Predictive
Controller (MPC), demonstrating better surface generalization and performance
in real-world zero-shot deployment. This work highlights the computational
approaches and performance benefits of fully simulating parallel mechanisms in
end-to-end learning pipelines for legged humanoids.

</details>


### [251] [When Digital Twins Meet Large Language Models: Realistic, Interactive, and Editable Simulation for Autonomous Driving](https://arxiv.org/abs/2507.00319)
*Tanmay Vilas Samak,Chinmay Vilas Samak,Bing Li,Venkat Krovi*

Main category: cs.RO

TL;DR: 提出了一种统一框架，用于创建和管理高保真数字孪生，以加速自动驾驶研究。该框架结合物理和数据驱动技术，支持实时动态仿真和自然语言编辑场景。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以平衡动态保真度、逼真渲染、场景编排和实时性能，阻碍了自动驾驶系统的开发与验证。

Method: 结合物理和数据驱动技术，重建真实场景（real2sim），并利用大语言模型（LLM）通过自然语言编辑场景。

Result: 框架能重建3D场景（97%结构相似性），保持60Hz帧率，且自然语言生成场景的重复性达95%，泛化性达85%。

Conclusion: 该框架在保真度、性能和可用性方面表现优异，为自动驾驶研究提供了高效工具。

Abstract: Simulation frameworks have been key enablers for the development and
validation of autonomous driving systems. However, existing methods struggle to
comprehensively address the autonomy-oriented requirements of balancing: (i)
dynamical fidelity, (ii) photorealistic rendering, (iii) context-relevant
scenario orchestration, and (iv) real-time performance. To address these
limitations, we present a unified framework for creating and curating
high-fidelity digital twins to accelerate advancements in autonomous driving
research. Our framework leverages a mix of physics-based and data-driven
techniques for developing and simulating digital twins of autonomous vehicles
and their operating environments. It is capable of reconstructing real-world
scenes and assets (real2sim) with geometric and photorealistic accuracy and
infusing them with various physical properties to enable real-time dynamical
simulation of the ensuing driving scenarios. Additionally, it also incorporates
a large language model (LLM) interface to flexibly edit the driving scenarios
online via natural language prompts. We analyze the presented framework in
terms of its fidelity, performance, and serviceability. Results indicate that
our framework can reconstruct 3D scenes and assets with up to 97% structural
similarity, while maintaining frame rates above 60 Hz. We also demonstrate that
it can handle natural language prompts to generate diverse driving scenarios
with up to 95% repeatability and 85% generalizability.

</details>


### [252] [Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding](https://arxiv.org/abs/2507.00416)
*Tao Lin,Gen Li,Yilei Zhong,Yanwen Zou,Bo Zhao*

Main category: cs.RO

TL;DR: 提出了一种通过视觉几何基础模型隐式注入3D几何特征的模块，显著提升了VLA模型在空间理解任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型基于预训练的视觉语言模型（VLM），缺乏精确的空间理解能力，而现有方法需要额外的3D输入或深度传感器。

Method: 设计了一个即插即用模块，利用现成的视觉几何基础模型隐式注入3D几何特征。

Result: 在五个需要精确空间理解能力的任务中，该方法显著提升了VLA模型的性能。

Conclusion: 该方法无需额外传感器即可增强VLA模型的空间理解能力，适用于多样化场景。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising framework for
enabling generalist robots capable of perceiving, reasoning, and acting in the
real world. These models usually build upon pretrained Vision-Language Models
(VLMs), which excel at semantic understanding due to large-scale text
pretraining. However, VLMs typically lack precise spatial understanding
capabilities, as they are primarily tuned on 2D image-text pairs without 3D
supervision. To address this limitation, recent approaches have incorporated
explicit 3D inputs such as point clouds or depth maps, but this necessitates
additional depth sensors or defective estimation. In contrast, our work
introduces a plug-and-play module that implicitly injects 3D geometry features
into VLA models by leveraging an off-the-shelf visual geometry foundation
models. We design five spatially challenging tasks that require precise spatial
understanding ability to validate effectiveness of our method. Extensive
evaluations show that our method significantly improves the performance of
state-of-the-art VLA models across diverse scenarios.

</details>


### [253] [RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation](https://arxiv.org/abs/2507.00435)
*Yi Ru Wang,Carter Ung,Grant Tannert,Jiafei Duan,Josephine Li,Amy Le,Rishabh Oswal,Markus Grotz,Wilbert Pumacay,Yuquan Deng,Ranjay Krishna,Dieter Fox,Siddhartha Srinivasa*

Main category: cs.RO

TL;DR: RoboEval是一个用于评估双手机器人操作策略的仿真基准和结构化框架，揭示当前策略的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅报告二元任务成功率，掩盖了策略行为中的关键弱点，如协调不良、抓取时滑动或手臂使用不对称。

Method: RoboEval引入了一套分层、语义明确的任务，分解为技能特定阶段，并通过变体系统性地挑战空间、物理和协调能力。任务配有细粒度诊断指标和3000+人类示范。

Result: 实验显示，成功率相近的策略在执行方式上差异显著，行为指标在超过半数任务-指标对中与成功率相关。

Conclusion: RoboEval通过揭示策略失败的具体原因，提供了更深入、可操作的机器人操作理解，强调需要超越单纯成功率的评估工具。

Abstract: We present RoboEval, a simulation benchmark and structured evaluation
framework designed to reveal the limitations of current bimanual manipulation
policies. While prior benchmarks report only binary task success, we show that
such metrics often conceal critical weaknesses in policy behavior -- such as
poor coordination, slipping during grasping, or asymmetric arm usage. RoboEval
introduces a suite of tiered, semantically grounded tasks decomposed into
skill-specific stages, with variations that systematically challenge spatial,
physical, and coordination capabilities. Tasks are paired with fine-grained
diagnostic metrics and 3000+ human demonstrations to support imitation
learning. Our experiments reveal that policies with similar success rates
diverge in how tasks are executed -- some struggle with alignment, others with
temporally consistent bimanual control. We find that behavioral metrics
correlate with success in over half of task-metric pairs, and remain
informative even when binary success saturates. By pinpointing when and how
policies fail, RoboEval enables a deeper, more actionable understanding of
robotic manipulation -- and highlights the need for evaluation tools that go
beyond success alone.

</details>


### [254] [DIJE: Dense Image Jacobian Estimation for Robust Robotic Self-Recognition and Visual Servoing](https://arxiv.org/abs/2507.00446)
*Yasunori Toshimitsu,Kento Kawaharazuka,Akihiro Miki,Kei Okada,Masayuki Inaba*

Main category: cs.RO

TL;DR: 提出了DIJE算法，用于实时估计图像雅可比矩阵，支持机器人自我识别和视觉伺服控制。


<details>
  <summary>Details</summary>
Motivation: 机器人需要准确理解自身状态和工具状态以实现真实世界中的运动。

Method: 基于光流计算和简化卡尔曼滤波的DIJE算法，无需标记或机器人结构知识。

Result: 实现了自我识别和视觉伺服控制，并在物理机器人上验证性能。

Conclusion: DIJE的全局估计潜力可扩展为更通用的操作框架。

Abstract: For robots to move in the real world, they must first correctly understand
the state of its own body and the tools that it holds. In this research, we
propose DIJE, an algorithm to estimate the image Jacobian for every pixel. It
is based on an optical flow calculation and a simplified Kalman Filter that can
be efficiently run on the whole image in real time. It does not rely on markers
nor knowledge of the robotic structure. We use the DIJE in a self-recognition
process which can robustly distinguish between movement by the robot and by
external entities, even when the motion overlaps. We also propose a visual
servoing controller based on DIJE, which can learn to control the robot's body
to conduct reaching movements or bimanual tool-tip control. The proposed
algorithms were implemented on a physical musculoskeletal robot and its
performance was verified. We believe that such global estimation of the
visuomotor policy has the potential to be extended into a more general
framework for manipulation.

</details>


### [255] [A Miniature High-Resolution Tension Sensor Based on a Photo-Reflector for Robotic Hands and Grippers](https://arxiv.org/abs/2507.00464)
*Hyun-Bin Kim,Kyung-Soo Kim*

Main category: cs.RO

TL;DR: 该论文提出了一种基于光反射器的小型张力传感器，适用于紧凑型肌腱驱动夹持器和机器人手，具有高分辨率和机械耐用性。


<details>
  <summary>Details</summary>
Motivation: 设计一种小型、高分辨率的张力传感器，以满足紧凑型机器人系统对高精度力反馈的需求。

Method: 采用对称弹性体结构和光反射器测量位移，结合Timoshenko梁理论和有限元分析优化设计，使用16位ADC和CAN-FD通信实现高效信号采集。

Result: 传感器分辨率达9.9mN，RMSE为0.455N，力控制实验中RMSE低至0.073N，相比光遮断器设计分辨率提升十倍以上。

Conclusion: 该传感器设计简单、轻便、易于组装，适用于需要高分辨率力反馈的机器人和假肢系统。

Abstract: This paper presents a miniature tension sensor using a photo-reflector,
designed for compact tendon-driven grippers and robotic hands. The proposed
sensor has a small form factor of 13~mm x 7~mm x 6.5~mm and is capable of
measuring tensile forces up to 200~N. A symmetric elastomer structure
incorporating fillets and flexure hinges is designed based on Timoshenko beam
theory and verified via FEM analysis, enabling improved sensitivity and
mechanical durability while minimizing torsional deformation. The sensor
utilizes a compact photo-reflector (VCNT2020) to measure displacement in the
near-field region, eliminating the need for light-absorbing materials or
geometric modifications required in photo-interrupter-based designs. A 16-bit
analog-to-digital converter (ADC) and CAN-FD (Flexible Data-rate) communication
enable efficient signal acquisition with up to 5~kHz sampling rate. Calibration
experiments demonstrate a resolution of 9.9~mN (corresponding to over 14-bit
accuracy) and a root mean square error (RMSE) of 0.455~N. Force control
experiments using a twisted string actuator and PI control yield RMSEs as low
as 0.073~N. Compared to previous research using photo-interrupter, the proposed
method achieves more than tenfold improvement in resolution while also reducing
nonlinearity and hysteresis. The design is mechanically simple, lightweight,
easy to assemble, and suitable for integration into robotic and prosthetic
systems requiring high-resolution force feedback.

</details>


### [256] [Edge Computing and its Application in Robotics: A Survey](https://arxiv.org/abs/2507.00523)
*Nazish Tahir,Ramviyas Parasuraman*

Main category: cs.RO

TL;DR: 本文综述了边缘计算在机器人领域的应用，强调其低延迟、移动性和位置感知优势，并探讨了当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 填补边缘计算与机器人结合领域的综述空白，分析其优势、挑战及未来发展方向。

Method: 通过全面评估边缘机器人领域的最新进展，深入分析关键应用、动机和挑战。

Result: 总结了边缘计算在机器人中的重要性，特别是在实时响应场景中的应用，并提出了开放研究问题。

Conclusion: 边缘计算为机器人领域提供了显著优势，但仍需解决多项挑战以推动未来发展。

Abstract: The Edge computing paradigm has gained prominence in both academic and
industry circles in recent years. By implementing edge computing facilities and
services in robotics, it becomes a key enabler in the deployment of artificial
intelligence applications to robots. Time-sensitive robotics applications
benefit from the reduced latency, mobility, and location awareness provided by
the edge computing paradigm, which enables real-time data processing and
intelligence at the network's edge. While the advantages of integrating edge
computing into robotics are numerous, there has been no recent survey that
comprehensively examines these benefits. This paper aims to bridge that gap by
highlighting important work in the domain of edge robotics, examining recent
advancements, and offering deeper insight into the challenges and motivations
behind both current and emerging solutions. In particular, this article
provides a comprehensive evaluation of recent developments in edge robotics,
with an emphasis on fundamental applications, providing in-depth analysis of
the key motivations, challenges, and future directions in this rapidly evolving
domain. It also explores the importance of edge computing in real-world
robotics scenarios where rapid response times are critical. Finally, the paper
outlines various open research challenges in the field of edge robotics.

</details>


### [257] [Generation of Indoor Open Street Maps for Robot Navigation from CAD Files](https://arxiv.org/abs/2507.00552)
*Jiajie Zhang,Shenrui Wu,Xu Ma,Sören Schwertfeger*

Main category: cs.RO

TL;DR: 提出一种自动化系统，将建筑CAD文件转换为分层拓扑OpenStreetMap表示，以解决SLAM在动态大规模室内环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统SLAM方法在动态大规模室内环境中存在时间、人力和鲁棒性不足的问题，导致地图过时和定位失败。

Method: 采用多阶段流程，从CAD数据提取关键结构层，利用AreaGraph进行拓扑分割，生成分层导航图，并整合多楼层数据。

Result: 生成语义丰富且拓扑正确的地图，避免SLAM的低效和脆弱性，适用于复杂室内空间。

Conclusion: 该系统通过CAD文件的结构信息提供高效、可扩展的机器人导航解决方案，附带GUI工具便于使用。

Abstract: The deployment of autonomous mobile robots is predicated on the availability
of environmental maps, yet conventional generation via SLAM (Simultaneous
Localization and Mapping) suffers from significant limitations in time, labor,
and robustness, particularly in dynamic, large-scale indoor environments where
map obsolescence can lead to critical localization failures. To address these
challenges, this paper presents a complete and automated system for converting
architectural Computer-Aided Design (CAD) files into a hierarchical topometric
OpenStreetMap (OSM) representation, tailored for robust life-long robot
navigation. Our core methodology involves a multi-stage pipeline that first
isolates key structural layers from the raw CAD data and then employs an
AreaGraph-based topological segmentation to partition the building layout into
a hierarchical graph of navigable spaces. This process yields a comprehensive
and semantically rich map, further enhanced by automatically associating
textual labels from the CAD source and cohesively merging multiple building
floors into a unified, topologically-correct model. By leveraging the permanent
structural information inherent in CAD files, our system circumvents the
inefficiencies and fragility of SLAM, offering a practical and scalable
solution for deploying robots in complex indoor spaces. The software is
encapsulated within an intuitive Graphical User Interface (GUI) to facilitate
practical use. The code and dataset are available at
https://github.com/jiajiezhang7/osmAG-from-cad.

</details>


### [258] [Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery](https://arxiv.org/abs/2507.00635)
*Tinghe Hong,Shenlin Cai,Boyang Li,Kai Huang*

Main category: cs.RO

TL;DR: 提出一种结合机器学习和传统算法的创新眼动追踪方法，解决了手术机器人导航中眼动估计的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有眼动估计技术依赖额外传感器或面部检测，且易受遮挡和光线影响，限制了手术机器人的术前导航精度。

Method: 结合机器学习和传统算法，无需依赖面部标志点，在光线变化和阴影条件下实现稳定的虹膜检测和眼动估计。

Result: 实验显示，眼动方向估计平均误差为0.58度，机器人臂控制平均误差为2.08度。

Conclusion: 该方法显著提升了手术机器人导航的精度和稳定性，为术前自动化提供了可行方案。

Abstract: Ophthalmic surgical robots offer superior stability and precision by reducing
the natural hand tremors of human surgeons, enabling delicate operations in
confined surgical spaces. Despite the advancements in developing vision- and
force-based control methods for surgical robots, preoperative navigation
remains heavily reliant on manual operation, limiting the consistency and
increasing the uncertainty. Existing eye gaze estimation techniques in the
surgery, whether traditional or deep learning-based, face challenges including
dependence on additional sensors, occlusion issues in surgical environments,
and the requirement for facial detection. To address these limitations, this
study proposes an innovative eye localization and tracking method that combines
machine learning with traditional algorithms, eliminating the requirements of
landmarks and maintaining stable iris detection and gaze estimation under
varying lighting and shadow conditions. Extensive real-world experiment results
show that our proposed method has an average estimation error of 0.58 degrees
for eye orientation estimation and 2.08-degree average control error for the
robotic arm's movement based on the calculated orientation.

</details>


### [259] [Parallel Transmission Aware Co-Design: Enhancing Manipulator Performance Through Actuation-Space Optimization](https://arxiv.org/abs/2507.00644)
*Rohit Kumar,Melya Boukheddimi,Dennis Mronga,Shivesh Kumar,Frank Kirchner*

Main category: cs.RO

TL;DR: 提出了一种新的机器人协同设计方法，显式纳入并行耦合约束，显著提升动态负载能力。


<details>
  <summary>Details</summary>
Motivation: 传统机器人结构设计与行为优化分离，且多数方法忽略并行机制，限制了系统能力。

Method: 采用双层优化框架，外环优化设计参数（如传动比），内环在驱动空间进行轨迹优化。

Result: 相比传统树型模型方法，新方法显著提高了动态负载能力。

Conclusion: 显式考虑并行约束的协同设计方法更高效，适用于复杂机器人平台。

Abstract: In robotics, structural design and behavior optimization have long been
considered separate processes, resulting in the development of systems with
limited capabilities. Recently, co-design methods have gained popularity, where
bi-level formulations are used to simultaneously optimize the robot design and
behavior for specific tasks. However, most implementations assume a serial or
tree-type model of the robot, overlooking the fact that many robot platforms
incorporate parallel mechanisms. In this paper, we present a novel co-design
approach that explicitly incorporates parallel coupling constraints into the
dynamic model of the robot. In this framework, an outer optimization loop
focuses on the design parameters, in our case the transmission ratios of a
parallel belt-driven manipulator, which map the desired torques from the joint
space to the actuation space. An inner loop performs trajectory optimization in
the actuation space, thus exploiting the entire dynamic range of the
manipulator. We compare the proposed method with a conventional co-design
approach based on a simplified tree-type model. By taking advantage of the
actuation space representation, our approach leads to a significant increase in
dynamic payload capacity compared to the conventional co-design implementation.

</details>


### [260] [Learning Steerable Imitation Controllers from Unstructured Animal Motions](https://arxiv.org/abs/2507.00677)
*Dongho Kang,Jin Cheng,Fatemeh Zargarbashi,Taerim Yoon,Sungjoon Choi,Stelian Coros*

Main category: cs.RO

TL;DR: 提出了一种利用动物运动数据控制腿式机器人的框架，生成动物化且用户可控的行为。


<details>
  <summary>Details</summary>
Motivation: 利用真实世界的动物运动数据，解决机器人运动多样性与用户控制的平衡问题。

Method: 通过逆运动学和模型预测控制转换数据，使用变分自编码器合成运动，结合强化学习控制器。

Result: 四足机器人能自适应切换步态并准确跟踪用户指令，保持运动风格一致性。

Conclusion: 该方法实现了更准确可靠的运动模仿，适用于腿式机器人控制。

Abstract: This paper presents a control framework for legged robots that leverages
unstructured real-world animal motion data to generate animal-like and
user-steerable behaviors. Our framework learns to follow velocity commands
while reproducing the diverse gait patterns in the original dataset. To begin
with, animal motion data is transformed into a robot-compatible database using
constrained inverse kinematics and model predictive control, bridging the
morphological and physical gap between the animal and the robot. Subsequently,
a variational autoencoder-based motion synthesis module captures the diverse
locomotion patterns in the motion database and generates smooth transitions
between them in response to velocity commands. The resulting kinematic motions
serve as references for a reinforcement learning-based feedback controller
deployed on physical robots. We show that this approach enables a quadruped
robot to adaptively switch gaits and accurately track user velocity commands
while maintaining the stylistic coherence of the motion data. Additionally, we
provide component-wise evaluations to analyze the system's behavior in depth
and demonstrate the efficacy of our method for more accurate and reliable
motion imitation.

</details>


### [261] [PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments](https://arxiv.org/abs/2507.00816)
*Mengyun Wang,Bo Wang,Yifeng Niu,Chang Wang*

Main category: cs.RO

TL;DR: 论文提出了一种结合物理知识与数据驱动的方法（PI-WAN），通过嵌入物理约束改进四旋翼飞行器的动态建模，提升了在未知环境中的轨迹跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统物理建模方法在未知环境中表现不佳，而数据驱动方法对分布外数据泛化能力差。需要一种结合两者的方法以提高鲁棒性。

Method: 采用时间卷积网络（TCN）架构，结合物理约束的损失函数，嵌入到模型预测控制（MPC）框架中。

Result: 仿真和实际飞行实验表明，PI-WAN在预测精度、跟踪性能和鲁棒性上优于基线方法。

Conclusion: PI-WAN通过结合物理与数据驱动方法，显著提升了四旋翼飞行器在未知环境中的动态建模和轨迹跟踪能力。

Abstract: Accurate dynamics modeling is essential for quadrotors to achieve precise
trajectory tracking in various applications. Traditional physical
knowledge-driven modeling methods face substantial limitations in unknown
environments characterized by variable payloads, wind disturbances, and
external perturbations. On the other hand, data-driven modeling methods suffer
from poor generalization when handling out-of-distribution (OoD) data,
restricting their effectiveness in unknown scenarios. To address these
challenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN),
which combines knowledge-driven and data-driven modeling methods by embedding
physical constraints directly into the training process for robust quadrotor
dynamics learning. Specifically, PI-WAN employs a Temporal Convolutional
Network (TCN) architecture that efficiently captures temporal dependencies from
historical flight data, while a physics-informed loss function applies physical
principles to improve model generalization and robustness across previously
unseen conditions. By incorporating real-time prediction results into a model
predictive control (MPC) framework, we achieve improvements in closed-loop
tracking performance. Comprehensive simulations and real-world flight
experiments demonstrate that our approach outperforms baseline methods in terms
of prediction accuracy, tracking precision, and robustness to unknown
environments.

</details>


### [262] [HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning](https://arxiv.org/abs/2507.00833)
*Zhi Jing,Siyuan Yang,Jicong Ao,Ting Xiao,Yugang Jiang,Chenjia Bai*

Main category: cs.RO

TL;DR: HumanoidGen是一个自动化任务创建和演示收集框架，专注于双手机器人操作，利用原子操作和LLM推理生成空间约束。


<details>
  <summary>Details</summary>
Motivation: 现有机器人数据集和仿真基准主要针对单臂机器人，缺乏适用于双手机器人的高质量演示和任务。

Method: 通过原子操作和LLM规划生成空间约束，并利用蒙特卡洛树搜索增强LLM推理能力。

Result: 实验表明，生成的2D和3D扩散策略性能随数据集规模提升。

Conclusion: HumanoidGen为双手机器人操作提供了有效的任务生成和数据收集方法。

Abstract: For robotic manipulation, existing robotics datasets and simulation
benchmarks predominantly cater to robot-arm platforms. However, for humanoid
robots equipped with dual arms and dexterous hands, simulation tasks and
high-quality demonstrations are notably lacking. Bimanual dexterous
manipulation is inherently more complex, as it requires coordinated arm
movements and hand operations, making autonomous data collection challenging.
This paper presents HumanoidGen, an automated task creation and demonstration
collection framework that leverages atomic dexterous operations and LLM
reasoning to generate relational constraints. Specifically, we provide spatial
annotations for both assets and dexterous hands based on the atomic operations,
and perform an LLM planner to generate a chain of actionable spatial
constraints for arm movements based on object affordances and scenes. To
further improve planning ability, we employ a variant of Monte Carlo tree
search to enhance LLM reasoning for long-horizon tasks and insufficient
annotation. In experiments, we create a novel benchmark with augmented
scenarios to evaluate the quality of the collected data. The results show that
the performance of the 2D and 3D diffusion policies can scale with the
generated dataset. Project page is https://openhumanoidgen.github.io.

</details>


### [263] [I Move Therefore I Learn: Experience-Based Traversability in Outdoor Robotics](https://arxiv.org/abs/2507.00882)
*Miguel Ángel de Miguel,Jorge Beltrán,Juan S. Cely,Francisco Martín,Juan Carlos Manzanares,Alberto García*

Main category: cs.RO

TL;DR: 提出了一种基于经验的方法，通过自主学习和聚类技术实现户外机器人复杂环境中的可通行性估计，无需依赖预标记数据集。


<details>
  <summary>Details</summary>
Motivation: 解决户外机器人在复杂环境中安全导航的问题，避免依赖大量预标记数据的限制。

Method: 结合高程和纹理数据构建多层网格地图，使用VAE和BIRCH算法进行特征编码与聚类，通过实时匹配评估可通行性。

Result: 在合成和真实场景中验证了方法的有效性、鲁棒性和适应性，优于现有技术。

Conclusion: 该方法无需目标场景数据训练，能动态适应新地形，适用于多种平台和表面。

Abstract: Accurate traversability estimation is essential for safe and effective
navigation of outdoor robots operating in complex environments. This paper
introduces a novel experience-based method that allows robots to autonomously
learn which terrains are traversable based on prior navigation experience,
without relying on extensive pre-labeled datasets. The approach integrates
elevation and texture data into multi-layered grid maps, which are processed
using a variational autoencoder (VAE) trained on a generic texture dataset.
During an initial teleoperated phase, the robot collects sensory data while
moving around the environment. These experiences are encoded into compact
feature vectors and clustered using the BIRCH algorithm to represent
traversable terrain areas efficiently. In deployment, the robot compares new
terrain patches to its learned feature clusters to assess traversability in
real time. The proposed method does not require training with data from the
targeted scenarios, generalizes across diverse surfaces and platforms, and
dynamically adapts as new terrains are encountered. Extensive evaluations on
both synthetic benchmarks and real-world scenarios with wheeled and legged
robots demonstrate its effectiveness, robustness, and superior adaptability
compared to state-of-the-art approaches.

</details>


### [264] [A Survey: Learning Embodied Intelligence from Physical Simulators and World Models](https://arxiv.org/abs/2507.00917)
*Xiaoxiao Long,Qingrui Zhao,Kaiwen Zhang,Zihao Zhang,Dingrui Wang,Yumeng Liu,Zhengjie Shu,Yi Lu,Shouzheng Wang,Xinzhe Wei,Wei Li,Wei Yin,Yao Yao,Jia Pan,Qiu Shen,Ruigang Yang,Xun Cao,Qionghai Dai*

Main category: cs.RO

TL;DR: 综述探讨了物理模拟器和世界模型在实现具身智能中的作用，分析了它们在提升机器人自主性、适应性和泛化能力中的互补性。


<details>
  <summary>Details</summary>
Motivation: 推动人工通用智能（AGI）的发展，需解决具身智能中感知、推理和行动的综合问题。

Method: 通过整合物理模拟器和世界模型，系统回顾了学习具身AI的最新进展。

Result: 物理模拟器提供高保真训练环境，世界模型支持内部表征和预测规划，两者结合可缩小模拟训练与实际部署的差距。

Conclusion: 综述为未来更具能力和泛化性的具身AI系统提供了全面视角，并指出了开放挑战。

Abstract: The pursuit of artificial general intelligence (AGI) has placed embodied
intelligence at the forefront of robotics research. Embodied intelligence
focuses on agents capable of perceiving, reasoning, and acting within the
physical world. Achieving robust embodied intelligence requires not only
advanced perception and control, but also the ability to ground abstract
cognition in real-world interactions. Two foundational technologies, physical
simulators and world models, have emerged as critical enablers in this quest.
Physical simulators provide controlled, high-fidelity environments for training
and evaluating robotic agents, allowing safe and efficient development of
complex behaviors. In contrast, world models empower robots with internal
representations of their surroundings, enabling predictive planning and
adaptive decision-making beyond direct sensory input. This survey
systematically reviews recent advances in learning embodied AI through the
integration of physical simulators and world models. We analyze their
complementary roles in enhancing autonomy, adaptability, and generalization in
intelligent robots, and discuss the interplay between external simulation and
internal modeling in bridging the gap between simulated training and real-world
deployment. By synthesizing current progress and identifying open challenges,
this survey aims to provide a comprehensive perspective on the path toward more
capable and generalizable embodied AI systems. We also maintain an active
repository that contains up-to-date literature and open-source projects at
https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey.

</details>


### [265] [RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles](https://arxiv.org/abs/2507.00937)
*David Hunt,Shaocheng Luo,Spencer Hallyburton,Shafii Nillongo,Yi Li,Tingjun Chen,Miroslav Pajic*

Main category: cs.RO

TL;DR: RaGNNarok是一个基于图神经网络的轻量级框架，用于增强毫米波雷达点云，适用于低成本室内移动机器人，具有实时性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有基于激光雷达和摄像头的解决方案在视觉遮挡环境下性能不佳、计算开销大且成本高，而毫米波雷达虽然成本低但存在点云稀疏、噪声和误检问题。

Method: 提出RaGNNarok框架，利用图神经网络增强雷达点云，适用于复杂动态环境，并在低成本设备（如Raspberry Pi 5）上高效运行。

Result: 在定位、SLAM和自主导航等任务中表现出高可靠性和通用性，推理时间仅为7.3毫秒。

Conclusion: RaGNNarok为低成本室内移动机器人提供了一种鲁棒的解决方案。

Abstract: Low-cost indoor mobile robots have gained popularity with the increasing
adoption of automation in homes and commercial spaces. However, existing lidar
and camera-based solutions have limitations such as poor performance in
visually obscured environments, high computational overhead for data
processing, and high costs for lidars. In contrast, mmWave radar sensors offer
a cost-effective and lightweight alternative, providing accurate ranging
regardless of visibility. However, existing radar-based localization suffers
from sparse point cloud generation, noise, and false detections. Thus, in this
work, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph
neural network (GNN)-based framework to enhance radar point clouds, even in
complex and dynamic environments. With an inference time of just 7.3 ms on the
low-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such
resource-constrained devices, requiring no additional computational resources.
We evaluate its performance across key tasks, including localization, SLAM, and
autonomous navigation, in three different environments. Our results demonstrate
strong reliability and generalizability, making RaGNNarok a robust solution for
low-cost indoor mobile robots.

</details>


### [266] [Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation](https://arxiv.org/abs/2507.00984)
*Xihang Yu,Rajat Talak,Jingnan Shi,Ulrich Viereck,Igor Gilitschenski,Luca Carlone*

Main category: cs.RO

TL;DR: 本文提出了一种自监督领域适应方法，利用未标注数据提升仓库自动化系统中机器人对箱子姿态和形状的感知能力。


<details>
  <summary>Details</summary>
Motivation: 现代仓库自动化系统依赖机器人产生大量未标注数据，如何利用这些数据提升感知模型是一个关键问题。

Method: 提出了一种自监督领域适应流程，通过真实世界未标注数据改进感知模型，无需人工标注。专注于箱子姿态和形状估计，采用纠正与认证流程。

Result: 在模拟和真实工业环境中广泛评估，包括5万张真实图像数据集。自监督模型显著优于仅基于模拟训练的模型，并大幅超越零样本3D边界框估计基线。

Conclusion: 自监督领域适应方法能有效利用未标注数据，显著提升箱子姿态和形状估计的准确性。

Abstract: Modern warehouse automation systems rely on fleets of intelligent robots that
generate vast amounts of data -- most of which remains unannotated. This paper
develops a self-supervised domain adaptation pipeline that leverages
real-world, unlabeled data to improve perception models without requiring
manual annotations. Our work focuses specifically on estimating the pose and
shape of boxes and presents a correct-and-certify pipeline for self-supervised
box pose and shape estimation. We extensively evaluate our approach across a
range of simulated and real industrial settings, including adaptation to a
large-scale real-world dataset of 50,000 images. The self-supervised model
significantly outperforms models trained solely in simulation and shows
substantial improvements over a zero-shot 3D bounding box estimation baseline.

</details>


### [267] [Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations](https://arxiv.org/abs/2507.00990)
*Shivansh Patel,Shraddhaa Mohan,Hanlin Mai,Unnat Jain,Svetlana Lazebnik,Yunzhu Li*

Main category: cs.RO

TL;DR: RIGVid系统通过模仿AI生成的视频让机器人完成复杂任务，无需物理演示或机器人特定训练。


<details>
  <summary>Details</summary>
Motivation: 解决机器人学习复杂任务时对物理演示和特定训练的依赖问题。

Method: 结合视频扩散模型生成演示视频，用视觉语言模型过滤不符合命令的视频，6D姿态跟踪器提取物体轨迹并适配机器人。

Result: 生成的视频与真实演示效果相当，性能随生成质量提升，优于关键点预测等方法。

Conclusion: AI生成的视频可作为机器人操作的有效监督来源。

Abstract: This work introduces Robots Imitating Generated Videos (RIGVid), a system
that enables robots to perform complex manipulation tasks--such as pouring,
wiping, and mixing--purely by imitating AI-generated videos, without requiring
any physical demonstrations or robot-specific training. Given a language
command and an initial scene image, a video diffusion model generates potential
demonstration videos, and a vision-language model (VLM) automatically filters
out results that do not follow the command. A 6D pose tracker then extracts
object trajectories from the video, and the trajectories are retargeted to the
robot in an embodiment-agnostic fashion. Through extensive real-world
evaluations, we show that filtered generated videos are as effective as real
demonstrations, and that performance improves with generation quality. We also
show that relying on generated videos outperforms more compact alternatives
such as keypoint prediction using VLMs, and that strong 6D pose tracking
outperforms other ways to extract trajectories, such as dense feature point
tracking. These findings suggest that videos produced by a state-of-the-art
off-the-shelf model can offer an effective source of supervision for robotic
manipulation.

</details>


### [268] [DexWrist: A Robotic Wrist for Constrained and Dynamic Manipulation](https://arxiv.org/abs/2507.01008)
*Martin Peticco,Gabriella Ulloa,John Marangola,Pulkit Agrawal*

Main category: cs.RO

TL;DR: DexWrist是一种柔顺的机器人手腕，旨在提升受限环境中的机器人操作能力，支持动态任务并加速数据收集。


<details>
  <summary>Details</summary>
Motivation: 设计DexWrist是为了接近人类手腕的功能能力，并在机械柔顺性和工作空间上超越现有机器人手腕设计。

Method: 通过实现快速遥操作、减少任务步骤、设计扭矩透明且易于模拟的动力学，以及扩展工作空间来优化数据收集和策略学习。

Result: DexWrist能够加速策略学习，缩短任务完成时间，并支持在高度杂乱场景中的操作。

Conclusion: DexWrist在机器人操作和数据收集方面表现出显著优势，适用于复杂环境。

Abstract: We present the DexWrist, a compliant robotic wrist designed to advance
robotic manipulation in highly-constrained environments, enable dynamic tasks,
and speed up data collection. DexWrist is designed to be close to the
functional capabilities of the human wrist and achieves mechanical compliance
and a greater workspace as compared to existing robotic wrist designs. The
DexWrist can supercharge policy learning by (i) enabling faster teleoperation
and therefore making data collection more scalable; (ii) completing tasks in
fewer steps which reduces trajectory lengths and therefore can ease policy
learning; (iii) DexWrist is designed to be torque transparent with easily
simulatable kinematics for simulated data collection; and (iv) most importantly
expands the workspace of manipulation for approaching highly cluttered scenes
and tasks. More details about the wrist can be found at:
dexwrist.csail.mit.edu.

</details>


### [269] [VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers](https://arxiv.org/abs/2507.01016)
*Yating Wang,Haoyi Zhu,Mingyu Liu,Jiange Yang,Hao-Shu Fang,Tong He*

Main category: cs.RO

TL;DR: 本文提出了一种基于向量量化的动作分词器，利用大规模动作轨迹数据集，显著提升了动作生成的质量和效率，并在零样本任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决动作生成中的时空动态捕捉问题，并利用合成数据弥补真实数据的不足，提出了一种高效的动作分词器。

Method: 基于向量量化技术，利用超大规模动作轨迹数据集训练分词器，支持零样本适应多种下游任务。

Result: 实验表明，随着合成数据量的增加，分词器性能显著提升，在长时程任务中成功率提高30%。

Conclusion: 该动作分词器为实时智能系统提供了高效可靠的解决方案，具有广泛的应用潜力。

Abstract: In this paper, we introduce an innovative vector quantization based action
tokenizer built upon the largest-scale action trajectory dataset to date,
leveraging over 100 times more data than previous approaches. This extensive
dataset enables our tokenizer to capture rich spatiotemporal dynamics,
resulting in a model that not only accelerates inference but also generates
smoother and more coherent action outputs. Once trained, the tokenizer can be
seamlessly adapted to a wide range of downstream tasks in a zero-shot manner,
from short-horizon reactive behaviors to long-horizon planning. A key finding
of our work is that the domain gap between synthetic and real action
trajectories is marginal, allowing us to effectively utilize a vast amount of
synthetic data during training without compromising real-world performance. To
validate our approach, we conducted extensive experiments in both simulated
environments and on real robotic platforms. The results demonstrate that as the
volume of synthetic trajectory data increases, the performance of our tokenizer
on downstream tasks improves significantly-most notably, achieving up to a 30%
higher success rate on two real-world tasks in long-horizon scenarios. These
findings highlight the potential of our action tokenizer as a robust and
scalable solution for real-time embodied intelligence systems, paving the way
for more efficient and reliable robotic control in diverse application
domains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [270] [Horus: A Protocol for Trustless Delegation Under Uncertainty](https://arxiv.org/abs/2507.00631)
*David Shi,Kevin Joo*

Main category: cs.GT

TL;DR: 提出一种通过抵押索赔和递归验证游戏确保AI代理正确性的协议，使正确性成为纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 在动态、低信任环境中，无法通过预先规范或集中监督确保AI代理的正确性。

Method: 任务发布为意图，解决者竞争完成；验证者事后检查正确性，挑战者可抵押触发验证。

Result: 错误代理受罚，正确挑战者获奖励，激励对齐使正确性成为纳什均衡。

Conclusion: 通过抵押和验证游戏，可在低信任环境中实现AI代理的正确性。

Abstract: Correctness is an emergent property of systems where exposing error is
cheaper than committing it. In dynamic, low-trust environments, autonomous AI
agents benefit from delegating work to sub-agents, yet correctness cannot be
assured through upfront specification or centralized oversight. We propose a
protocol that enforces correctness through collateralized claims in a recursive
verification game. Tasks are published as intents, and solvers compete to
fulfill them. Selected solvers carry out tasks under risk, with correctness
checked post hoc by verifiers. Any challenger can challenge a result by staking
against it to trigger the verification process. Incorrect agents are slashed
and correct opposition is rewarded, with an escalation path that penalizes
erroneous verifiers themselves. When incentives are aligned across solvers,
challengers, and verifiers, falsification conditions make correctness the Nash
equilibrium.

</details>
